// Copyright 2025 Advanced Micro Devices, Inc.
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

//===----------------------------------------------------------------------===//
//
// This file contains the `TensorAttr` class definition for all compile-time
// constant metadata pertaining to tensors.
//
// A note on memory formats, strides and contiguity of tensors:
//
// A contiguous tensor has the same logical and physical (in-memory) layout and
// doesn't require strides to define memory access patterns. But if we were to
// assign strides, a contiguous tensor of shape (n, c, h, w) may be accessed in
// row-major order using strides (c*h*w, h*w, w, 1) - reversed cumulative
// product of its dimensions. Likewise a contiguous tensor of shape (n, h, w, c)
// may be accessed in row-major order using strides (h*w*c , w*c, c, 1).
// Essentially, strides of contiguous row-major tensors are monotonically
// decreasing (increasing for column major) and the last dimension (first for
// column major) always has a stride of 1.
//
// A non-contiguous tensor has a physical (in-memory) layout that is different
// from its logical layout. So a non-contiguous tensor of logical shape (n, c,
// h, w) may be stored in-memory as channels-last (NHWC) using strides (h*w*c,
// 1, w*c, c).
//
// Basically, strides can be used to represent different physical (in-memory)
// layouts while preserving the same logical layout for tensors.
//
// PyTorch enforces tensors to always conform to the logical channels-first
// (NCHW) layout, and uses strides as a way to specify the exact physical
// (in-memory) layout - contiguous (NCHW) or channels-last (NHWC). Not all
// operators support the non-contiguous channels-last format though as this
// is specific to the backend that implements it.
//
// On the other hand, there is no notion of strides in MLIR / IREE compiler.
// It expects the tensors to always be contiguous (row-major) as specified by
// its shape. It therefore assumes physical layout is always the same as the
// logical layout.
//
// Fusilli sits between the high-level ML frameworks (PyTorch) and the low-level
// MLIR compilers (IREE), so it needs to conform to the constraints in tensor
// representations between the frontend and the backend while supporting
// both memory formats (for performance reasons). With this in mind, we follow
// the following convention for memory formats in Fusilli:
//
// A tensor in Fusilli is defined by its logical shape which is always in
// channels-first (NCHW if 4D) format. The physical (in-memory) layout
// is then determined by the strides.
//
//  * Tensor T1 of logical shape (n,c,h,w) and stride (c*h*w, h*w, w, 1) is NCHW
//  in-memory (also called contiguous or channels-first)
//
//  * Tensor T2 of logical shape (n,c,h,w) and stride (h*w*c, 1, w*c, c) is NHWC
//  in-memory (also called non-contiguous or channels-last)
//
// Any other stride configuration is considered invalid and will result in an
// error during validation.
//
// Fusilli emits MLIR assembly for the `torch` dialect which is tied to PyTorch
// semantics where ops always operate on channels-first (NCHW if 4D) tensors.
// This works fine when inputs to a Fusilli node are channels-first, but we need
// special handling for when they are in channels-last format. We introduce the
// applicable transposes to convert tensors from channels-last (NHWC if 4D) to
// channels-first (NCHW if 4D) at the inputs and vice versa at the outputs of
// the Fusilli node. This ensures that the MLIR assembly generated by Fusilli
// for that node is compatible with the `torch` dialect semantics at the op
// boundaries. The transposes are expected to eventually be propagated and
// optimized out by the IREE compiler. In addition to this, we also transpose
// the shapes for channels-last tensors in the generated MLIR assembly to
// satisfy the contiguity requirement (physical layout == logical layout) in
// MLIR. This allows us to drop the strides in the MLIR assembly.
//
// For example, here's what the initial and subsequent IRs for a convolution on
// a channels-last tensor (NHWC) would look like as emitted by Fusilli and after
// IREE's transpose propagation and elimination passes:
//
//  User input (Fusilli graph):
//    x [NCHW + stride] -> conv [NHWC] -> y [NCHW + stride]
//
//  Generated ASM (Torch dialect):
//    x [NHWC] -> T [NCHW] -> conv [NCHW] -> T' [NHWC] -> y [NHWC]
//
//  Compiler passes (Linalg dialect):
//  Transpose propagation
//    x [NHWC] -> conv [NHWC] -> T [NCHW] -> T' [NHWC] -> y [NHWC]
//
//  Transpose elimination
//    x [NHWC] -> conv [NHWC] -> y [NHWC]
//
//===----------------------------------------------------------------------===//

#ifndef FUSILLI_ATTRIBUTES_TENSOR_ATTRIBUTES_H
#define FUSILLI_ATTRIBUTES_TENSOR_ATTRIBUTES_H

#include "fusilli/attributes/types.h"
#include "fusilli/graph/context.h"
#include "fusilli/support/logging.h"

#include <algorithm>
#include <cassert>
#include <cstddef>
#include <cstdint>
#include <memory>
#include <numeric>
#include <optional>
#include <ranges>
#include <string>
#include <type_traits>
#include <variant>
#include <vector>

namespace fusilli {

// Concept that will accept any type that models a range (something with
// .begin(), and .end()) with value type of int64_t.
template <typename R>
concept Int64Range =
    std::ranges::forward_range<R> &&
    std::is_same_v<std::ranges::range_value_t<R>, int64_t>; // C++ 20

// Generates stride order for a contiguous tensor. For a 4D tensor, this would
// return {N: 3, C: 2, H: 1, W: 0} to represent an NCHW in-memory layout.
// Here N is the slowest changing and W is the fastest changing dimension.
inline std::vector<size_t> getContiguousStrideOrder(size_t numDims) {
  assert(numDims >= 1 && "Contiguous layout requires at least 1 dimension");

  std::vector<size_t> strideOrder(numDims);
  size_t order = 0;
  // Caution: Reverse iteration with size_t (unsigned) can lead to underflow
  // when `i == 0` is decremented due to wrap-around to SIZE_MAX (which is
  // positive and doesn't prevent control from entering the loop as intended).
  // This can cause heap corruption when accessing strideOrder[SIZE_MAX].
  // So we instead gate loop entry with `i > 0`.
  for (size_t i = numDims; i > 0; --i)
    strideOrder[i - 1] = order++;
  return strideOrder;
}

// Generates stride order for a channels-last tensor. For a 4D tensor, this
// would return {N: 3, C: 0, H: 2, W: 1} to represent an NHWC in-memory layout.
// Here N is the slowest changing and C is the fastest changing dimension.
inline std::vector<size_t> getChannelsLastStrideOrder(size_t numDims) {
  assert(numDims >= 3 && "Channels-last layout requires at least 3 dimensions");

  std::vector<size_t> strideOrder(numDims);
  size_t order = 0;
  strideOrder[1] = order++;
  for (size_t i = numDims - 1; i > 1; --i)
    strideOrder[i] = order++;
  strideOrder[0] = order;
  return strideOrder;
}

// Generates a stride order preserving the format of `inputStride`. When the
// desired format has a larger size, the result is padded to be of size
// `outputDimSize`.
//
// For example: an input of {10, 30, 20} would return a stride order of
// {0, 2, 1}.
inline std::vector<size_t>
generateStrideOrderPreservingFormat(const std::vector<int64_t> inputStride,
                                    size_t outputDimSize) {
  std::vector<size_t> indices(inputStride.size());
  std::iota(indices.begin(), indices.end(), 0);

  // Sort indices based on stride values in descending order
  std::sort(indices.begin(), indices.end(), [&inputStride](size_t i, size_t j) {
    return inputStride[i] < inputStride[j];
  });

  // Create the stride order
  std::vector<size_t> strideOrder(inputStride.size());
  for (size_t i = 0; i < indices.size(); ++i) {
    strideOrder[indices[i]] = i;
  }

  // If output_dim_size is larger, pad with remaining dimensions
  if (outputDimSize > inputStride.size()) {
    size_t start = strideOrder.size();
    strideOrder.resize(outputDimSize);
    std::iota(strideOrder.begin() + start, strideOrder.end(), start);
  }
  return strideOrder;
}

inline std::vector<int64_t>
generateStrideFromDim(const std::vector<int64_t> &dim,
                      const std::vector<size_t> &strideOrder) {
  size_t numDims = dim.size();
  std::vector<int64_t> stride(numDims);
  std::vector<std::pair<size_t, int64_t>> idxToDimInStrideOrder(numDims);

  // Example dim = (10, 3, 12, 12) and strideOrder = {3, 0, 2, 1}
  // idxToDimInStrideOrder will be:
  // { (1, 3), (3, 12), (2, 12), (0, 10) }
  // ordered from fastest changing dim to slowest changing dim
  for (size_t i = 0; i < numDims; ++i)
    idxToDimInStrideOrder[strideOrder[i]] = std::make_pair(i, dim[i]);

  // Stride is cumulative product of dimensions in the order specified by
  // strideOrder. For the above example, this will yield:
  // stride = (432, 1, 36, 3)
  int64_t cumulativeProduct = 1;
  for (size_t i = 0; i < numDims; ++i) {
    stride[idxToDimInStrideOrder[i].first] = cumulativeProduct;
    cumulativeProduct *= idxToDimInStrideOrder[i].second;
  }

  return stride;
}

// Generates permute order to preserve the layout of a contiguous tensor.
// For a 4D tensor, this would return {0, 1, 2, 3} to preserve the NCHW
// layout. This is effectively used for a no-op permute.
inline std::vector<int64_t> getPreserveContiguousPermuteOrder(size_t numDims) {
  assert(numDims >= 1 && "Contiguous layout requires at least 1 dimension");

  std::vector<int64_t> permuteOrder(numDims);
  std::iota(permuteOrder.begin(), permuteOrder.end(), 0);
  return permuteOrder;
}

// Generates permute order to convert the layout of a channels-last tensor to
// a contiguous tensor. For a 4D tensor, this would return {0, 3, 1, 2} to go
// from NHWC to NCHW layout.
inline std::vector<int64_t>
getChannelsLastToContiguousPermuteOrder(size_t numDims) {
  assert(numDims >= 3 && "Channels-last layout requires at least 3 dimensions");

  std::vector<int64_t> permuteOrder(numDims);
  int64_t order = 0;
  permuteOrder[0] = order++;
  for (size_t i = 2; i < numDims; ++i)
    permuteOrder[i] = order++;
  permuteOrder[1] = order;
  return permuteOrder;
}

// Generates permute order to convert the layout of a contiguous tensor to a
// channels-last tensor. For a 4D tensor, this would return {0, 2, 3, 1} to go
// from NCHW to NHWC layout.
inline std::vector<int64_t>
getContiguousToChannelsLastPermuteOrder(size_t numDims) {
  assert(numDims >= 3 && "Channels-last layout requires at least 3 dimensions");

  std::vector<int64_t> permuteOrder(numDims);
  int64_t order = 0;
  permuteOrder[0] = order++;
  permuteOrder[numDims - 1] = order++;
  for (size_t i = 1; i < numDims - 1; ++i)
    permuteOrder[i] = order++;
  return permuteOrder;
}

// Takes a set of input shapes and computes a common shape that all inputs
// shapes can be broadcast to. This implements Pytorch style broadcasting where
// shapes are right-aligned. For example:
//
// Input shapes:
//   {64, 16,  1, 1}
//       { 1, 32, 1}
//
// Result:
//   {64, 16, 32, 1}
inline ErrorOr<std::vector<int64_t>>
computeBroadcastShape(const std::vector<std::vector<int64_t>> &shapes) {
  // Remove empty shapes.
  auto filteredShapes =
      shapes | std::views::filter([](const std::vector<int64_t> &shape) {
        return !shape.empty();
      });
  FUSILLI_RETURN_ERROR_IF(filteredShapes.empty(), ErrorCode::InvalidAttribute,
                          "All input shapes are empty");

  // Find the maximum rank in `shapes`.
  size_t maxSize =
      std::max_element(
          filteredShapes.begin(), filteredShapes.end(),
          [](const std::vector<int64_t> &lhs, const std::vector<int64_t> &rhs) {
            return lhs.size() < rhs.size();
          })
          ->size();

  std::vector<int64_t> commonShape(maxSize, 1);
  for (const std::vector<int64_t> &shape : filteredShapes) {
    // When broadcasting shapes of differing ranks, the dimensions are
    // right-aligned. Process from rightmost dimension to leftmost.
    for (size_t offset = 0; offset < shape.size(); ++offset) {
      size_t commonIdx = commonShape.size() - 1 - offset;
      size_t shapeIdx = shape.size() - 1 - offset;

      if (commonShape[commonIdx] == 1) {
        commonShape[commonIdx] = shape[shapeIdx];
      }

      FUSILLI_RETURN_ERROR_IF((shape[shapeIdx] != 1) &&
                                  (commonShape[commonIdx] != shape[shapeIdx]),
                              ErrorCode::InvalidAttribute,
                              "Cannot broadcast two non unit dimensions");
    }
  }
  return ok(std::move(commonShape));
}

class TensorAttr {
public:
  using scalar_t = std::variant<int64_t, int32_t, float, double>;

  ErrorObject validate() const {
    FUSILLI_LOG_LABEL_ENDL("INFO: Validating tensor '" << name_ << "'");

    FUSILLI_RETURN_ERROR_IF(dim_.empty(), ErrorCode::AttributeNotSet,
                            "Tensor '" + name_ + "' dims not set");

    FUSILLI_RETURN_ERROR_IF(stride_.empty(), ErrorCode::AttributeNotSet,
                            "Tensor '" + name_ + "' strides not set");

    FUSILLI_RETURN_ERROR_IF(
        dim_.size() != stride_.size(), ErrorCode::InvalidAttribute,
        "Tensor '" + name_ +
            "' uses dim and stride of different dimensionality");

    FUSILLI_RETURN_ERROR_IF(dataType_ == DataType::NotSet,
                            ErrorCode::AttributeNotSet,
                            "Tensor '" + name_ + "' data type not set");

    FUSILLI_RETURN_ERROR_IF(
        isVirtual_ && isScalar_, ErrorCode::InvalidAttribute,
        "Tensor '" + name_ +
            "' cannot be both virtual (intermediate) and a scalar constant");

    FUSILLI_RETURN_ERROR_IF(
        scalarValue_.has_value() && !isScalar_, ErrorCode::InvalidAttribute,
        "Tensor '" + name_ +
            "' has a scalar value set but is not marked as a scalar");

    FUSILLI_RETURN_ERROR_IF(
        !scalarValue_.has_value() && isScalar_, ErrorCode::InvalidAttribute,
        "Tensor '" + name_ +
            "' is marked as a scalar but does not have a scalar value set");

    return ok();
  }

  TensorAttr() = default;

  // Constructors for scalar values:
  explicit TensorAttr(float value) {
    scalarValue_ = value;
    isScalar_ = true;
    dim_ = stride_ = {1};
    dataType_ = DataType::Float;
  }

  explicit TensorAttr(double value) {
    scalarValue_ = value;
    isScalar_ = true;
    dim_ = stride_ = {1};
    dataType_ = DataType::Double;
  }

  explicit TensorAttr(int32_t value) {
    scalarValue_ = value;
    isScalar_ = true;
    dim_ = stride_ = {1};
    dataType_ = DataType::Int32;
  }

  explicit TensorAttr(int64_t value) {
    scalarValue_ = value;
    isScalar_ = true;
    dim_ = stride_ = {1};
    dataType_ = DataType::Int64;
  }

  // Fill datatypes from overall context when not set.
  TensorAttr &fillFromContext(const Context &context) {
    if (getDataType() == DataType::NotSet) {
      if (isVirtual())
        setDataType(context.getIntermediateDataType());
      else
        setDataType(context.getIODataType());
    }
    return *this;
  }

  // MLIR assembly emitter helper methods:
  std::string getTensorTypeAsm(bool isValueTensor = true,
                               bool useLogicalDims = false) const;
  std::string getValueNameAsm(bool isOutputAliased = false) const;

  // Setters:
  TensorAttr &setName(const std::string &name) {
    name_ = name;
    return *this;
  }

  TensorAttr &setDataType(DataType dataType) {
    dataType_ = dataType;
    return *this;
  }

  TensorAttr &setDim(const std::vector<int64_t> &dim) {
    dim_ = dim;
    return *this;
  }
  template <Int64Range R> TensorAttr &setDim(R &&dim) {
    dim_.assign(dim.begin(), dim.end());
    return *this;
  }

  TensorAttr &setStride(const std::vector<int64_t> &stride) {
    stride_ = stride;
    return *this;
  }
  template <Int64Range R> TensorAttr &setStride(R &&stride) {
    stride_.assign(stride.begin(), stride.end());
    return *this;
  }

  TensorAttr &setIsVirtual(bool isVirtual) {
    isVirtual_ = isVirtual;
    return *this;
  }

  TensorAttr &setOutput(bool isOutput) { return setIsVirtual(!isOutput); }

  TensorAttr &setIsScalar(bool isScalar) {
    isScalar_ = isScalar;
    return *this;
  }

  // Getters:
  const std::string &getName() const { return name_; }

  DataType getDataType() const { return dataType_; }

  const std::vector<int64_t> &getDim() const { return dim_; }

  const std::vector<int64_t> &getStride() const { return stride_; }

  int64_t getVolume() const {
    int64_t volume = 1;
    for (const auto &d : dim_)
      volume *= d;
    return volume;
  }

  bool isVirtual() const { return isVirtual_; }

  bool isScalar() const { return isScalar_; }

  bool isContiguous() const {
    std::vector<int64_t> expectedStride =
        generateStrideFromDim(dim_, getContiguousStrideOrder(dim_.size()));
    return expectedStride == stride_;
  }

  bool isChannelsLast() const {
    std::vector<int64_t> expectedStride =
        generateStrideFromDim(dim_, getChannelsLastStrideOrder(dim_.size()));
    return expectedStride == stride_;
  }

  // Convert logical dims + stride into physical dims
  //  dims [N, C, H, W] + strideOrder [3, 2, 1, 0] -> [N, C, H, W]
  //  dims [N, C, H, W] + strideOrder [3, 0, 2, 1] -> [N, H, W, C]
  std::vector<int64_t> getPhysicalDim() const {
    size_t numDims = dim_.size();
    std::vector<int64_t> physicalDims(numDims);
    std::vector<size_t> strideOrder;
    if (isContiguous())
      strideOrder = getContiguousStrideOrder(numDims);
    else if (isChannelsLast())
      strideOrder = getChannelsLastStrideOrder(numDims);
    else
      assert(false && "TensorAttr::getPhysicalDim unexpected stride order");
    for (size_t i = 0; i < numDims; ++i)
      physicalDims[numDims - 1 - strideOrder[i]] = dim_[i];
    return physicalDims;
  }

  std::optional<scalar_t> getScalarValue() const { return scalarValue_; }

private:
  std::string name_;
  DataType dataType_ = DataType::NotSet;
  std::vector<int64_t> dim_ = {};
  std::vector<int64_t> stride_ = {};

  // Intermediate tensors that are not inputs/outputs are virtual
  // and not stored/read as they appear internal to the kernel.
  // They also don't need their shapes and sizes specified.
  bool isVirtual_ = false;

  // To represent scalar constants either obtained through
  // constant folding, or passed in as scalars during execution.
  bool isScalar_ = false;
  std::optional<scalar_t> scalarValue_ = std::nullopt;
};

// Sorting function for deterministic lookups on TensorAttr containers
// (`std::set`) ensuring iteration orders are deterministic. It sorts
// by name.
struct TensorAttrSortByName {
  bool operator()(const std::shared_ptr<TensorAttr> &a,
                  const std::shared_ptr<TensorAttr> &b) const {
    return a->getName() < b->getName();
  }
};

} // namespace fusilli

#endif // FUSILLI_ATTRIBUTES_TENSOR_ATTRIBUTES_H
