# Copyright 2025 Advanced Micro Devices, Inc.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

name: Release Llama 3.1 405B FP4 Docker Test

on:
  workflow_dispatch:
  pull_request:
  push:
   branches:
     - main
  schedule:
    # Weekdays at 11:00 AM UTC = 03:00 AM PST / 04:00 AM PDT
    - cron: "0 11 * * 1-5"

permissions:
  contents: write

concurrency:
  # A PR number if a pull request and otherwise the commit hash. This cancels
  # queued and in-progress runs for the same PR (presubmit) or commit
  # (postsubmit). The workflow name is prepended to avoid conflicts between
  # different workflows.
  group: ${{ github.workflow }}-${{ github.event.number || github.sha }}
  cancel-in-progress: true

jobs:
  test_docker_container:
    if: ${{ github.repository_owner == 'nod-ai' || github.event_name != 'schedule' }}
    timeout-minutes: 240
    name: "Test MI355 docker for llama"
    strategy:
      matrix:
        version: [3.11]
      fail-fast: false
    runs-on: linux-mi355-1gpu-ossci-nod-ai
    defaults:
      run:
        shell: bash
    env:
      VENV_DIR: ${{ github.workspace }}/.venv
    steps:
      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: "Setting up Python"
        id: setup_python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5.6.0
        with:
          python-version: ${{matrix.version}}

      - name: Create Python venv
        run: |
          echo "Check Artifacts"
          ls /shark-dev/llama3.1/405b/fp4/
          python -m venv ${VENV_DIR}
          source ${VENV_DIR}/bin/activate

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435 # v3.11.1

      - name: Build docker image
        working-directory: .
        run: |
          echo "Building docker image ${PWD}"
          ls scripts/docker/
          docker build --platform linux/amd64 --tag llama_405b_ci --file scripts/docker/llama_405b.dockerfile .

      - name: Run docker image
        run: |
          echo "Running docker image ... ${PATH}"
          docker run --name llama_docker_ci_test -v "/usr":"/usr" llama_405b_ci sh -c "echo Test"
          # docker run --name llama_docker_ci_test -d --network=host --device=/dev/kfd --device=/dev/dri --workdir $PWD --cap-add=SYS_PTRACE -p8000:8000 -p18000:18000 --security-opt seccomp=unconfined -e "HOME=/github/home" -e GITHUB_ACTIONS=true -e CI=true -v "/var/run/docker.sock":"/var/run/docker.sock" -v "/usr":"/usr" -v ${PWD}:/shark-ai -v /shark-dev/llama3.1/405b/fp4/:/artifacts/ llama_405b_ci
          # docker exec llama_docker_ci_test ls /artifacts
          # docker exec llama_docker_ci_test python3.11 --version
          # docker exec llama_docker_ci_test rocm-smi
          # docker exec llama_docker_ci_test pip install -f https://iree.dev/pip-release-links.html --upgrade --pre iree-base-compiler iree-base-runtime iree-turbine
          # docker exec llama_docker_ci_test iree-run-module --list_devices
          # docker rm -f llama_docker_ci_test
