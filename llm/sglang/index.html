<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<title id="head-title">index.html</title>
<style type="text/css">body {
  font-family: Helvetica, Arial, sans-serif;
  font-size: 12px;
  /* do not increase min-width as some may use split screens */
  min-width: 800px;
  color: #999;
}

h1 {
  font-size: 24px;
  color: black;
}

h2 {
  font-size: 16px;
  color: black;
}

p {
  color: black;
}

a {
  color: #999;
}

table {
  border-collapse: collapse;
}

/******************************
 * SUMMARY INFORMATION
 ******************************/
#environment td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  vertical-align: top;
}
#environment tr:nth-child(odd) {
  background-color: #f6f6f6;
}
#environment ul {
  margin: 0;
  padding: 0 20px;
}

/******************************
 * TEST RESULT COLORS
 ******************************/
span.passed,
.passed .col-result {
  color: green;
}

span.skipped,
span.xfailed,
span.rerun,
.skipped .col-result,
.xfailed .col-result,
.rerun .col-result {
  color: orange;
}

span.error,
span.failed,
span.xpassed,
.error .col-result,
.failed .col-result,
.xpassed .col-result {
  color: red;
}

.col-links__extra {
  margin-right: 3px;
}

/******************************
 * RESULTS TABLE
 *
 * 1. Table Layout
 * 2. Extra
 * 3. Sorting items
 *
 ******************************/
/*------------------
 * 1. Table Layout
 *------------------*/
#results-table {
  border: 1px solid #e6e6e6;
  color: #999;
  font-size: 12px;
  width: 100%;
}
#results-table th,
#results-table td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  text-align: left;
}
#results-table th {
  font-weight: bold;
}

/*------------------
 * 2. Extra
 *------------------*/
.logwrapper {
  max-height: 230px;
  overflow-y: scroll;
  background-color: #e6e6e6;
}
.logwrapper.expanded {
  max-height: none;
}
.logwrapper.expanded .logexpander:after {
  content: "collapse [-]";
}
.logwrapper .logexpander {
  z-index: 1;
  position: sticky;
  top: 10px;
  width: max-content;
  border: 1px solid;
  border-radius: 3px;
  padding: 5px 7px;
  margin: 10px 0 10px calc(100% - 80px);
  cursor: pointer;
  background-color: #e6e6e6;
}
.logwrapper .logexpander:after {
  content: "expand [+]";
}
.logwrapper .logexpander:hover {
  color: #000;
  border-color: #000;
}
.logwrapper .log {
  min-height: 40px;
  position: relative;
  top: -50px;
  height: calc(100% + 50px);
  border: 1px solid #e6e6e6;
  color: black;
  display: block;
  font-family: "Courier New", Courier, monospace;
  padding: 5px;
  padding-right: 80px;
  white-space: pre-wrap;
}

div.media {
  border: 1px solid #e6e6e6;
  float: right;
  height: 240px;
  margin: 0 5px;
  overflow: hidden;
  width: 320px;
}

.media-container {
  display: grid;
  grid-template-columns: 25px auto 25px;
  align-items: center;
  flex: 1 1;
  overflow: hidden;
  height: 200px;
}

.media-container--fullscreen {
  grid-template-columns: 0px auto 0px;
}

.media-container__nav--right,
.media-container__nav--left {
  text-align: center;
  cursor: pointer;
}

.media-container__viewport {
  cursor: pointer;
  text-align: center;
  height: inherit;
}
.media-container__viewport img,
.media-container__viewport video {
  object-fit: cover;
  width: 100%;
  max-height: 100%;
}

.media__name,
.media__counter {
  display: flex;
  flex-direction: row;
  justify-content: space-around;
  flex: 0 0 25px;
  align-items: center;
}

.collapsible td:not(.col-links) {
  cursor: pointer;
}
.collapsible td:not(.col-links):hover::after {
  color: #bbb;
  font-style: italic;
  cursor: pointer;
}

.col-result {
  width: 130px;
}
.col-result:hover::after {
  content: " (hide details)";
}

.col-result.collapsed:hover::after {
  content: " (show details)";
}

#environment-header h2:hover::after {
  content: " (hide details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

#environment-header.collapsed h2:hover::after {
  content: " (show details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

/*------------------
 * 3. Sorting items
 *------------------*/
.sortable {
  cursor: pointer;
}
.sortable.desc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: -12.5px;
  border: 10px solid #4caf50;
  border-bottom: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}
.sortable.asc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: 12.5px;
  border: 10px solid #4caf50;
  border-top: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}

.hidden, .summary__reload__button.hidden {
  display: none;
}

.summary__data {
  flex: 0 0 550px;
}
.summary__reload {
  flex: 1 1;
  display: flex;
  justify-content: center;
}
.summary__reload__button {
  flex: 0 0 300px;
  display: flex;
  color: white;
  font-weight: bold;
  background-color: #4caf50;
  text-align: center;
  justify-content: center;
  align-items: center;
  border-radius: 3px;
  cursor: pointer;
}
.summary__reload__button:hover {
  background-color: #46a049;
}
.summary__spacer {
  flex: 0 0 550px;
}

.controls {
  display: flex;
  justify-content: space-between;
}

.filters,
.collapse {
  display: flex;
  align-items: center;
}
.filters button,
.collapse button {
  color: #999;
  border: none;
  background: none;
  cursor: pointer;
  text-decoration: underline;
}
.filters button:hover,
.collapse button:hover {
  color: #ccc;
}

.filter__label {
  margin-right: 10px;
}

      </style>
</head>
<body>
<h1 id="title">index.html</h1>
<p>Report generated on 19-May-2025 at 11:10:17 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
<div id="environment-header">
<h2>Environment</h2>
</div>
<table id="environment"></table>
<!-- TEMPLATES -->
<template id="template_environment_row">
<tr>
<td></td>
<td></td>
</tr>
</template>
<template id="template_results-table__body--empty">
<tbody class="results-table-row">
<tr id="not-found-message">
<td colspan="4">No results found. Check the filters.
</td></tr>
</tbody></template>
<template id="template_results-table__tbody">
<tbody class="results-table-row">
<tr class="collapsible">
</tr>
<tr class="extras-row">
<td class="extra" colspan="4">
<div class="extraHTML"></div>
<div class="media">
<div class="media-container">
<div class="media-container__nav--left">&lt;</div>
<div class="media-container__viewport">
<img src=""/>
<video controls="">
<source src="" type="video/mp4"/>
</video>
</div>
<div class="media-container__nav--right">&gt;</div>
</div>
<div class="media__name"></div>
<div class="media__counter"></div>
</div>
<div class="logwrapper">
<div class="logexpander"></div>
<div class="log"></div>
</div>
</td>
</tr>
</tbody>
</template>
<!-- END TEMPLATES -->
<div class="summary">
<div class="summary__data">
<h2>Summary</h2>
<div class="additional-summary prefix">
</div>
<p class="run-count">18 tests ran in 439 seconds</p>
<p class="filter">(Un)check the boxes to filter the results.</p>
<div class="summary__reload">
<div class="summary__reload__button hidden" onclick="location.reload()">
<div>There are still tests running. <br/>Reload this page to get the latest results!</div>
</div>
</div>
<div class="summary__spacer"></div>
<div class="controls">
<div class="filters">
<input checked="true" class="filter" data-test-result="failed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="failed">0 Failed,</span>
<input checked="true" class="filter" data-test-result="passed" name="filter_checkbox" type="checkbox"/>
<span class="passed">18 Passed,</span>
<input checked="true" class="filter" data-test-result="skipped" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="skipped">0 Skipped,</span>
<input checked="true" class="filter" data-test-result="xfailed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xfailed">0 Expected failures,</span>
<input checked="true" class="filter" data-test-result="xpassed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xpassed">0 Unexpected passes,</span>
<input checked="true" class="filter" data-test-result="error" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="error">0 Errors,</span>
<input checked="true" class="filter" data-test-result="rerun" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="rerun">0 Reruns</span>
</div>
<div class="collapse">
<button id="show_all_details">Show all details</button> / <button id="hide_all_details">Hide all details</button>
</div>
</div>
</div>
<div class="additional-summary summary">
</div>
<div class="additional-summary postfix">
</div>
</div>
<table id="results-table">
<thead id="results-table-head">
<tr>
<th class="sortable" data-column-type="result">Result</th>
<th class="sortable" data-column-type="testId">Test</th>
<th class="sortable" data-column-type="duration">Duration</th>
<th>Links</th>
</tr>
</thead>
</table>
</body>
<footer>
<div data-jsonblob='{"environment": {"Python": "3.11.12", "Platform": "Linux-6.8.0-58-generic-x86_64-with-glibc2.35", "Packages": {"pytest": "8.0.0", "pluggy": "1.6.0"}, "Plugins": {"html": "4.1.1", "metadata": "3.1.1", "asyncio": "0.23.8", "anyio": "4.9.0", "timeout": "2.4.0", "xdist": "3.5.0"}, "CI": "true"}, "tests": {"reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]", "duration": "00:03:17", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:03:17&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stderr setup -----------------------------\nINFO:integration_tests.llm.model_management:Downloading model SanctumAI/Meta-Llama-3.1-8B-Instruct-GGUF from HuggingFace\nINFO:integration_tests.llm.model_management:Downloading tokenizer NousResearch/Meta-Llama-3.1-8B using transformers\nINFO:integration_tests.llm.model_management:Exporting model with following settings:\n  MLIR Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\n  Config Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json\n  Batch Sizes: 4\nINFO:integration_tests.llm.model_management:Running export command: python -m sharktank.examples.export_paged_llm_v1 --use-attention-mask --block-seq-stride=16 --gguf-file=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/meta-llama-3.1-8b-instruct.f16.gguf --output-mlir=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir --output-config=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json --bs-prefill=4 --bs-decode=4\nINFO:integration_tests.llm.model_management:Export succeeded.\nINFO:integration_tests.llm.model_management:Model successfully exported to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\nINFO:integration_tests.llm.model_management:Compiling model to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\nINFO:integration_tests.llm.model_management:Running compiler command: iree-compile /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir -o /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb --iree-hal-target-device=hip --iree-hip-target=gfx942\nINFO:integration_tests.llm.model_management:Compilation succeeded\nINFO:integration_tests.llm.model_management:Model successfully compiled to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\n[2025-05-19 11:07:34] Started server process [4410]\n[2025-05-19 11:07:34] Waiting for application startup.\n[2025-05-19 11:07:36] Application startup complete.\n[2025-05-19 11:07:36] Uvicorn running on http://0.0.0.0:41917 (Press CTRL+C to quit)\n[2025-05-19 11:07:37] 127.0.0.1:38564 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\n\n------------------------------ Captured log setup ------------------------------\nINFO     integration_tests.llm.model_management:model_management.py:263 Downloading model SanctumAI/Meta-Llama-3.1-8B-Instruct-GGUF from HuggingFace\nINFO     integration_tests.llm.model_management:model_management.py:385 Downloading tokenizer NousResearch/Meta-Llama-3.1-8B using transformers\nINFO     integration_tests.llm.model_management:model_management.py:452 Exporting model with following settings:\n  MLIR Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\n  Config Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json\n  Batch Sizes: 4\nINFO     integration_tests.llm.model_management:model_management.py:480 Running export command: python -m sharktank.examples.export_paged_llm_v1 --use-attention-mask --block-seq-stride=16 --gguf-file=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/meta-llama-3.1-8b-instruct.f16.gguf --output-mlir=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir --output-config=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json --bs-prefill=4 --bs-decode=4\nINFO     integration_tests.llm.model_management:model_management.py:486 Export succeeded.\nINFO     integration_tests.llm.model_management:model_management.py:493 Model successfully exported to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\nINFO     integration_tests.llm.model_management:model_management.py:499 Compiling model to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\nINFO     integration_tests.llm.model_management:model_management.py:510 Running compiler command: iree-compile /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir -o /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb --iree-hal-target-device=hip --iree-hip-target=gfx942\nINFO     integration_tests.llm.model_management:model_management.py:515 Compilation succeeded\nINFO     integration_tests.llm.model_management:model_management.py:522 Model successfully compiled to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\n\n----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-1...\n::group::Benchmark run on llama31_8b_none-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:41917&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=1, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_1_llama31_8b_none-1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json\n\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 0.00/642M [00:00&amp;lt;?, ?B/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 429k/642M [00:00&amp;lt;02:34, 4.34MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   1%|          | 6.01M/642M [00:00&amp;lt;00:18, 36.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   3%|\u258e         | 20.0M/642M [00:00&amp;lt;00:07, 86.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   5%|\u258c         | 34.6M/642M [00:00&amp;lt;00:05, 113MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   8%|\u258a         | 49.4M/642M [00:00&amp;lt;00:04, 128MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  10%|\u2588         | 64.3M/642M [00:00&amp;lt;00:04, 138MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  12%|\u2588\u258f        | 79.3M/642M [00:00&amp;lt;00:04, 144MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  15%|\u2588\u258d        | 94.3M/642M [00:00&amp;lt;00:03, 148MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 109M/642M [00:00&amp;lt;00:03, 151MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u2589        | 124M/642M [00:01&amp;lt;00:03, 153MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  22%|\u2588\u2588\u258f       | 139M/642M [00:01&amp;lt;00:03, 154MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  24%|\u2588\u2588\u258d       | 154M/642M [00:01&amp;lt;00:03, 155MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  26%|\u2588\u2588\u258b       | 169M/642M [00:01&amp;lt;00:03, 156MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  29%|\u2588\u2588\u258a       | 184M/642M [00:01&amp;lt;00:03, 156MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 199M/642M [00:01&amp;lt;00:02, 157MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 214M/642M [00:01&amp;lt;00:02, 157MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  36%|\u2588\u2588\u2588\u258c      | 230M/642M [00:01&amp;lt;00:02, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  38%|\u2588\u2588\u2588\u258a      | 245M/642M [00:01&amp;lt;00:02, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  40%|\u2588\u2588\u2588\u2588      | 260M/642M [00:01&amp;lt;00:02, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 275M/642M [00:02&amp;lt;00:02, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258c     | 290M/642M [00:02&amp;lt;00:02, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  48%|\u2588\u2588\u2588\u2588\u258a     | 305M/642M [00:02&amp;lt;00:02, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  50%|\u2588\u2588\u2588\u2588\u2589     | 320M/642M [00:02&amp;lt;00:02, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 335M/642M [00:02&amp;lt;00:02, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 350M/642M [00:02&amp;lt;00:01, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 365M/642M [00:02&amp;lt;00:01, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 381M/642M [00:02&amp;lt;00:01, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 396M/642M [00:02&amp;lt;00:01, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 411M/642M [00:02&amp;lt;00:01, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 426M/642M [00:03&amp;lt;00:01, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 441M/642M [00:03&amp;lt;00:01, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 456M/642M [00:03&amp;lt;00:01, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 471M/642M [00:03&amp;lt;00:01, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 486M/642M [00:03&amp;lt;00:01, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 501M/642M [00:03&amp;lt;00:00, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 516M/642M [00:03&amp;lt;00:00, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 532M/642M [00:03&amp;lt;00:00, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 547M/642M [00:03&amp;lt;00:00, 158MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 562M/642M [00:03&amp;lt;00:00, 157MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 577M/642M [00:04&amp;lt;00:00, 151MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 591M/642M [00:04&amp;lt;00:00, 150MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 606M/642M [00:04&amp;lt;00:00, 152MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 621M/642M [00:04&amp;lt;00:00, 154MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 636M/642M [00:04&amp;lt;00:00, 155MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642M/642M [00:04&amp;lt;00:00, 151MB/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:07:46] 127.0.0.1:50772 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:07:47] 127.0.0.1:50786 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:07:48] 127.0.0.1:50798 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:07:49] 127.0.0.1:50802 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:07:49] 127.0.0.1:50818 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:07:49] 127.0.0.1:50826 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:19,  2.18s/it][2025-05-19 11:07:49] 127.0.0.1:50832 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:07,  1.01it/s][2025-05-19 11:07:50] 127.0.0.1:50842 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:07:50] 127.0.0.1:50858 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:02&amp;lt;00:02,  2.15it/s][2025-05-19 11:07:50] 127.0.0.1:50872 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:07:51] 127.0.0.1:50880 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:02,  1.53it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:06&amp;lt;00:02,  1.10it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:07&amp;lt;00:01,  1.04it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:12&amp;lt;00:02,  2.13s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:13&amp;lt;00:00,  1.95s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:13&amp;lt;00:00,  1.39s/it]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    1         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  13.87     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      1467      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1518      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    139       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.43      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          105.79    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         109.46    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          215.25    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.57      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5940.09   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5402.10   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          274.20    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        259.12    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           425.82    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          21.34     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        21.66     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           25.72     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           22.39     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         24.72     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            36.11     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 23.857941150665283 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 1\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 13.8676363308914\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1467\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1518\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 139\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.4326620526264064\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 105.78587186715636\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 109.46349931448081\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5940.08979445789\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5402.1020490908995\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 3141.316477857459\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 10248.748539201915\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 274.20081380599487\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 259.1200874885544\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 125.35053160608255\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 425.82012159982696\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 21.33831321140287\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 21.656461804597\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 3.2268534825478676\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 25.724081398152872\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 22.393680353709087\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 24.717400432564318\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 16.702780299521017\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 36.11454333644359\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.570051443255319\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-1...\n::group::Benchmark run on llama31_8b_none-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 23.857941150665283 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 13.8676363308914\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1467\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1518\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 139\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.4326620526264064\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 105.78587186715636\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 109.46349931448081\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5940.08979445789\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5402.1020490908995\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 3141.316477857459\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 10248.748539201915\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 274.20081380599487\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 259.1200874885544\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 125.35053160608255\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 425.82012159982696\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 21.33831321140287\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 21.656461804597\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 3.2268534825478676\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 25.724081398152872\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 22.393680353709087\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 24.717400432564318\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 16.702780299521017\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 36.11454333644359\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.570051443255319\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]", "duration": "00:00:13", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:13&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-2...\n::group::Benchmark run on llama31_8b_none-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:41917&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=2, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_2_llama31_8b_none-2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:08:04] 127.0.0.1:36466 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:08:06] 127.0.0.1:45108 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:06] 127.0.0.1:45124 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:07] 127.0.0.1:45132 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:07] 127.0.0.1:45142 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:07] 127.0.0.1:45154 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:09,  1.09s/it][2025-05-19 11:08:07] 127.0.0.1:45156 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:07] 127.0.0.1:45160 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 30%|\u2588\u2588\u2588       | 3/10 [00:01&amp;lt;00:02,  3.02it/s][2025-05-19 11:08:07] 127.0.0.1:45166 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:01&amp;lt;00:01,  3.97it/s][2025-05-19 11:08:07] 127.0.0.1:45176 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:01&amp;lt;00:01,  4.18it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:01&amp;lt;00:00,  4.26it/s][2025-05-19 11:08:08] 127.0.0.1:45178 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:04&amp;lt;00:02,  1.14it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:01,  1.27it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.24s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.23it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    2         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     5         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  8.12      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      1037      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1169      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    116       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.62      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          127.72    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         143.97    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          271.69    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.38      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   3860.82   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 4036.74   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          254.58    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        214.58    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           421.82    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          15.93     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        16.10     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           17.22     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           15.42     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.61     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            18.43     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 12.898126363754272 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 2\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 8.119630825938657\nINFO:sglang_benchmarks.utils:COMPLETED: 5\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1037\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1169\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 116\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6157915436287071\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 127.71516614859387\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 143.97206290039173\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3860.8162447810173\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 4036.7387770675123\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1758.3279727330564\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6252.954249633476\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 254.58000795915726\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 214.57605785690248\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 141.9038458050684\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 421.8159115407616\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 15.931678440145022\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 16.096257008268633\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.8817644896077864\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 17.215834883254857\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 15.422124619387773\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.613000908866525\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 14.13058689482042\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 18.43414247967299\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.377457995040491\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-2...\n::group::Benchmark run on llama31_8b_none-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 12.898126363754272 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.119630825938657\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 5\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1037\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1169\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 116\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6157915436287071\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 127.71516614859387\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 143.97206290039173\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3860.8162447810173\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4036.7387770675123\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1758.3279727330564\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6252.954249633476\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 254.58000795915726\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 214.57605785690248\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 141.9038458050684\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 421.8159115407616\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 15.931678440145022\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 16.096257008268633\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.8817644896077864\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.215834883254857\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 15.422124619387773\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.613000908866525\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 14.13058689482042\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 18.43414247967299\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.377457995040491\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-4...\n::group::Benchmark run on llama31_8b_none-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:41917&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=4, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_4_llama31_8b_none-4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:08:22] 127.0.0.1:33540 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:08:23] 127.0.0.1:33546 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:23] 127.0.0.1:33550 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:24] 127.0.0.1:33562 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:24] 127.0.0.1:33572 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:24] 127.0.0.1:33584 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  1.82it/s][2025-05-19 11:08:24] 127.0.0.1:33594 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:24] 127.0.0.1:33606 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:24] 127.0.0.1:33612 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00&amp;lt;00:00,  7.40it/s][2025-05-19 11:08:24] 127.0.0.1:33626 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:24] 127.0.0.1:33642 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:00&amp;lt;00:00,  7.86it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:01,  1.48it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  1.98it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.27it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  4.40      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    88        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.91      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          204.49    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         170.15    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          374.64    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.89      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   3177.72   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 3796.61   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          272.21    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        272.41    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           430.65    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          15.92     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        15.85     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           17.04     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           15.54     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.61     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            17.49     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 13.669695854187012 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 4\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 4.3962495140731335\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 88\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.909866463947355\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 204.49248777216806\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 170.1450287581554\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3177.7210375876166\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3796.612612088211\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1360.6154730655405\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 4252.301832991652\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 272.2138835815713\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 272.40519307088107\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 158.28941762159434\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 430.6540852156468\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 15.921953044567015\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 15.850993987166138\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.7736578559712459\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 17.0446414565553\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 15.535268172146104\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.607728458940983\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 16.76160945506525\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 17.49409320764243\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.891301803880965\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-4...\n::group::Benchmark run on llama31_8b_none-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 13.669695854187012 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 4.3962495140731335\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 88\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.909866463947355\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 204.49248777216806\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 170.1450287581554\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3177.7210375876166\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3796.612612088211\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1360.6154730655405\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 4252.301832991652\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 272.2138835815713\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 272.40519307088107\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 158.28941762159434\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 430.6540852156468\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 15.921953044567015\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 15.850993987166138\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.7736578559712459\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.0446414565553\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 15.535268172146104\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.607728458940983\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 16.76160945506525\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 17.49409320764243\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.891301803880965\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-8...\n::group::Benchmark run on llama31_8b_none-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:41917&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=8, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_8_llama31_8b_none-8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:08:34] 127.0.0.1:59072 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:08:36] 127.0.0.1:49730 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:36] 127.0.0.1:49746 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:36] 127.0.0.1:49758 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:36] 127.0.0.1:49770 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:36] 127.0.0.1:49784 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.63it/s][2025-05-19 11:08:36] 127.0.0.1:49800 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:36] 127.0.0.1:49812 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:36] 127.0.0.1:49822 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:36] 127.0.0.1:49836 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:00&amp;lt;00:00, 15.39it/s][2025-05-19 11:08:36] 127.0.0.1:49852 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:03&amp;lt;00:01,  1.71it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.13it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.34it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    8         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  4.28      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    90        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.93      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          210.10    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         174.81    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          384.90    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.98      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   3182.83   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 3816.60   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          273.66    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        290.41    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           460.45    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          15.94     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        15.88     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           17.05     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           15.56     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.66     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            17.42     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 12.392823696136475 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 8\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 4.279006974073127\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 90\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.9347963263991729\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 210.09547435821412\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 174.80691303664534\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3182.8329901909456\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3816.603994462639\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1342.094669527773\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 4204.712811240461\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 273.6604134552181\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 290.4062974266708\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 188.09497592701996\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 460.44572833459824\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 15.936426911726995\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 15.877220747772498\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.7703329137785815\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 17.04559837480488\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 15.555110790848952\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.664345537312329\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 16.969023691922846\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 17.417264604009684\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.975300586772591\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-8...\n::group::Benchmark run on llama31_8b_none-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 12.392823696136475 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 4.279006974073127\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 90\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.9347963263991729\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 210.09547435821412\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 174.80691303664534\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3182.8329901909456\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3816.603994462639\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1342.094669527773\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 4204.712811240461\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 273.6604134552181\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 290.4062974266708\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 188.09497592701996\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 460.44572833459824\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 15.936426911726995\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 15.877220747772498\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.7703329137785815\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.04559837480488\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 15.555110790848952\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.664345537312329\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 16.969023691922846\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 17.417264604009684\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.975300586772591\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-16...\n::group::Benchmark run on llama31_8b_none-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:41917&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=16, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_16_llama31_8b_none-16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:08:47] 127.0.0.1:55340 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:08:48] 127.0.0.1:55342 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:48] 127.0.0.1:55354 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:48] 127.0.0.1:55370 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:48] 127.0.0.1:55380 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:48] 127.0.0.1:55382 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:01,  7.13it/s][2025-05-19 11:08:48] 127.0.0.1:55396 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:48] 127.0.0.1:55400 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:48] 127.0.0.1:55402 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:48] 127.0.0.1:55410 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:48] 127.0.0.1:55422 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:00&amp;lt;00:00,  7.06it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:03&amp;lt;00:01,  1.67it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:04&amp;lt;00:00,  1.85it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.11it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.33it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    16        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  4.29      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    98        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.93      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          209.69    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         174.47    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          384.15    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.98      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   3198.26   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 3832.84   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          273.65    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        282.19    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           442.87    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          16.04     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        15.96     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           17.24     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           15.64     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.71     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            17.92     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 12.440174579620361 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 16\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 4.287372868973762\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 98\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.9329722704891427\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 209.6855177924348\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 174.4658145814697\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3198.2639640336856\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3832.843957003206\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1360.842963988621\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 4246.483614181634\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 273.64805998513475\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 282.1870835032314\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 169.5934483546883\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 442.86623970605433\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 16.044945564316176\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 15.962156052186549\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.8191382849932579\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 17.23789270800981\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 15.638395120143231\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.710907475091517\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 16.669554733042236\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 17.919353810138997\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.9838915921481135\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-16...\n::group::Benchmark run on llama31_8b_none-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 12.440174579620361 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 4.287372868973762\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 98\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.9329722704891427\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 209.6855177924348\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 174.4658145814697\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3198.2639640336856\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3832.843957003206\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1360.842963988621\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 4246.483614181634\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 273.64805998513475\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 282.1870835032314\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 169.5934483546883\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 442.86623970605433\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 16.044945564316176\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 15.962156052186549\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.8191382849932579\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.23789270800981\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 15.638395120143231\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.710907475091517\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 16.669554733042236\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 17.919353810138997\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.9838915921481135\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]", "duration": "00:00:09", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:09&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-32...\n::group::Benchmark run on llama31_8b_none-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:41917&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=32, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_32_llama31_8b_none-32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:08:56] 127.0.0.1:44252 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:08:58] 127.0.0.1:44262 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:58] 127.0.0.1:44278 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:58] 127.0.0.1:44288 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:58] 127.0.0.1:44304 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:08:58] 127.0.0.1:44318 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:58] 127.0.0.1:44334 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:58] 127.0.0.1:44336 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:58] 127.0.0.1:44342 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:08:58] 127.0.0.1:44348 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:00&amp;lt;00:00, 49.79it/s][2025-05-19 11:08:58] 127.0.0.1:44360 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.02it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.35it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    32        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  4.25      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    89        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.94      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          211.71    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         176.15    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          387.85    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.94      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   3119.66   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 3747.69   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          363.59    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        359.70    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           396.00    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          15.24     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        14.83     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           16.62     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           14.74     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.61     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            17.28     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 9.406046628952026 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 32\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 4.246463591000065\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 89\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.9419602721845022\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 211.70557117346686\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 176.14657089850192\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3119.662706449162\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3747.692349483259\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1390.0232519518413\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 4221.000841911882\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 363.5909791919403\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 359.6960959257558\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 25.239352274024395\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 395.9996368549764\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 15.240161846147192\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 14.82901771873624\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.8318901717067636\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 16.617681670899696\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 14.736130218865737\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.60723439231515\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 1.4291638978906436\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 17.279385153669864\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.9385983320906934\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-32...\n::group::Benchmark run on llama31_8b_none-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41917\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 9.406046628952026 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 4.246463591000065\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 89\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.9419602721845022\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 211.70557117346686\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 176.14657089850192\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3119.662706449162\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3747.692349483259\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1390.0232519518413\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 4221.000841911882\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 363.5909791919403\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 359.6960959257558\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 25.239352274024395\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 395.9996368549764\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 15.240161846147192\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 14.82901771873624\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.8318901717067636\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 16.617681670899696\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 14.736130218865737\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.60723439231515\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 1.4291638978906436\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 17.279385153669864\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.9385983320906934\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]", "duration": "00:00:24", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:24&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stderr setup -----------------------------\n[2025-05-19 11:09:02] Shutting down\n[2025-05-19 11:09:02] Waiting for application shutdown.\n[2025-05-19 11:09:02] Application shutdown complete.\n[2025-05-19 11:09:02] Finished server process [4410]\n[2025-05-19 11:09:04] Started server process [5207]\n[2025-05-19 11:09:04] Waiting for application startup.\n[2025-05-19 11:09:06] Application startup complete.\n[2025-05-19 11:09:06] Uvicorn running on http://0.0.0.0:49673 (Press CTRL+C to quit)\n[2025-05-19 11:09:07] 127.0.0.1:59824 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\n\n----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-1...\n::group::Benchmark run on llama31_8b_trie-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:49673&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=1, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_1_llama31_8b_trie-1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:09:11] 127.0.0.1:59830 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:09:12] 127.0.0.1:59838 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:13] 127.0.0.1:59852 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:14] 127.0.0.1:59866 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:14] 127.0.0.1:59870 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:14] 127.0.0.1:59874 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:19,  2.18s/it][2025-05-19 11:09:15] 127.0.0.1:59882 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:07,  1.01it/s][2025-05-19 11:09:15] 127.0.0.1:59888 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:09:15] 127.0.0.1:59892 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:02&amp;lt;00:02,  2.15it/s][2025-05-19 11:09:15] 127.0.0.1:42718 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:16] 127.0.0.1:42724 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:02,  1.55it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:06&amp;lt;00:02,  1.10it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:07&amp;lt;00:01,  1.03it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:12&amp;lt;00:02,  2.15s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:13&amp;lt;00:00,  1.98s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:13&amp;lt;00:00,  1.40s/it]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    1         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  14.00     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      1467      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1518      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    140       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.43      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          104.80    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         108.44    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          213.23    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.56      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5980.48   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5437.77   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          274.30    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        257.04    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           430.72    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          21.44     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        21.81     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           25.94     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           22.55     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         25.06     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            36.27     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 19.01083207130432 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 1\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 13.998658298049122\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1467\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1518\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 140\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.4286125050167251\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 104.79575747658929\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 108.43896376923145\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5980.481032825385\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5437.766422983259\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 3182.301852768801\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 10371.907245216426\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 274.29904784852016\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 257.0361226098612\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 127.93774558204049\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 430.71840883931145\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 21.439530939560917\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 21.81058058124153\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 3.3740192786554033\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 25.936409973467537\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 22.55310128735592\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 25.06217046175152\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 16.755490718700187\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 36.27086196560413\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.5633089566843\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-1...\n::group::Benchmark run on llama31_8b_trie-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 19.01083207130432 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 13.998658298049122\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1467\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1518\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 140\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.4286125050167251\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 104.79575747658929\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 108.43896376923145\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5980.481032825385\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5437.766422983259\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 3182.301852768801\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 10371.907245216426\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 274.29904784852016\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 257.0361226098612\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 127.93774558204049\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 430.71840883931145\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 21.439530939560917\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 21.81058058124153\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 3.3740192786554033\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 25.936409973467537\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 22.55310128735592\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 25.06217046175152\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 16.755490718700187\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 36.27086196560413\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.5633089566843\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]", "duration": "00:00:13", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:13&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-2...\n::group::Benchmark run on llama31_8b_trie-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:49673&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=2, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_2_llama31_8b_trie-2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:09:30] 127.0.0.1:58664 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:09:31] 127.0.0.1:58678 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:32] 127.0.0.1:58690 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:32] 127.0.0.1:58702 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:32] 127.0.0.1:58708 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:32] 127.0.0.1:58710 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:09,  1.09s/it][2025-05-19 11:09:32] 127.0.0.1:58722 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:09:33] 127.0.0.1:58738 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 30%|\u2588\u2588\u2588       | 3/10 [00:01&amp;lt;00:02,  3.03it/s][2025-05-19 11:09:33] 127.0.0.1:58752 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:01&amp;lt;00:01,  3.97it/s][2025-05-19 11:09:33] 127.0.0.1:58758 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:01&amp;lt;00:01,  4.18it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:01&amp;lt;00:00,  4.21it/s][2025-05-19 11:09:33] 127.0.0.1:58766 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:04&amp;lt;00:02,  1.14it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:01,  1.27it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.23s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.24it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    2         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     5         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  8.09      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      1037      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1169      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    112       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.62      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          128.12    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         144.43    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          272.56    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.39      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   3865.36   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 4048.55   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          259.07    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        208.70    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           431.70    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          15.93     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        16.14     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           17.19     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           15.42     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.61     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            17.99     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 13.160514831542969 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 2\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 8.093708992004395\nINFO:sglang_benchmarks.utils:COMPLETED: 5\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1037\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1169\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 112\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6177637477378288\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 128.1242012808257\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 144.4331642211044\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3865.361006325111\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 4048.553251894191\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1748.748496383638\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6228.7590464018285\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 259.06916870735586\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 208.69825291447341\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 145.1448769468101\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 431.70327888801694\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 15.933953290900387\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 16.1352647090575\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.8900528350314013\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 17.190629556443643\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 15.423748169573926\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.611100777983665\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 14.07599775821912\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 17.990162391215538\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.387879901627066\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-2...\n::group::Benchmark run on llama31_8b_trie-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 13.160514831542969 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.093708992004395\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 5\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1037\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1169\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 112\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6177637477378288\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 128.1242012808257\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 144.4331642211044\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3865.361006325111\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4048.553251894191\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1748.748496383638\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6228.7590464018285\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 259.06916870735586\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 208.69825291447341\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 145.1448769468101\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 431.70327888801694\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 15.933953290900387\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 16.1352647090575\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.8900528350314013\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.190629556443643\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 15.423748169573926\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.611100777983665\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 14.07599775821912\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 17.990162391215538\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.387879901627066\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]", "duration": "00:00:09", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:09&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-4...\n::group::Benchmark run on llama31_8b_trie-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:49673&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=4, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_4_llama31_8b_trie-4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:09:43] 127.0.0.1:34498 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:09:44] 127.0.0.1:34502 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:44] 127.0.0.1:34508 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:45] 127.0.0.1:34514 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:45] 127.0.0.1:34530 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:45] 127.0.0.1:34540 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  1.83it/s][2025-05-19 11:09:45] 127.0.0.1:34546 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:09:45] 127.0.0.1:34552 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:09:45] 127.0.0.1:34556 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00&amp;lt;00:00,  7.40it/s][2025-05-19 11:09:45] 127.0.0.1:34568 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:09:45] 127.0.0.1:53456 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:00&amp;lt;00:00,  7.86it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:01,  1.48it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  1.97it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.27it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  4.41      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    89        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.91      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          203.86    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         169.62    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          373.48    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.89      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   3190.57   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 3813.61   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          271.19    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        274.21    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           427.18    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          16.04     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        15.91     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           17.32     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           15.61     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.65     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            18.85     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 9.220181465148926 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 4\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 4.409821483073756\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 89\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.9070661965236515\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 203.86312766869068\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 169.62137874992283\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3190.5680352356285\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3813.6066580191255\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1365.6599902225412\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 4266.211189602036\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 271.19000820675865\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 274.2074734997004\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 155.9775198437411\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 427.17783597530797\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 16.044772005327054\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 15.90614144068079\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.8479483759541071\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 17.32050641591157\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 15.610476057443588\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.650769531726837\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 16.840436224223577\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 18.846726249903433\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.894056412471121\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-4...\n::group::Benchmark run on llama31_8b_trie-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 9.220181465148926 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 4.409821483073756\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 89\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.9070661965236515\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 203.86312766869068\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 169.62137874992283\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3190.5680352356285\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3813.6066580191255\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1365.6599902225412\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 4266.211189602036\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 271.19000820675865\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 274.2074734997004\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 155.9775198437411\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 427.17783597530797\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 16.044772005327054\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 15.90614144068079\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.8479483759541071\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.32050641591157\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 15.610476057443588\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.650769531726837\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 16.840436224223577\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 18.846726249903433\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.894056412471121\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]", "duration": "00:00:09", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:09&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-8...\n::group::Benchmark run on llama31_8b_trie-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:49673&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=8, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_8_llama31_8b_trie-8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:09:52] 127.0.0.1:53464 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:09:54] 127.0.0.1:53478 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:54] 127.0.0.1:53482 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:54] 127.0.0.1:53498 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:54] 127.0.0.1:53514 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:09:54] 127.0.0.1:53516 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.61it/s][2025-05-19 11:09:54] 127.0.0.1:53524 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:09:54] 127.0.0.1:53540 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:09:54] 127.0.0.1:53550 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:09:54] 127.0.0.1:53564 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:00&amp;lt;00:00, 15.37it/s][2025-05-19 11:09:54] 127.0.0.1:53574 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:03&amp;lt;00:01,  1.71it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.13it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.33it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    8         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  4.29      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    91        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.93      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          209.59    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         174.39    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          383.98    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.97      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   3187.37   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 3823.41   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          267.02    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        284.06    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           450.84    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          16.02     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        15.92     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           17.22     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           15.62     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.66     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            18.94     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 9.20258903503418 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 8\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 4.28923052106984\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 91\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.9325682031662643\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 209.5947036616179\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 174.39025399209143\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3187.36549699679\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3823.4133925288916\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1347.810600535606\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 4214.380389498547\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 267.01547781703994\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 284.05953850597143\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 185.33611466086654\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 450.8386813872494\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 16.024194164909687\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 15.924659966993556\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.8109709867434115\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 17.221996422025345\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 15.616236855637002\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.663803041912615\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 16.701700613814577\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 18.940669386647635\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.972435714368444\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-8...\n::group::Benchmark run on llama31_8b_trie-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 9.20258903503418 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 4.28923052106984\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 91\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.9325682031662643\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 209.5947036616179\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 174.39025399209143\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3187.36549699679\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3823.4133925288916\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1347.810600535606\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 4214.380389498547\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 267.01547781703994\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 284.05953850597143\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 185.33611466086654\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 450.8386813872494\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 16.024194164909687\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 15.924659966993556\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.8109709867434115\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.221996422025345\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 15.616236855637002\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.663803041912615\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 16.701700613814577\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 18.940669386647635\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.972435714368444\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]", "duration": "00:00:09", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:09&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-16...\n::group::Benchmark run on llama31_8b_trie-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:49673&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=16, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_16_llama31_8b_trie-16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:10:01] 127.0.0.1:59638 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:10:03] 127.0.0.1:59642 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:10:03] 127.0.0.1:59646 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:10:03] 127.0.0.1:59656 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:10:03] 127.0.0.1:59660 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:10:03] 127.0.0.1:59674 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:01,  7.13it/s][2025-05-19 11:10:03] 127.0.0.1:59678 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:10:03] 127.0.0.1:59682 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:10:03] 127.0.0.1:59688 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:10:03] 127.0.0.1:59702 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:10:03] 127.0.0.1:59704 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:00&amp;lt;00:00,  7.12it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:03&amp;lt;00:01,  1.68it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:04&amp;lt;00:00,  1.86it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.12it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.35it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    16        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  4.25      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    94        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.94      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          211.36    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         175.86    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          387.23    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.98      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   3171.72   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 3799.86   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          270.29    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        278.99    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           440.39    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          15.91     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        15.84     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           17.08     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           15.51     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.56     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            17.84     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 9.393210172653198 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 16\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 4.2533073290251195\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 94\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.9404446212253421\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 211.36492862039563\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 175.86314416913896\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3171.723770792596\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3799.860098515637\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1349.1526869052848\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 4212.6010214095\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 270.2886357437819\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 278.9946449920535\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 170.43877437003965\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 440.3899359703064\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 15.912977992821968\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 15.842772337747016\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.8102103802968598\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 17.083948758955984\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 15.51387170865076\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.563213102519512\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 16.68665104380906\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 17.84167965175584\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.982830560254457\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-16...\n::group::Benchmark run on llama31_8b_trie-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 9.393210172653198 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 4.2533073290251195\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 94\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.9404446212253421\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 211.36492862039563\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 175.86314416913896\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3171.723770792596\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3799.860098515637\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1349.1526869052848\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 4212.6010214095\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 270.2886357437819\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 278.9946449920535\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 170.43877437003965\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 440.3899359703064\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 15.912977992821968\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 15.842772337747016\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.8102103802968598\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.083948758955984\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 15.51387170865076\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.563213102519512\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 16.68665104380906\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 17.84167965175584\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.982830560254457\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]", "duration": "00:00:09", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:09&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-32...\n::group::Benchmark run on llama31_8b_trie-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:49673&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=32, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_32_llama31_8b_trie-32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-19 11:10:10] 127.0.0.1:60686 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-19 11:10:12] 127.0.0.1:60690 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:10:12] 127.0.0.1:60704 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:10:12] 127.0.0.1:60718 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:10:12] 127.0.0.1:60724 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-19 11:10:12] 127.0.0.1:60736 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:10:12] 127.0.0.1:60742 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:10:12] 127.0.0.1:60758 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:10:12] 127.0.0.1:60770 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-19 11:10:12] 127.0.0.1:60772 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:00&amp;lt;00:00, 49.50it/s][2025-05-19 11:10:12] 127.0.0.1:60786 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.01it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&amp;lt;00:00,  2.35it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    32        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  4.26      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    88        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.94      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          211.02    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         175.58    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          386.60    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.94      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   3128.53   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 3759.08   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          361.59    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        357.32    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           394.58    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          15.29     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        14.89     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           16.65     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           14.79     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.63     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            18.18     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 9.08226490020752 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 32\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 4.26025648904033\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 88\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.9389106055680334\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 211.0201586014155\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 175.57628324122223\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3128.529064299073\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3759.081906522624\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1395.8996802018353\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 4234.811445968226\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 361.5893602836877\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 357.3177020298317\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 25.537887435370003\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 394.57776334136724\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 15.291568692610268\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 14.888234534292943\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.8178177870491181\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 16.645267496842294\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 14.794814823703334\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.633332029916346\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 1.4029537548804831\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 18.177695129998025\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.9374091182982354\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-32...\n::group::Benchmark run on llama31_8b_trie-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 9.08226490020752 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 4.26025648904033\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 88\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.9389106055680334\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 211.0201586014155\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 175.57628324122223\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3128.529064299073\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3759.081906522624\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1395.8996802018353\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 4234.811445968226\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 361.5893602836877\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 357.3177020298317\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 25.537887435370003\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 394.57776334136724\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 15.291568692610268\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 14.888234534292943\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.8178177870491181\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 16.645267496842294\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 14.794814823703334\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.633332029916346\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 1.4029537548804831\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 18.177695129998025\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.9374091182982354\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n--------------------------- Captured stderr teardown ---------------------------\n[2025-05-19 11:10:16] Shutting down\n[2025-05-19 11:10:17] Waiting for application shutdown.\n[2025-05-19 11:10:17] Application shutdown complete.\n[2025-05-19 11:10:17] Finished server process [5207]\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:49", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:49&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 0 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 1.001838207244873 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 2.0031418800354004 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 3.004467248916626 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 4.005827188491821 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 5.007530212402344 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 6.009521007537842 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 7.011228084564209 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 8.012801170349121 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 9.014491081237793 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 10.01614499092102 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 11.017751216888428 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 12.019875764846802 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 13.021530151367188 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 14.023151636123657 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 15.02446460723877 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 16.025713443756104 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 17.027004957199097 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 18.028374910354614 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 19.029411792755127 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 20.030466079711914 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 21.03163433074951 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 22.032626390457153 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 23.033963918685913 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 24.03497314453125 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 25.035956144332886 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0\nRequest Rate: 1\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b1269-78e2d5a848d045253deec405;7d54ffe1-54ae-400b-b8a0-b6d3ea3d64f8)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0&amp;#x27;, request_rate=1, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/sglang_10_1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json\n\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 0.00/642M [00:00&amp;lt;?, ?B/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   2%|\u258f         | 14.3M/642M [00:00&amp;lt;00:04, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   5%|\u258d         | 29.8M/642M [00:00&amp;lt;00:04, 157MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   7%|\u258b         | 45.5M/642M [00:00&amp;lt;00:03, 161MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  10%|\u2589         | 61.3M/642M [00:00&amp;lt;00:03, 163MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  12%|\u2588\u258f        | 76.9M/642M [00:00&amp;lt;00:03, 163MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  14%|\u2588\u258d        | 92.6M/642M [00:00&amp;lt;00:03, 164MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 108M/642M [00:00&amp;lt;00:03, 164MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u2589        | 124M/642M [00:00&amp;lt;00:03, 164MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  22%|\u2588\u2588\u258f       | 140M/642M [00:00&amp;lt;00:03, 164MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  24%|\u2588\u2588\u258d       | 156M/642M [00:01&amp;lt;00:03, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  27%|\u2588\u2588\u258b       | 171M/642M [00:01&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  29%|\u2588\u2588\u2589       | 187M/642M [00:01&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 203M/642M [00:01&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258d      | 219M/642M [00:01&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  37%|\u2588\u2588\u2588\u258b      | 234M/642M [00:01&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|\u2588\u2588\u2588\u2589      | 250M/642M [00:01&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588\u258f     | 266M/642M [00:01&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258d     | 282M/642M [00:01&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258b     | 298M/642M [00:01&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  49%|\u2588\u2588\u2588\u2588\u2589     | 313M/642M [00:02&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 329M/642M [00:02&amp;lt;00:01, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 345M/642M [00:02&amp;lt;00:01, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 361M/642M [00:02&amp;lt;00:01, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 376M/642M [00:02&amp;lt;00:01, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 392M/642M [00:02&amp;lt;00:01, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 408M/642M [00:02&amp;lt;00:01, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 424M/642M [00:02&amp;lt;00:01, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 439M/642M [00:02&amp;lt;00:01, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 455M/642M [00:02&amp;lt;00:01, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 471M/642M [00:03&amp;lt;00:01, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 487M/642M [00:03&amp;lt;00:00, 164MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 503M/642M [00:03&amp;lt;00:00, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 518M/642M [00:03&amp;lt;00:00, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 534M/642M [00:03&amp;lt;00:00, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 550M/642M [00:03&amp;lt;00:00, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 566M/642M [00:03&amp;lt;00:00, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 581M/642M [00:03&amp;lt;00:00, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 597M/642M [00:03&amp;lt;00:00, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 613M/642M [00:03&amp;lt;00:00, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 629M/642M [00:04&amp;lt;00:00, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642M/642M [00:04&amp;lt;00:00, 165MB/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:18,  2.11s/it]\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:08,  1.11s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.41it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:04&amp;lt;00:04,  1.24it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:05,  1.25s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:07&amp;lt;00:02,  1.03it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:07&amp;lt;00:01,  1.34it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:08&amp;lt;00:00,  1.19it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.60it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.18it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    1         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  8.49      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.18      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          230.98    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         326.90    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          557.88    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             3.85      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3270.33   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3576.21   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          26.89     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        22.04     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           47.41     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          11.81     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        11.85     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           14.42     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           11.73     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.35     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            22.49     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 18.331496715545654 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 1\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 8.48574756202288\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.1784465572313283\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 230.97552521734033\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 326.90107497597046\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3270.3277280088514\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3576.2100650463253\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1850.47368450747\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6027.692071918864\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 26.893279305659235\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 22.042697528377175\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 11.73357044157169\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 47.41352655692026\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 11.8140523712529\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 11.853308577081567\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.2664446778539578\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 14.422746150536073\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 11.734549748234844\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.350202072411776\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.6472709356641353\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 22.488986894022666\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.853906452090183\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 0 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 1.001838207244873 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 2.0031418800354004 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 3.004467248916626 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 4.005827188491821 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 5.007530212402344 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 6.009521007537842 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 7.011228084564209 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 8.012801170349121 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 9.014491081237793 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 10.01614499092102 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 11.017751216888428 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 12.019875764846802 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 13.021530151367188 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 14.023151636123657 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 15.02446460723877 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 16.025713443756104 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 17.027004957199097 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 18.028374910354614 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 19.029411792755127 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 20.030466079711914 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 21.03163433074951 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 22.032626390457153 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 23.033963918685913 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 24.03497314453125 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 25.035956144332886 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0\nRequest Rate: 1\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b1269-78e2d5a848d045253deec405;7d54ffe1-54ae-400b-b8a0-b6d3ea3d64f8)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0&amp;#x27;, request_rate=1, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/sglang_10_1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    1         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  8.49      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.18      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          230.98    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         326.90    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          557.88    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             3.85      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3270.33   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3576.21   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          26.89     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        22.04     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           47.41     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          11.81     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        11.85     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           14.42     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           11.73     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.35     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            22.49     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 18.331496715545654 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.48574756202288\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.1784465572313283\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 230.97552521734033\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 326.90107497597046\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3270.3277280088514\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3576.2100650463253\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1850.47368450747\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6027.692071918864\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 26.893279305659235\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 22.042697528377175\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 11.73357044157169\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 47.41352655692026\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 11.8140523712529\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 11.853308577081567\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.2664446778539578\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 14.422746150536073\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 11.734549748234844\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.350202072411776\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.6472709356641353\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 22.488986894022666\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.853906452090183\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1\nRequest Rate: 2\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b127c-31f6caa972f011404ef48514;bee257a1-afe1-4718-a14d-50933f8f3a8b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1&amp;#x27;, request_rate=2, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/sglang_10_2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:11,  1.28s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:06,  1.06it/s]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.22it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.65it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:04,  1.07s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:05&amp;lt;00:02,  1.30it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.59it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.61it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.35it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    2         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  7.39      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.35      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          265.06    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         375.14    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          640.19    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             4.63      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3426.97   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3812.89   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          26.40     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        20.41     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           51.81     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          12.60     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.57     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           14.37     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.30     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.50     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            25.77     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 11.7263822555542 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 2\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 7.394637817051262\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.3523312767179814\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 265.0569302367244\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 375.13669616156807\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3426.971907611005\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3812.8937005531043\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1867.4411539425898\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6116.586103469599\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 26.39559879899025\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 20.40644793305546\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 13.498772228050676\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 51.8071445543319\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 12.599651547472414\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.57351294633253\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.7425684459620087\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 14.370509217694217\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.303082832248474\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.499969569034874\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.5643488922161985\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 25.76781076146289\nINFO:sglang_benchmarks.utils:CONCURRENCY: 4.6344012950962465\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1\nRequest Rate: 2\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b127c-31f6caa972f011404ef48514;bee257a1-afe1-4718-a14d-50933f8f3a8b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1&amp;#x27;, request_rate=2, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/sglang_10_2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    2         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  7.39      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.35      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          265.06    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         375.14    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          640.19    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             4.63      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3426.97   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3812.89   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          26.40     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        20.41     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           51.81     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.60     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.57     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           14.37     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.30     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.50     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            25.77     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.7263822555542 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 7.394637817051262\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.3523312767179814\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 265.0569302367244\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 375.13669616156807\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3426.971907611005\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3812.8937005531043\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1867.4411539425898\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6116.586103469599\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 26.39559879899025\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 20.40644793305546\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 13.498772228050676\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 51.8071445543319\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 12.599651547472414\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.57351294633253\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.7425684459620087\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 14.370509217694217\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.303082832248474\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.499969569034874\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.5643488922161985\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 25.76781076146289\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 4.6344012950962465\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:11", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:11&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2\nRequest Rate: 4\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b1288-69bbfdb0259fe7e1522c7d3b;a8fce12f-9d2b-4b3b-9b96-71ff14dbb06b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2&amp;#x27;, request_rate=4, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/sglang_10_4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:06,  1.32it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:03,  2.60it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:08,  1.21s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.25it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:02,  1.68it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:03,  1.04it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.82it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.63it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.47it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    4         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.79      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.47      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          288.57    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         408.42    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          696.99    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.09      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3456.51   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3869.50   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          28.65     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        26.78     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           46.26     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          12.92     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.61     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           15.15     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.40     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.45     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            27.23     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 10.967340469360352 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 4\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.792017776984721\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.472316523358607\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 288.574038578287\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 408.4206035796776\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3456.506311125122\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3869.4984678877518\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1849.8128595885505\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6102.670162313152\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 28.64818323869258\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 26.779950014315546\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 11.130368681463352\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 46.26472947886214\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 12.919560590255397\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.61345291258857\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.0010693174828476\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 15.14987682348583\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.401785853981982\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.4467610148713\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.446731499284159\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 27.229223400354385\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.089071354962823\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2\nRequest Rate: 4\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b1288-69bbfdb0259fe7e1522c7d3b;a8fce12f-9d2b-4b3b-9b96-71ff14dbb06b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2&amp;#x27;, request_rate=4, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/sglang_10_4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    4         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.79      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.47      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          288.57    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         408.42    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          696.99    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.09      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3456.51   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3869.50   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          28.65     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        26.78     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           46.26     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.92     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.61     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           15.15     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.40     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.45     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            27.23     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.967340469360352 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.792017776984721\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.472316523358607\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 288.574038578287\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 408.4206035796776\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3456.506311125122\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3869.4984678877518\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1849.8128595885505\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6102.670162313152\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 28.64818323869258\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 26.779950014315546\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 11.130368681463352\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 46.26472947886214\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 12.919560590255397\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.61345291258857\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.0010693174828476\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 15.14987682348583\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.401785853981982\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.4467610148713\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.446731499284159\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 27.229223400354385\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.089071354962823\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:11", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:11&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3\nRequest Rate: 8\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b1294-7aafe97539431c3b057b6f82;7d070620-d9aa-42e3-97e4-8e98d32a81bf)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3&amp;#x27;, request_rate=8, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/sglang_10_8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  2.04it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.16it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:08,  1.24s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.25it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.60it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.16it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.03it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.69it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.55it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    8         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.45      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.55      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          303.99    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         430.24    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          734.24    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.37      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3465.27   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3890.60   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          30.07     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        28.18     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           51.01     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          13.53     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.74     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           18.92     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.43     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.40     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            27.37     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 10.620479822158813 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 8\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.447510466910899\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.5509862374509884\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 303.9933025403937\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 430.24358226890416\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3465.274866251275\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3890.5971840722486\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1825.0274157680315\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6054.274490673561\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 30.074407579377294\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 28.177669504657388\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 12.372920939996355\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 51.013307000976056\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 13.532632187283257\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.735228582352233\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.2246549474919974\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 18.920974067400675\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.42835086692184\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.397348531521857\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.86914550430384\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 27.369568599388\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.374593626540542\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3\nRequest Rate: 8\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b1294-7aafe97539431c3b057b6f82;7d070620-d9aa-42e3-97e4-8e98d32a81bf)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3&amp;#x27;, request_rate=8, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/sglang_10_8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    8         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.45      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.55      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          303.99    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         430.24    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          734.24    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.37      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3465.27   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3890.60   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          30.07     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        28.18     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           51.01     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.53     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.74     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           18.92     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.43     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.40     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            27.37     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.620479822158813 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.447510466910899\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.5509862374509884\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 303.9933025403937\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 430.24358226890416\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3465.274866251275\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3890.5971840722486\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1825.0274157680315\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6054.274490673561\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 30.074407579377294\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 28.177669504657388\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 12.372920939996355\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 51.013307000976056\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.532632187283257\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.735228582352233\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.2246549474919974\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 18.920974067400675\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.42835086692184\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.397348531521857\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.86914550430384\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 27.369568599388\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.374593626540542\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:11", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:11&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4\nRequest Rate: 16\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b129f-1dac22913220e0ab52962207;2076a93b-73d3-4d9f-aa23-0c2253e7d56c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4&amp;#x27;, request_rate=16, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/sglang_10_16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:03,  2.96it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.83it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:08,  1.21s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.19it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.57it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.23it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.25it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.07it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.60it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.58it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    16        \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.32      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.58      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          310.01    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         438.76    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          748.77    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.49      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3470.38   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3900.85   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          32.65     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        30.02     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           48.05     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          13.43     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.71     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           18.62     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.44     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.43     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            18.38     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 10.348217487335205 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 16\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.322345638880506\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.5816914435210623\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 310.0115229301282\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 438.76120643274265\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3470.379133755341\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3900.8514400338754\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1836.4261675092066\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6073.864127914421\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 32.645912282168865\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 30.02267354167998\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 11.086492367432315\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 48.04918211186305\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 13.425208640226662\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.712995744123923\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.0512620169190248\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 18.617181356146467\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.437514562072845\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.432723538950086\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 3.710478931266857\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 18.381801056675613\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.489068981634859\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4\nRequest Rate: 16\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b129f-1dac22913220e0ab52962207;2076a93b-73d3-4d9f-aa23-0c2253e7d56c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4&amp;#x27;, request_rate=16, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/sglang_10_16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    16        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.32      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.58      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          310.01    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         438.76    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          748.77    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.49      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3470.38   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3900.85   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          32.65     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        30.02     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           48.05     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.43     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.71     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           18.62     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.44     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.43     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            18.38     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.348217487335205 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.322345638880506\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.5816914435210623\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 310.0115229301282\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 438.76120643274265\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3470.379133755341\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3900.8514400338754\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1836.4261675092066\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6073.864127914421\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 32.645912282168865\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 30.02267354167998\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 11.086492367432315\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 48.04918211186305\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.425208640226662\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.712995744123923\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.0512620169190248\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 18.617181356146467\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.437514562072845\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.432723538950086\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.710478931266857\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 18.381801056675613\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.489068981634859\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:11", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:11&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5\nRequest Rate: 32\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b12aa-3beed4cf66f5aaba527199f1;e3f3b784-ddb1-461f-9cb1-82decb079d47)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5&amp;#x27;, request_rate=32, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/sglang_10_32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.82it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:01,  4.27it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:08,  1.19s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.17it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.57it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.26it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.30it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.12it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.62it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.61it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    32        \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.21      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.61      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          315.41    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         446.41    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          761.82    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.56      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3454.08   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3887.70   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          52.18     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        50.58     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           79.74     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          12.80     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.59     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           14.79     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.31     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.41     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            13.28     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 10.521034479141235 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 32\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.2140673531685024\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.6092519491120547\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 315.41338202596273\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 446.406490683684\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3454.0802085073665\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3887.6994645688683\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1832.310260108387\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6044.0535669913515\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 52.17715450562537\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 50.582578871399164\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 20.76532922276727\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 79.73538691876456\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 12.797346919651666\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.594467704820586\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.9144029778081\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 14.787574983132071\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.307880658591664\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.40986306220293\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 3.318770903944227\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 13.283328425604848\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.558485307929852\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5\nRequest Rate: 32\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-682b12aa-3beed4cf66f5aaba527199f1;e3f3b784-ddb1-461f-9cb1-82decb079d47)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5&amp;#x27;, request_rate=32, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/sglang_10_32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    32        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.21      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.61      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          315.41    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         446.41    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          761.82    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.56      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3454.08   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3887.70   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          52.18     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        50.58     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           79.74     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.80     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.59     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           14.79     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.31     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.41     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            13.28     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.521034479141235 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.2140673531685024\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.6092519491120547\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 315.41338202596273\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 446.406490683684\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3454.0802085073665\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3887.6994645688683\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1832.310260108387\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6044.0535669913515\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 52.17715450562537\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 50.582578871399164\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 20.76532922276727\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 79.73538691876456\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 12.797346919651666\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.594467704820586\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.9144029778081\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 14.787574983132071\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.307880658591664\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.40986306220293\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.318770903944227\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 13.283328425604848\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.558485307929852\n\n"}]}, "renderCollapsed": ["passed"], "initialSort": "result", "title": "shortfin_index.html"}' id="data-container"></div>
<script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
</footer>
</html>