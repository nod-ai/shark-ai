<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<title id="head-title">index.html</title>
<style type="text/css">body {
  font-family: Helvetica, Arial, sans-serif;
  font-size: 12px;
  /* do not increase min-width as some may use split screens */
  min-width: 800px;
  color: #999;
}

h1 {
  font-size: 24px;
  color: black;
}

h2 {
  font-size: 16px;
  color: black;
}

p {
  color: black;
}

a {
  color: #999;
}

table {
  border-collapse: collapse;
}

/******************************
 * SUMMARY INFORMATION
 ******************************/
#environment td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  vertical-align: top;
}
#environment tr:nth-child(odd) {
  background-color: #f6f6f6;
}
#environment ul {
  margin: 0;
  padding: 0 20px;
}

/******************************
 * TEST RESULT COLORS
 ******************************/
span.passed,
.passed .col-result {
  color: green;
}

span.skipped,
span.xfailed,
span.rerun,
.skipped .col-result,
.xfailed .col-result,
.rerun .col-result {
  color: orange;
}

span.error,
span.failed,
span.xpassed,
.error .col-result,
.failed .col-result,
.xpassed .col-result {
  color: red;
}

.col-links__extra {
  margin-right: 3px;
}

/******************************
 * RESULTS TABLE
 *
 * 1. Table Layout
 * 2. Extra
 * 3. Sorting items
 *
 ******************************/
/*------------------
 * 1. Table Layout
 *------------------*/
#results-table {
  border: 1px solid #e6e6e6;
  color: #999;
  font-size: 12px;
  width: 100%;
}
#results-table th,
#results-table td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  text-align: left;
}
#results-table th {
  font-weight: bold;
}

/*------------------
 * 2. Extra
 *------------------*/
.logwrapper {
  max-height: 230px;
  overflow-y: scroll;
  background-color: #e6e6e6;
}
.logwrapper.expanded {
  max-height: none;
}
.logwrapper.expanded .logexpander:after {
  content: "collapse [-]";
}
.logwrapper .logexpander {
  z-index: 1;
  position: sticky;
  top: 10px;
  width: max-content;
  border: 1px solid;
  border-radius: 3px;
  padding: 5px 7px;
  margin: 10px 0 10px calc(100% - 80px);
  cursor: pointer;
  background-color: #e6e6e6;
}
.logwrapper .logexpander:after {
  content: "expand [+]";
}
.logwrapper .logexpander:hover {
  color: #000;
  border-color: #000;
}
.logwrapper .log {
  min-height: 40px;
  position: relative;
  top: -50px;
  height: calc(100% + 50px);
  border: 1px solid #e6e6e6;
  color: black;
  display: block;
  font-family: "Courier New", Courier, monospace;
  padding: 5px;
  padding-right: 80px;
  white-space: pre-wrap;
}

div.media {
  border: 1px solid #e6e6e6;
  float: right;
  height: 240px;
  margin: 0 5px;
  overflow: hidden;
  width: 320px;
}

.media-container {
  display: grid;
  grid-template-columns: 25px auto 25px;
  align-items: center;
  flex: 1 1;
  overflow: hidden;
  height: 200px;
}

.media-container--fullscreen {
  grid-template-columns: 0px auto 0px;
}

.media-container__nav--right,
.media-container__nav--left {
  text-align: center;
  cursor: pointer;
}

.media-container__viewport {
  cursor: pointer;
  text-align: center;
  height: inherit;
}
.media-container__viewport img,
.media-container__viewport video {
  object-fit: cover;
  width: 100%;
  max-height: 100%;
}

.media__name,
.media__counter {
  display: flex;
  flex-direction: row;
  justify-content: space-around;
  flex: 0 0 25px;
  align-items: center;
}

.collapsible td:not(.col-links) {
  cursor: pointer;
}
.collapsible td:not(.col-links):hover::after {
  color: #bbb;
  font-style: italic;
  cursor: pointer;
}

.col-result {
  width: 130px;
}
.col-result:hover::after {
  content: " (hide details)";
}

.col-result.collapsed:hover::after {
  content: " (show details)";
}

#environment-header h2:hover::after {
  content: " (hide details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

#environment-header.collapsed h2:hover::after {
  content: " (show details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

/*------------------
 * 3. Sorting items
 *------------------*/
.sortable {
  cursor: pointer;
}
.sortable.desc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: -12.5px;
  border: 10px solid #4caf50;
  border-bottom: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}
.sortable.asc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: 12.5px;
  border: 10px solid #4caf50;
  border-top: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}

.hidden, .summary__reload__button.hidden {
  display: none;
}

.summary__data {
  flex: 0 0 550px;
}
.summary__reload {
  flex: 1 1;
  display: flex;
  justify-content: center;
}
.summary__reload__button {
  flex: 0 0 300px;
  display: flex;
  color: white;
  font-weight: bold;
  background-color: #4caf50;
  text-align: center;
  justify-content: center;
  align-items: center;
  border-radius: 3px;
  cursor: pointer;
}
.summary__reload__button:hover {
  background-color: #46a049;
}
.summary__spacer {
  flex: 0 0 550px;
}

.controls {
  display: flex;
  justify-content: space-between;
}

.filters,
.collapse {
  display: flex;
  align-items: center;
}
.filters button,
.collapse button {
  color: #999;
  border: none;
  background: none;
  cursor: pointer;
  text-decoration: underline;
}
.filters button:hover,
.collapse button:hover {
  color: #ccc;
}

.filter__label {
  margin-right: 10px;
}

      </style>
</head>
<body>
<h1 id="title">index.html</h1>
<p>Report generated on 12-May-2025 at 11:36:43 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
<div id="environment-header">
<h2>Environment</h2>
</div>
<table id="environment"></table>
<!-- TEMPLATES -->
<template id="template_environment_row">
<tr>
<td></td>
<td></td>
</tr>
</template>
<template id="template_results-table__body--empty">
<tbody class="results-table-row">
<tr id="not-found-message">
<td colspan="4">No results found. Check the filters.
</td></tr>
</tbody></template>
<template id="template_results-table__tbody">
<tbody class="results-table-row">
<tr class="collapsible">
</tr>
<tr class="extras-row">
<td class="extra" colspan="4">
<div class="extraHTML"></div>
<div class="media">
<div class="media-container">
<div class="media-container__nav--left">&lt;</div>
<div class="media-container__viewport">
<img src=""/>
<video controls="">
<source src="" type="video/mp4"/>
</video>
</div>
<div class="media-container__nav--right">&gt;</div>
</div>
<div class="media__name"></div>
<div class="media__counter"></div>
</div>
<div class="logwrapper">
<div class="logexpander"></div>
<div class="log"></div>
</div>
</td>
</tr>
</tbody>
</template>
<!-- END TEMPLATES -->
<div class="summary">
<div class="summary__data">
<h2>Summary</h2>
<div class="additional-summary prefix">
</div>
<p class="run-count">18 tests ran in 2275 seconds</p>
<p class="filter">(Un)check the boxes to filter the results.</p>
<div class="summary__reload">
<div class="summary__reload__button hidden" onclick="location.reload()">
<div>There are still tests running. <br/>Reload this page to get the latest results!</div>
</div>
</div>
<div class="summary__spacer"></div>
<div class="controls">
<div class="filters">
<input checked="true" class="filter" data-test-result="failed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="failed">0 Failed,</span>
<input checked="true" class="filter" data-test-result="passed" name="filter_checkbox" type="checkbox"/>
<span class="passed">18 Passed,</span>
<input checked="true" class="filter" data-test-result="skipped" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="skipped">0 Skipped,</span>
<input checked="true" class="filter" data-test-result="xfailed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xfailed">0 Expected failures,</span>
<input checked="true" class="filter" data-test-result="xpassed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xpassed">0 Unexpected passes,</span>
<input checked="true" class="filter" data-test-result="error" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="error">0 Errors,</span>
<input checked="true" class="filter" data-test-result="rerun" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="rerun">0 Reruns</span>
</div>
<div class="collapse">
<button id="show_all_details">Show all details</button> / <button id="hide_all_details">Hide all details</button>
</div>
</div>
</div>
<div class="additional-summary summary">
</div>
<div class="additional-summary postfix">
</div>
</div>
<table id="results-table">
<thead id="results-table-head">
<tr>
<th class="sortable" data-column-type="result">Result</th>
<th class="sortable" data-column-type="testId">Test</th>
<th class="sortable" data-column-type="duration">Duration</th>
<th>Links</th>
</tr>
</thead>
</table>
</body>
<footer>
<div data-jsonblob='{"environment": {"Python": "3.11.12", "Platform": "Linux-5.15.0-131-generic-x86_64-with-glibc2.35", "Packages": {"pytest": "8.0.0", "pluggy": "1.5.0"}, "Plugins": {"timeout": "2.4.0", "asyncio": "0.23.8", "metadata": "3.1.1", "xdist": "3.5.0", "anyio": "4.9.0", "html": "4.1.1"}, "CI": "true"}, "tests": {"reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]", "duration": "00:27:44", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:27:44&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stderr setup -----------------------------\nINFO:integration_tests.llm.model_management:Downloading model SanctumAI/Meta-Llama-3.1-8B-Instruct-GGUF from HuggingFace\nINFO:integration_tests.llm.model_management:Downloading tokenizer NousResearch/Meta-Llama-3.1-8B using transformers\nINFO:integration_tests.llm.model_management:Exporting model with following settings:\n  MLIR Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\n  Config Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json\n  Batch Sizes: 4\nINFO:integration_tests.llm.model_management:Running export command: python -m sharktank.examples.export_paged_llm_v1 --use-attention-mask --block-seq-stride=16 --gguf-file=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/meta-llama-3.1-8b-instruct.f16.gguf --output-mlir=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir --output-config=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json --bs-prefill=4 --bs-decode=4\nINFO:integration_tests.llm.model_management:Export succeeded.\nINFO:integration_tests.llm.model_management:Model successfully exported to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\nINFO:integration_tests.llm.model_management:Compiling model to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\nINFO:integration_tests.llm.model_management:Running compiler command: iree-compile /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir -o /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb --iree-hal-target-device=hip --iree-hip-target=gfx942\nINFO:integration_tests.llm.model_management:Compilation succeeded\nINFO:integration_tests.llm.model_management:Model successfully compiled to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\n[2025-05-12 11:33:03] Started server process [4456]\n[2025-05-12 11:33:03] Waiting for application startup.\n[2025-05-12 11:33:06] Application startup complete.\n[2025-05-12 11:33:06] Uvicorn running on http://0.0.0.0:36505 (Press CTRL+C to quit)\n[2025-05-12 11:33:07] 127.0.0.1:41944 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\n\n------------------------------ Captured log setup ------------------------------\nINFO     integration_tests.llm.model_management:model_management.py:263 Downloading model SanctumAI/Meta-Llama-3.1-8B-Instruct-GGUF from HuggingFace\nINFO     integration_tests.llm.model_management:model_management.py:385 Downloading tokenizer NousResearch/Meta-Llama-3.1-8B using transformers\nINFO     integration_tests.llm.model_management:model_management.py:452 Exporting model with following settings:\n  MLIR Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\n  Config Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json\n  Batch Sizes: 4\nINFO     integration_tests.llm.model_management:model_management.py:480 Running export command: python -m sharktank.examples.export_paged_llm_v1 --use-attention-mask --block-seq-stride=16 --gguf-file=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/meta-llama-3.1-8b-instruct.f16.gguf --output-mlir=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir --output-config=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json --bs-prefill=4 --bs-decode=4\nINFO     integration_tests.llm.model_management:model_management.py:486 Export succeeded.\nINFO     integration_tests.llm.model_management:model_management.py:493 Model successfully exported to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\nINFO     integration_tests.llm.model_management:model_management.py:499 Compiling model to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\nINFO     integration_tests.llm.model_management:model_management.py:510 Running compiler command: iree-compile /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir -o /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb --iree-hal-target-device=hip --iree-hip-target=gfx942\nINFO     integration_tests.llm.model_management:model_management.py:515 Compilation succeeded\nINFO     integration_tests.llm.model_management:model_management.py:522 Model successfully compiled to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\n\n----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-1...\n::group::Benchmark run on llama31_8b_none-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36505&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=1, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_1_llama31_8b_none-1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json\n\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 0.00/642M [00:00&amp;lt;?, ?B/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   1%|          | 4.39M/642M [00:00&amp;lt;00:14, 46.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   2%|\u258f         | 12.1M/642M [00:00&amp;lt;00:09, 66.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   3%|\u258e         | 19.3M/642M [00:00&amp;lt;00:09, 70.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   4%|\u258d         | 27.2M/642M [00:00&amp;lt;00:08, 75.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   6%|\u258c         | 35.8M/642M [00:00&amp;lt;00:07, 80.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   7%|\u258b         | 44.7M/642M [00:00&amp;lt;00:07, 85.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   8%|\u258a         | 54.0M/642M [00:00&amp;lt;00:06, 88.9MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  10%|\u2589         | 63.3M/642M [00:00&amp;lt;00:06, 91.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  11%|\u2588\u258f        | 72.8M/642M [00:00&amp;lt;00:06, 94.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  13%|\u2588\u258e        | 82.4M/642M [00:01&amp;lt;00:06, 96.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  14%|\u2588\u258d        | 92.0M/642M [00:01&amp;lt;00:05, 97.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|\u2588\u258c        | 102M/642M [00:01&amp;lt;00:05, 99.1MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 112M/642M [00:01&amp;lt;00:05, 100MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u2589        | 121M/642M [00:01&amp;lt;00:05, 101MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2588        | 131M/642M [00:01&amp;lt;00:05, 102MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  22%|\u2588\u2588\u258f       | 141M/642M [00:01&amp;lt;00:05, 102MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  24%|\u2588\u2588\u258e       | 151M/642M [00:01&amp;lt;00:05, 103MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  25%|\u2588\u2588\u258c       | 161M/642M [00:01&amp;lt;00:04, 103MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  27%|\u2588\u2588\u258b       | 171M/642M [00:01&amp;lt;00:04, 103MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  28%|\u2588\u2588\u258a       | 181M/642M [00:02&amp;lt;00:04, 103MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2589       | 191M/642M [00:02&amp;lt;00:04, 103MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 200M/642M [00:02&amp;lt;00:04, 103MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 210M/642M [00:02&amp;lt;00:04, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258d      | 220M/642M [00:02&amp;lt;00:04, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  36%|\u2588\u2588\u2588\u258c      | 230M/642M [00:02&amp;lt;00:04, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  37%|\u2588\u2588\u2588\u258b      | 240M/642M [00:02&amp;lt;00:04, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|\u2588\u2588\u2588\u2589      | 250M/642M [00:02&amp;lt;00:03, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588      | 260M/642M [00:02&amp;lt;00:03, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  42%|\u2588\u2588\u2588\u2588\u258f     | 270M/642M [00:02&amp;lt;00:03, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258e     | 280M/642M [00:03&amp;lt;00:03, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258c     | 290M/642M [00:03&amp;lt;00:03, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 300M/642M [00:03&amp;lt;00:03, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  48%|\u2588\u2588\u2588\u2588\u258a     | 310M/642M [00:03&amp;lt;00:03, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  50%|\u2588\u2588\u2588\u2588\u2589     | 320M/642M [00:03&amp;lt;00:03, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 329M/642M [00:03&amp;lt;00:03, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 339M/642M [00:03&amp;lt;00:03, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 349M/642M [00:03&amp;lt;00:02, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 359M/642M [00:03&amp;lt;00:02, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 369M/642M [00:03&amp;lt;00:02, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 379M/642M [00:04&amp;lt;00:02, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 389M/642M [00:04&amp;lt;00:02, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 399M/642M [00:04&amp;lt;00:02, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 409M/642M [00:04&amp;lt;00:02, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 419M/642M [00:04&amp;lt;00:02, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 429M/642M [00:04&amp;lt;00:02, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 439M/642M [00:04&amp;lt;00:02, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 449M/642M [00:04&amp;lt;00:01, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 459M/642M [00:04&amp;lt;00:01, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 469M/642M [00:04&amp;lt;00:01, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 479M/642M [00:05&amp;lt;00:01, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 489M/642M [00:05&amp;lt;00:01, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 499M/642M [00:05&amp;lt;00:01, 105MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 509M/642M [00:05&amp;lt;00:01, 105MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 518M/642M [00:05&amp;lt;00:01, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 528M/642M [00:05&amp;lt;00:01, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 538M/642M [00:05&amp;lt;00:01, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 548M/642M [00:05&amp;lt;00:00, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 558M/642M [00:05&amp;lt;00:00, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 568M/642M [00:05&amp;lt;00:00, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 578M/642M [00:06&amp;lt;00:00, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 588M/642M [00:06&amp;lt;00:00, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 598M/642M [00:06&amp;lt;00:00, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 608M/642M [00:06&amp;lt;00:00, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 618M/642M [00:06&amp;lt;00:00, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 628M/642M [00:06&amp;lt;00:00, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 638M/642M [00:06&amp;lt;00:00, 104MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642M/642M [00:06&amp;lt;00:00, 101MB/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:33:17] 127.0.0.1:53898 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:33:18] 127.0.0.1:53906 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:19] 127.0.0.1:53914 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:20] 127.0.0.1:53920 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:20] 127.0.0.1:53934 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:21] 127.0.0.1:53940 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:21] 127.0.0.1:53942 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:21] 127.0.0.1:53952 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:21,  2.44s/it][2025-05-12 11:33:21] 127.0.0.1:53956 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:08,  1.12s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:05,  1.38it/s][2025-05-12 11:33:21] 127.0.0.1:53964 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:22] 127.0.0.1:53972 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.41it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:05&amp;lt;00:05,  1.01s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:03,  1.31it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:07&amp;lt;00:03,  1.17s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:09&amp;lt;00:02,  1.48s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:09&amp;lt;00:01,  1.13s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:10&amp;lt;00:00,  1.11s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:10&amp;lt;00:00,  1.10s/it]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    1         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     7         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  10.98     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      1383      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1986      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    162       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.64      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          125.95    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         180.86    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          306.81    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.60      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5647.43   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5600.09   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          171.73    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        64.27     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           362.11    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          21.14     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        20.28     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           28.34     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           19.30     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         16.42     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            34.60     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 22.793010234832764 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 1\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 10.980737699195743\nINFO:sglang_benchmarks.utils:COMPLETED: 7\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1383\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1986\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 162\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6374799391221865\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 125.9478222579977\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 180.86216558523748\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5647.42920880339\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5600.09285248816\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2253.1044719124798\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 8568.347499575464\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 171.72520142048597\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 64.26607351750135\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 140.1002772540334\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 362.1122846379876\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 21.140665770867763\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 20.281906791774354\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 4.016635448515355\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 28.339906947233906\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 19.29685914671388\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 16.422358341515064\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 17.41466416011624\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 34.59561755880717\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.6001228282248428\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-1...\n::group::Benchmark run on llama31_8b_none-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 22.793010234832764 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 10.980737699195743\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 7\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1383\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1986\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 162\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6374799391221865\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 125.9478222579977\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 180.86216558523748\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5647.42920880339\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5600.09285248816\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2253.1044719124798\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 8568.347499575464\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 171.72520142048597\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 64.26607351750135\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 140.1002772540334\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 362.1122846379876\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 21.140665770867763\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 20.281906791774354\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 4.016635448515355\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 28.339906947233906\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 19.29685914671388\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 16.422358341515064\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 17.41466416011624\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 34.59561755880717\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.6001228282248428\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]", "duration": "00:00:17", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:17&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-2...\n::group::Benchmark run on llama31_8b_none-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36505&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=2, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_2_llama31_8b_none-2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:33:33] 127.0.0.1:45586 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:33:34] 127.0.0.1:45600 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:35] 127.0.0.1:45616 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:35] 127.0.0.1:45620 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:35] 127.0.0.1:45622 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:36] 127.0.0.1:45632 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:36] 127.0.0.1:45640 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:36] 127.0.0.1:45656 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:11,  1.22s/it][2025-05-12 11:33:36] 127.0.0.1:45658 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:01&amp;lt;00:04,  1.77it/s][2025-05-12 11:33:36] 127.0.0.1:43232 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 30%|\u2588\u2588\u2588       | 3/10 [00:01&amp;lt;00:02,  2.47it/s]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:01&amp;lt;00:02,  2.99it/s][2025-05-12 11:33:36] 127.0.0.1:43238 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:04&amp;lt;00:05,  1.19s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:05,  1.45s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:07&amp;lt;00:03,  1.32s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:08&amp;lt;00:02,  1.32s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:08&amp;lt;00:00,  1.06it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:11&amp;lt;00:00,  1.53s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:11&amp;lt;00:00,  1.18s/it]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    2         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     7         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  11.78     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      1091      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  2058      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    156       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.59      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          92.64     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         174.75    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          267.40    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.70      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   6222.36   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 7023.40   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          197.29    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        157.00    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           414.95    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          20.69     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        19.86     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           27.09     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           20.49     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         16.56     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            41.55     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 16.819616079330444 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 2\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 11.77652432769537\nINFO:sglang_benchmarks.utils:COMPLETED: 7\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1091\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2058\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 156\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.5944028819723823\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 92.64193489026702\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 174.7544472998804\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 6222.357359847852\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 7023.403124883771\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2821.4486049449356\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 10439.268041457979\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 197.29142556233066\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 157.0045780390501\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 135.7400197164149\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 414.94527868926525\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 20.6854259655587\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 19.86413936311301\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 3.6098982282754495\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 27.086157565620077\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 20.489397680928136\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 16.563027165830135\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 14.352408954439323\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 41.55185837298635\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.6985871473556275\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-2...\n::group::Benchmark run on llama31_8b_none-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 16.819616079330444 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 11.77652432769537\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 7\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1091\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2058\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 156\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.5944028819723823\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 92.64193489026702\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 174.7544472998804\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 6222.357359847852\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 7023.403124883771\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2821.4486049449356\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 10439.268041457979\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 197.29142556233066\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 157.0045780390501\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 135.7400197164149\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 414.94527868926525\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 20.6854259655587\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 19.86413936311301\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 3.6098982282754495\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 27.086157565620077\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 20.489397680928136\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 16.563027165830135\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 14.352408954439323\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 41.55185837298635\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.6985871473556275\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]", "duration": "00:00:20", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:20&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-4...\n::group::Benchmark run on llama31_8b_none-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36505&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=4, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_4_llama31_8b_none-4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:33:55] 127.0.0.1:48552 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:33:57] 127.0.0.1:51838 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:57] 127.0.0.1:51846 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:57] 127.0.0.1:51848 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:57] 127.0.0.1:51850 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:57] 127.0.0.1:51858 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:57] 127.0.0.1:51874 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:33:57] 127.0.0.1:51876 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:05,  1.63it/s][2025-05-12 11:33:57] 127.0.0.1:51892 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:33:58] 127.0.0.1:51906 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 30%|\u2588\u2588\u2588       | 3/10 [00:00&amp;lt;00:01,  4.58it/s][2025-05-12 11:33:58] 127.0.0.1:51914 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00&amp;lt;00:01,  5.35it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:01&amp;lt;00:01,  2.60it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:04,  1.07s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:02,  1.03s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:07&amp;lt;00:01,  1.21s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:09&amp;lt;00:00,  1.21s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:09&amp;lt;00:00,  1.10it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  9.11      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      953       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1637      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    134       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.66      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          104.57    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         179.63    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          284.20    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.42      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5193.78   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5118.61   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          267.72    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        319.29    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           434.07    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          20.45     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        18.24     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           31.47     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           18.05     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         15.14     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            33.34     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 19.70545768737793 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 4\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 9.113340752199292\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 953\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1637\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 134\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6583754698903407\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 104.57197046758245\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 179.6267740350813\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5193.784994694094\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5118.6096696183085\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2417.42476231182\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 8467.723824130373\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 267.7231236981849\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 319.29305428639054\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 161.2561292371706\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 434.0718035120517\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 20.45308613356552\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 18.235868210875964\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 5.674783338789567\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 31.47195381221464\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 18.052850927299634\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 15.139663591980934\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 14.119761387987321\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 33.33810560405253\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.4194606363911246\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-4...\n::group::Benchmark run on llama31_8b_none-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 19.70545768737793 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 9.113340752199292\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 953\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1637\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 134\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6583754698903407\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 104.57197046758245\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 179.6267740350813\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5193.784994694094\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5118.6096696183085\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2417.42476231182\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 8467.723824130373\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 267.7231236981849\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 319.29305428639054\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 161.2561292371706\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 434.0718035120517\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 20.45308613356552\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 18.235868210875964\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 5.674783338789567\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 31.47195381221464\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 18.052850927299634\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 15.139663591980934\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 14.119761387987321\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 33.33810560405253\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.4194606363911246\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-8...\n::group::Benchmark run on llama31_8b_none-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36505&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=8, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_8_llama31_8b_none-8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:34:10] 127.0.0.1:34054 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:34:12] 127.0.0.1:34066 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:12] 127.0.0.1:34076 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:12] 127.0.0.1:34084 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:12] 127.0.0.1:34096 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:12] 127.0.0.1:34098 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:12] 127.0.0.1:34108 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:12] 127.0.0.1:34114 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.18it/s][2025-05-12 11:34:12] 127.0.0.1:34128 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:34:12] 127.0.0.1:34138 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:34:12] 127.0.0.1:34154 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00&amp;lt;00:00, 10.14it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.19it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:06&amp;lt;00:03,  1.11s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:01,  1.08it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.27it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.11s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.14it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    8         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  8.80      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      953       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1637      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    140       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.68      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          108.32    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         186.06    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          294.38    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.70      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5426.49   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 6200.99   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          273.99    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        362.33    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           430.60    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          21.73     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        20.53     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           32.40     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           18.88     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         15.93     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            34.28     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 14.399193286895752 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 8\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 8.798026805743575\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 953\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1637\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 140\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6819710978924328\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 108.31974271524808\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 186.06444787498543\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5426.490584698816\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 6200.985359959304\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2305.5505476405747\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 8403.904126258567\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 273.98722742994624\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 362.3306327499449\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 159.80012650098698\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 430.59692163951695\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 21.728314804136584\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 20.527864795034137\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 6.119472238716494\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 32.4044003132349\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 18.882954733249395\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 15.930674970149994\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 14.21171163367101\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 34.277589395642266\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.7007097417500017\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-8...\n::group::Benchmark run on llama31_8b_none-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 14.399193286895752 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.798026805743575\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 953\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1637\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 140\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6819710978924328\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 108.31974271524808\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 186.06444787498543\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5426.490584698816\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 6200.985359959304\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2305.5505476405747\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 8403.904126258567\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 273.98722742994624\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 362.3306327499449\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 159.80012650098698\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 430.59692163951695\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 21.728314804136584\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 20.527864795034137\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 6.119472238716494\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 32.4044003132349\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 18.882954733249395\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 15.930674970149994\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 14.21171163367101\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 34.277589395642266\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.7007097417500017\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]", "duration": "00:00:13", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:13&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-16...\n::group::Benchmark run on llama31_8b_none-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36505&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=16, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_16_llama31_8b_none-16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:34:24] 127.0.0.1:47472 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:34:25] 127.0.0.1:47486 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:25] 127.0.0.1:47500 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:25] 127.0.0.1:47510 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:25] 127.0.0.1:47526 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:25] 127.0.0.1:47538 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:25] 127.0.0.1:47550 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:25] 127.0.0.1:47560 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:01,  6.26it/s][2025-05-12 11:34:25] 127.0.0.1:47574 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:34:25] 127.0.0.1:47582 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:34:25] 127.0.0.1:47594 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:01&amp;lt;00:01,  3.76it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:03&amp;lt;00:03,  1.22it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:04&amp;lt;00:02,  1.26it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:02,  1.01s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:07&amp;lt;00:01,  1.13s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.05s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.17it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    16        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  8.57      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      953       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1637      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    142       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.70      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          111.24    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         191.09    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          302.33    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.72      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5312.27   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5441.99   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          277.62    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        376.55    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           407.20    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          20.79     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        19.44     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           30.66     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           18.45     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         15.47     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            33.47     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 13.47948408126831 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 16\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 8.566785368137062\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 953\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1637\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 142\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.7003794004593772\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 111.24359477296441\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 191.08684642533342\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5312.273702739428\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5441.994157154113\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2426.2111233514524\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 8374.767507705837\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 277.62241568416357\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 376.5484574250877\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 161.67964933569894\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 407.20222196541727\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 20.785373834765903\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 19.43686333514489\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 5.302303487210123\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 30.66208228099199\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 18.450299429234455\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 15.47287032008171\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 13.572311368146586\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 33.46655182540415\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.7206070710007557\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-16...\n::group::Benchmark run on llama31_8b_none-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 13.47948408126831 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.566785368137062\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 953\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1637\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 142\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.7003794004593772\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 111.24359477296441\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 191.08684642533342\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5312.273702739428\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5441.994157154113\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2426.2111233514524\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 8374.767507705837\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 277.62241568416357\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 376.5484574250877\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 161.67964933569894\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 407.20222196541727\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 20.785373834765903\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 19.43686333514489\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 5.302303487210123\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 30.66208228099199\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 18.450299429234455\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 15.47287032008171\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 13.572311368146586\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 33.46655182540415\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.7206070710007557\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-32...\n::group::Benchmark run on llama31_8b_none-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36505&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=32, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_32_llama31_8b_none-32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:34:38] 127.0.0.1:49126 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:34:39] 127.0.0.1:49140 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:39] 127.0.0.1:49146 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:39] 127.0.0.1:49154 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:39] 127.0.0.1:49164 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:39] 127.0.0.1:49174 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:39] 127.0.0.1:49186 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:34:39] 127.0.0.1:49188 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:34:39] 127.0.0.1:49194 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:34:39] 127.0.0.1:49202 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 30%|\u2588\u2588\u2588       | 3/10 [00:00&amp;lt;00:00, 29.00it/s][2025-05-12 11:34:39] 127.0.0.1:49214 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.11it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:01,  1.17it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.27it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.04s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.16it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    32        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  8.62      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      953       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1637      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    137       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.70      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          110.55    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         189.90    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          300.45    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.83      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5509.25   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5997.62   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          265.06    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        354.99    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           397.89    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          22.38     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        22.00     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           32.54     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           19.22     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         16.25     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            33.80     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 13.827408075332642 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 32\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 8.620370706543326\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 953\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1637\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 137\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6960257515892764\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 110.5520902107634\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 189.8990258919409\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5509.2527794962125\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5997.6153038442135\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2249.0320321380714\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 8449.720483412966\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 265.06174355745316\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 354.9945424310863\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 154.310637902534\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 397.8912552818656\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 22.37795036569988\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 21.99515735306955\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 6.21092605094602\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 32.53883443221028\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 19.21899098773227\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 16.253676265478134\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 13.845331133006113\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 33.7981928884983\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.8345818065441613\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-32...\n::group::Benchmark run on llama31_8b_none-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36505\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 13.827408075332642 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.620370706543326\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 953\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1637\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 137\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6960257515892764\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 110.5520902107634\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 189.8990258919409\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5509.2527794962125\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5997.6153038442135\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2249.0320321380714\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 8449.720483412966\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 265.06174355745316\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 354.9945424310863\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 154.310637902534\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 397.8912552818656\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 22.37795036569988\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 21.99515735306955\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 6.21092605094602\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 32.53883443221028\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 19.21899098773227\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 16.253676265478134\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 13.845331133006113\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 33.7981928884983\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.8345818065441613\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]", "duration": "00:00:32", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:32&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stderr setup -----------------------------\n[2025-05-12 11:34:48] Shutting down\n[2025-05-12 11:34:48] Waiting for application shutdown.\n[2025-05-12 11:34:48] Application shutdown complete.\n[2025-05-12 11:34:48] Finished server process [4456]\n[2025-05-12 11:34:50] Started server process [5474]\n[2025-05-12 11:34:50] Waiting for application startup.\n[2025-05-12 11:34:52] Application startup complete.\n[2025-05-12 11:34:52] Uvicorn running on http://0.0.0.0:58119 (Press CTRL+C to quit)\n[2025-05-12 11:34:53] 127.0.0.1:35864 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\n\n----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-1...\n::group::Benchmark run on llama31_8b_trie-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:58119&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=1, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_1_llama31_8b_trie-1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:35:07] 127.0.0.1:34254 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:35:08] 127.0.0.1:34268 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:09] 127.0.0.1:34278 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:10] 127.0.0.1:34294 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:10] 127.0.0.1:34298 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:10] 127.0.0.1:34314 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:10] 127.0.0.1:34324 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:10] 127.0.0.1:34330 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:21,  2.44s/it][2025-05-12 11:35:11] 127.0.0.1:34340 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:09,  1.13s/it][2025-05-12 11:35:11] 127.0.0.1:34350 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:12] 127.0.0.1:34358 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.41it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:05&amp;lt;00:05,  1.15s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:03,  1.18it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:06&amp;lt;00:02,  1.11it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:08&amp;lt;00:02,  1.10s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:10&amp;lt;00:01,  1.29s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:12&amp;lt;00:00,  1.49s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:12&amp;lt;00:00,  1.21s/it]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    1         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     7         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  12.13     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      1383      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1986      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    163       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.58      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          113.97    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         163.66    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          277.64    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.31      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5735.52   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 6271.12   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          181.40    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        64.74     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           375.76    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          19.91     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        19.63     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           23.84     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           19.57     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         16.05     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            35.26     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 27.13570475578308 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 1\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 12.13458875194192\nINFO:sglang_benchmarks.utils:COMPLETED: 7\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1383\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1986\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 163\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.5768633896949971\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 113.97172399259729\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 163.66438456203775\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5735.515700387104\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 6271.121024154127\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2552.981839852036\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 9636.903344076127\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 181.4008170206632\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 64.74406644701958\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 144.27005159217833\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 375.7571548037231\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 19.908723739941596\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 19.625455401837826\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.468744441021996\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 23.83965864897906\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 19.573107082771898\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 16.051342710852623\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 17.49454962848904\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 35.25922736153017\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.3086090285741796\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-1...\n::group::Benchmark run on llama31_8b_trie-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 27.13570475578308 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 12.13458875194192\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 7\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1383\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1986\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 163\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.5768633896949971\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 113.97172399259729\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 163.66438456203775\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5735.515700387104\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 6271.121024154127\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2552.981839852036\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 9636.903344076127\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 181.4008170206632\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 64.74406644701958\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 144.27005159217833\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 375.7571548037231\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 19.908723739941596\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 19.625455401837826\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.468744441021996\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 23.83965864897906\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 19.573107082771898\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 16.051342710852623\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 17.49454962848904\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 35.25922736153017\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.3086090285741796\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]", "duration": "00:00:25", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:25&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-2...\n::group::Benchmark run on llama31_8b_trie-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:58119&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=2, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_2_llama31_8b_trie-2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:35:34] 127.0.0.1:36894 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:35:35] 127.0.0.1:36906 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:35] 127.0.0.1:36910 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:36] 127.0.0.1:51468 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:36] 127.0.0.1:51482 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:36] 127.0.0.1:51488 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:36] 127.0.0.1:51494 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:36] 127.0.0.1:51496 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:11,  1.22s/it][2025-05-12 11:35:36] 127.0.0.1:51512 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:01&amp;lt;00:04,  1.77it/s][2025-05-12 11:35:37] 127.0.0.1:51528 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 30%|\u2588\u2588\u2588       | 3/10 [00:01&amp;lt;00:02,  2.47it/s][2025-05-12 11:35:37] 127.0.0.1:51532 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:01&amp;lt;00:02,  2.90it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:02&amp;lt;00:01,  2.87it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:04,  1.00s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:04&amp;lt;00:02,  1.33it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:01,  1.01it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.12it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:10&amp;lt;00:00,  1.75s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:10&amp;lt;00:00,  1.05s/it]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    2         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  10.50     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      953       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1637      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    135       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.57      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          90.76     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         155.89    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          246.65    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.89      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5056.08   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 4824.11   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          237.65    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        218.82    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           443.47    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          20.24     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        17.64     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           31.47     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           17.66     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         14.95     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            32.73     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 25.435156106948853 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 2\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 10.500738130882382\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 953\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1637\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 135\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.5713884038641213\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 90.75552481375125\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 155.89380285426108\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5056.078975709776\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 4824.10737266764\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2386.451840139658\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 9147.622374258937\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 237.64644113058844\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 218.8168759457767\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 161.72655898992386\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 443.46929979510605\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 20.242227778374485\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 17.64413946142342\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 5.7140922957522795\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 31.468325546942648\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 17.658717981519754\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 14.946880750358105\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 14.163199955451086\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 32.732858993113034\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.88898489574175\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-2...\n::group::Benchmark run on llama31_8b_trie-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 25.435156106948853 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 10.500738130882382\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 953\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1637\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 135\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.5713884038641213\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 90.75552481375125\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 155.89380285426108\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5056.078975709776\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4824.10737266764\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2386.451840139658\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 9147.622374258937\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 237.64644113058844\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 218.8168759457767\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 161.72655898992386\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 443.46929979510605\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 20.242227778374485\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 17.64413946142342\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 5.7140922957522795\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 31.468325546942648\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 17.658717981519754\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 14.946880750358105\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 14.163199955451086\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 32.732858993113034\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.88898489574175\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-4...\n::group::Benchmark run on llama31_8b_trie-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:58119&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=4, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_4_llama31_8b_trie-4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:35:49] 127.0.0.1:56202 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:35:51] 127.0.0.1:56218 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:51] 127.0.0.1:56226 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:51] 127.0.0.1:56236 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:51] 127.0.0.1:56240 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:51] 127.0.0.1:56246 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:51] 127.0.0.1:56256 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:35:51] 127.0.0.1:56268 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:05,  1.63it/s][2025-05-12 11:35:51] 127.0.0.1:56276 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:35:51] 127.0.0.1:56280 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 30%|\u2588\u2588\u2588       | 3/10 [00:00&amp;lt;00:01,  4.59it/s][2025-05-12 11:35:52] 127.0.0.1:56282 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00&amp;lt;00:01,  5.36it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:01&amp;lt;00:01,  2.56it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:04,  1.02s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:04&amp;lt;00:02,  1.18it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:01,  1.04it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:07&amp;lt;00:01,  1.32s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:09&amp;lt;00:00,  1.28s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:09&amp;lt;00:00,  1.10it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  9.08      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      953       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1637      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    140       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.66      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          104.99    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         180.35    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          285.34    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.39      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5135.29   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 4843.88   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          260.48    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        325.95    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           434.69    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          20.57     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        17.93     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           31.87     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           17.86     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         15.08     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            32.32     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 14.165853261947632 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 4\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 9.07695335894823\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 953\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1637\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 140\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6610147439046922\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 104.99117515686194\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 180.34685596199685\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5135.294859918456\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 4843.882599379867\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2340.5547207305804\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 8432.038063090295\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 260.48343162983656\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 325.950363650918\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 155.58826204235834\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 434.68902762979275\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 20.565461299462765\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 17.92948897506593\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 5.8402879770578275\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 31.874054646901435\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 17.86286819660234\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 15.082372352480888\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 14.309074537786195\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 32.31559976935386\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.39450561670408\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-4...\n::group::Benchmark run on llama31_8b_trie-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 14.165853261947632 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 9.07695335894823\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 953\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1637\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 140\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6610147439046922\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 104.99117515686194\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 180.34685596199685\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5135.294859918456\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4843.882599379867\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2340.5547207305804\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 8432.038063090295\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 260.48343162983656\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 325.950363650918\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 155.58826204235834\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 434.68902762979275\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 20.565461299462765\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 17.92948897506593\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 5.8402879770578275\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 31.874054646901435\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 17.86286819660234\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 15.082372352480888\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 14.309074537786195\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 32.31559976935386\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.39450561670408\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]", "duration": "00:00:15", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:15&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-8...\n::group::Benchmark run on llama31_8b_trie-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:58119&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=8, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_8_llama31_8b_trie-8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:36:03] 127.0.0.1:55282 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:36:05] 127.0.0.1:55284 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:05] 127.0.0.1:55288 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:05] 127.0.0.1:55290 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:05] 127.0.0.1:55300 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:05] 127.0.0.1:55306 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:05] 127.0.0.1:55322 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:05] 127.0.0.1:55324 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.23it/s][2025-05-12 11:36:05] 127.0.0.1:55338 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:36:05] 127.0.0.1:55348 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:36:05] 127.0.0.1:55354 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00&amp;lt;00:00, 10.24it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.18it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:05&amp;lt;00:02,  1.12it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:01,  1.13it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.33it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:10&amp;lt;00:00,  1.57s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:10&amp;lt;00:00,  1.01s/it]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    8         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  10.11     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      953       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1637      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    138       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.59      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          94.31     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         161.99    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          256.30    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.18      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5357.81   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5541.98   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          280.21    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        372.76    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           439.69    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          21.23     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        18.52     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           33.13     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           18.61     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         15.71     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            34.10     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 15.123331069946289 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 8\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 10.105290575884283\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 953\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1637\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 138\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.5937483890189826\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 94.30703578918174\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 161.99435213734577\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5357.808756176382\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5541.975075844675\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2571.9464757022406\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 9623.56184218079\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 280.21182973558706\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 372.7634660899639\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 165.217163333302\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 439.687180146575\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 21.229441865763434\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 18.521534297579876\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 6.192578586102829\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 33.13154410124283\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 18.606182582057425\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 15.710590407252312\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 14.51268390783517\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 34.09968957304954\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.181190317651526\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-8...\n::group::Benchmark run on llama31_8b_trie-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 15.123331069946289 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 10.105290575884283\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 953\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1637\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 138\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.5937483890189826\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 94.30703578918174\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 161.99435213734577\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5357.808756176382\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5541.975075844675\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2571.9464757022406\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 9623.56184218079\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 280.21182973558706\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 372.7634660899639\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 165.217163333302\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 439.687180146575\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 21.229441865763434\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 18.521534297579876\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 6.192578586102829\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 33.13154410124283\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 18.606182582057425\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 15.710590407252312\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 14.51268390783517\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 34.09968957304954\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.181190317651526\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-16...\n::group::Benchmark run on llama31_8b_trie-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:58119&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=16, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_16_llama31_8b_trie-16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:36:18] 127.0.0.1:40924 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:36:20] 127.0.0.1:40932 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:20] 127.0.0.1:40938 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:20] 127.0.0.1:40940 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:20] 127.0.0.1:40954 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:20] 127.0.0.1:40966 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:20] 127.0.0.1:40970 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:20] 127.0.0.1:40982 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:01,  6.29it/s][2025-05-12 11:36:20] 127.0.0.1:40988 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:36:20] 127.0.0.1:40990 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:36:20] 127.0.0.1:41002 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:01&amp;lt;00:01,  3.69it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:03&amp;lt;00:03,  1.21it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:06&amp;lt;00:03,  1.15s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:01,  1.08it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.08s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.11it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    16        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  8.98      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      953       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1637      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    136       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.67      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          106.17    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         182.37    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          288.54    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.63      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5437.78   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 6176.79   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          272.22    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        372.44    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           402.85    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          21.61     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        20.25     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           31.65     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           18.93     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         16.24     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            34.64     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 13.951540231704712 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 16\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 8.976351444609463\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 953\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1637\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 136\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6684230265519694\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 106.16785738400448\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 182.368082410929\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5437.776051461697\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 6176.793834660202\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2376.022046105089\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 8703.737859474495\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 272.2172898550828\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 372.4417178891599\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 157.37684083878509\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 402.85370550118387\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 21.61163816600155\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 20.250149574302153\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 5.924515344321883\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 31.653571569658506\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 18.93087267413708\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 16.23807940632105\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 13.997604815181056\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 34.639542028307915\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.634734726029845\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-16...\n::group::Benchmark run on llama31_8b_trie-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 13.951540231704712 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.976351444609463\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 953\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1637\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 136\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6684230265519694\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 106.16785738400448\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 182.368082410929\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5437.776051461697\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 6176.793834660202\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2376.022046105089\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 8703.737859474495\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 272.2172898550828\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 372.4417178891599\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 157.37684083878509\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 402.85370550118387\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 21.61163816600155\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 20.250149574302153\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 5.924515344321883\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 31.653571569658506\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 18.93087267413708\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 16.23807940632105\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 13.997604815181056\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 34.639542028307915\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.634734726029845\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-32...\n::group::Benchmark run on llama31_8b_trie-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:58119&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=32, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_32_llama31_8b_trie-32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-05-12 11:36:33] 127.0.0.1:56918 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-05-12 11:36:34] 127.0.0.1:56926 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:34] 127.0.0.1:56928 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:34] 127.0.0.1:56934 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:34] 127.0.0.1:56940 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:34] 127.0.0.1:56942 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:34] 127.0.0.1:56950 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-05-12 11:36:34] 127.0.0.1:56960 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:36:34] 127.0.0.1:56962 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-05-12 11:36:34] 127.0.0.1:56968 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 30%|\u2588\u2588\u2588       | 3/10 [00:00&amp;lt;00:00, 29.04it/s][2025-05-12 11:36:34] 127.0.0.1:56976 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.16it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:01,  1.16it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.30it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.08s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.15it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    32        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  8.69      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      953       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1637      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    139       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.69      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          109.70    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         188.44    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          298.15    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.71      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   5367.78   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5913.81   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          262.58    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        363.22    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           378.76    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          19.57     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        17.95     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           25.36     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           18.71     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         15.99     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            34.66     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 14.137275218963623 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 32\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 8.687021828256547\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 953\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1637\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 139\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6906854982778577\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 109.7038799764664\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 188.44202678014219\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 5367.782327036063\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5913.805585354567\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2381.3297777017633\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 8506.11659991555\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 262.5815096932153\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 363.22025768458843\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 153.3497844617369\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 378.7614774890244\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 19.56610080101641\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 17.94780930267555\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 3.9638773383458332\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 25.360496829736775\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 18.70933641032723\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 15.991689637303352\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 12.66086867616315\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 34.65616978704929\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.7074494111959817\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-32...\n::group::Benchmark run on llama31_8b_trie-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:58119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 14.137275218963623 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.687021828256547\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 953\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1637\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 139\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6906854982778577\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 109.7038799764664\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 188.44202678014219\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 5367.782327036063\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5913.805585354567\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2381.3297777017633\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 8506.11659991555\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 262.5815096932153\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 363.22025768458843\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 153.3497844617369\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 378.7614774890244\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 19.56610080101641\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 17.94780930267555\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 3.9638773383458332\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 25.360496829736775\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 18.70933641032723\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 15.991689637303352\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 12.66086867616315\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 34.65616978704929\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.7074494111959817\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n--------------------------- Captured stderr teardown ---------------------------\n[2025-05-12 11:36:43] Shutting down\n[2025-05-12 11:36:43] Waiting for application shutdown.\n[2025-05-12 11:36:43] Application shutdown complete.\n[2025-05-12 11:36:43] Finished server process [5474]\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]", "duration": "00:05:58", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:05:58&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 0 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 1.0027644634246826 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 2.005537986755371 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 3.00883412361145 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 4.011110067367554 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 5.013396978378296 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 6.016204118728638 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 7.0183422565460205 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 8.021005153656006 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 9.024004220962524 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 10.026884078979492 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 11.029734373092651 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 12.03339672088623 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 13.036310911178589 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 14.039863348007202 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 15.044543504714966 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 16.04663372039795 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 17.049084663391113 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 18.052741289138794 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 19.055694818496704 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 20.058908224105835 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 21.062407970428467 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 22.065725088119507 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 23.073348999023438 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 24.076596975326538 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 25.079514980316162 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 26.08271288871765 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 27.0849187374115 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 28.087331533432007 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 29.09062957763672 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 30.093969106674194 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 31.096092224121094 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 32.098259925842285 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 33.10020351409912 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 34.10257911682129 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 35.10529041290283 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 36.10723090171814 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 37.109349489212036 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 38.1116259098053 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 39.11374092102051 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 40.11586785316467 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 41.118582010269165 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 42.12103724479675 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 43.12319540977478 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 44.12515425682068 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 45.12720561027527 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 46.12935662269592 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 47.13183069229126 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 48.133991718292236 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 49.13620734214783 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 50.138524770736694 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 51.14052772521973 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 52.142560958862305 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 53.14522337913513 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 54.14732527732849 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 55.149657011032104 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 56.15199160575867 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 57.15406537055969 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 58.156192779541016 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 59.15878653526306 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 60.16103219985962 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 61.163318395614624 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 62.16525983810425 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 63.167245864868164 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 64.1698248386383 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 65.17213368415833 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 66.17452359199524 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 67.17691826820374 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 68.17936849594116 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 69.18149089813232 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 70.18439936637878 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 71.18666243553162 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 72.18893694877625 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 73.19117951393127 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 74.19313359260559 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 75.19576644897461 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 76.19833612442017 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 77.20049715042114 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 78.20435619354248 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 79.20632457733154 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 80.2082450389862 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 81.21024990081787 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 82.21247148513794 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 83.21441888809204 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 84.21637320518494 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 85.21903896331787 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 86.22158765792847 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 87.22428059577942 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 88.22724509239197 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 89.22956919670105 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 90.23157835006714 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 91.23390245437622 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 92.23608303070068 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 93.23812937736511 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 94.24120903015137 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 95.24327397346497 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 96.2458381652832 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 97.24825119972229 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 98.25093483924866 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 99.25294971466064 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 100.25548052787781 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 101.25751781463623 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 102.25949025154114 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 103.26156711578369 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 104.26377248764038 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 105.26615071296692 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 106.26816177368164 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 107.27009868621826 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 108.27200222015381 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 109.27393198013306 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 110.27661728858948 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 111.27906799316406 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 112.28118348121643 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 113.28320384025574 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 114.28534841537476 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 115.28734397888184 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 116.28973627090454 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 117.29232597351074 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 118.2943127155304 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 119.29629921913147 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 120.29865336418152 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 121.30116868019104 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 122.30377912521362 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 123.30740165710449 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 124.31108808517456 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 125.31548953056335 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 126.31857442855835 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 127.32106804847717 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 128.3253276348114 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 129.32917594909668 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 130.33185362815857 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 131.3350248336792 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 132.33720326423645 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 133.33984470367432 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 134.34301328659058 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 135.34635257720947 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 136.34887647628784 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 137.3522493839264 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 138.35579633712769 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 139.358295917511 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 140.36085295677185 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 141.36370849609375 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 142.36598944664001 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 143.36933064460754 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 144.37130403518677 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 145.37449288368225 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 146.37700176239014 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 147.3802375793457 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 148.38250184059143 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 149.3860569000244 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 150.38816237449646 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 151.3904893398285 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 152.3929557800293 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 153.39507055282593 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 154.39732837677002 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 155.3993000984192 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 156.4015018939972 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 157.40350580215454 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 158.40657830238342 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 159.4092080593109 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 160.41207098960876 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 161.4147527217865 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 162.41681790351868 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 163.41901397705078 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 164.42175388336182 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 165.42370223999023 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 166.4259991645813 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 167.42848825454712 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 168.43075299263 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 169.43307185173035 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 170.43538284301758 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 171.43756771087646 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 172.43955254554749 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 173.44178104400635 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 174.44429540634155 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 175.44649934768677 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 176.44899606704712 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 177.45134973526 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 178.4532699584961 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 179.45522141456604 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 180.45733952522278 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 181.45929837226868 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 182.46165084838867 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 183.4640531539917 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 184.4660894870758 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 185.4686324596405 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 186.4707133769989 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 187.47362422943115 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 188.47581458091736 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 189.4778106212616 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 190.48050332069397 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 191.48263144493103 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 192.48473238945007 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 193.48723816871643 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 194.48975229263306 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 195.49191188812256 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 196.49439454078674 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 197.4964690208435 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 198.49919033050537 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 199.50177788734436 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 200.50390458106995 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 201.50603199005127 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 202.50806856155396 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 203.51003217697144 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 204.51197934150696 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 205.51428079605103 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 206.51628255844116 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 207.5182421207428 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 208.52018928527832 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 209.52216410636902 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 210.52410292625427 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 211.52660512924194 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 212.52859234809875 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 213.53052520751953 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 214.53261947631836 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 215.53467178344727 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 216.53704929351807 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 217.53932571411133 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 218.5414218902588 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 219.54398107528687 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 220.54659509658813 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 221.54925847053528 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 222.551691532135 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 223.5549201965332 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 224.5569200515747 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 225.55911374092102 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 226.56166434288025 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 227.563782453537 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 228.5663137435913 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 229.56835508346558 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 230.5703706741333 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 231.57227158546448 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 232.57412815093994 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 233.57616901397705 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 234.57857728004456 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 235.58051705360413 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 236.5827236175537 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 237.58505535125732 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 238.5871295928955 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 239.5890917778015 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 240.5917627811432 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 241.5936849117279 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 242.5959689617157 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 243.5989866256714 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 244.60097765922546 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 245.60298871994019 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 246.6053764820099 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 247.60878205299377 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 248.6108250617981 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0\nRequest Rate: 1\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821db2c-3ad8b3563950890026038cbf;a550943b-1281-41b4-9cb5-fccdfd069f35)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0&amp;#x27;, request_rate=1, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/sglang_10_1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json\n\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 0.00/642M [00:00&amp;lt;?, ?B/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   1%|          | 3.97M/642M [00:00&amp;lt;00:16, 41.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   2%|\u258f         | 11.3M/642M [00:00&amp;lt;00:10, 62.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   3%|\u258e         | 17.8M/642M [00:00&amp;lt;00:10, 65.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   4%|\u258d         | 25.4M/642M [00:00&amp;lt;00:09, 70.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   5%|\u258c         | 32.2M/642M [00:00&amp;lt;00:09, 71.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   6%|\u258c         | 39.0M/642M [00:00&amp;lt;00:09, 65.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   7%|\u258b         | 45.6M/642M [00:00&amp;lt;00:09, 67.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   8%|\u258a         | 52.1M/642M [00:00&amp;lt;00:09, 67.3MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   9%|\u2589         | 58.6M/642M [00:00&amp;lt;00:11, 54.9MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  10%|\u2589         | 64.2M/642M [00:01&amp;lt;00:13, 46.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  11%|\u2588         | 69.0M/642M [00:01&amp;lt;00:15, 39.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  11%|\u2588\u258f        | 73.2M/642M [00:01&amp;lt;00:17, 34.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  12%|\u2588\u258f        | 76.7M/642M [00:01&amp;lt;00:19, 30.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  12%|\u2588\u258f        | 79.9M/642M [00:01&amp;lt;00:21, 27.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  13%|\u2588\u258e        | 82.6M/642M [00:02&amp;lt;00:23, 24.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  13%|\u2588\u258e        | 85.1M/642M [00:02&amp;lt;00:25, 23.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  14%|\u2588\u258e        | 87.3M/642M [00:02&amp;lt;00:27, 20.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  14%|\u2588\u258d        | 89.4M/642M [00:02&amp;lt;00:30, 18.9MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  14%|\u2588\u258d        | 91.2M/642M [00:02&amp;lt;00:33, 17.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  14%|\u2588\u258d        | 92.8M/642M [00:02&amp;lt;00:36, 15.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  15%|\u2588\u258d        | 94.3M/642M [00:02&amp;lt;00:39, 14.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  15%|\u2588\u258d        | 95.7M/642M [00:03&amp;lt;00:43, 13.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  15%|\u2588\u258c        | 96.9M/642M [00:03&amp;lt;00:47, 12.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  15%|\u2588\u258c        | 98.1M/642M [00:03&amp;lt;00:48, 11.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  15%|\u2588\u258c        | 99.2M/642M [00:03&amp;lt;00:49, 11.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|\u2588\u258c        | 100M/642M [00:03&amp;lt;00:48, 11.6MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|\u2588\u258c        | 101M/642M [00:03&amp;lt;00:49, 11.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|\u2588\u258c        | 102M/642M [00:03&amp;lt;00:50, 11.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|\u2588\u258c        | 104M/642M [00:03&amp;lt;00:51, 10.9MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|\u2588\u258b        | 105M/642M [00:03&amp;lt;00:54, 10.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|\u2588\u258b        | 106M/642M [00:04&amp;lt;00:56, 9.92MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 107M/642M [00:04&amp;lt;00:59, 9.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 107M/642M [00:04&amp;lt;01:01, 9.05MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 108M/642M [00:04&amp;lt;01:03, 8.84MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 109M/642M [00:04&amp;lt;01:03, 8.77MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 110M/642M [00:04&amp;lt;01:05, 8.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 111M/642M [00:04&amp;lt;01:07, 8.24MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 112M/642M [00:04&amp;lt;01:06, 8.37MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  18%|\u2588\u258a        | 113M/642M [00:04&amp;lt;01:04, 8.61MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  18%|\u2588\u258a        | 113M/642M [00:05&amp;lt;01:02, 8.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  18%|\u2588\u258a        | 114M/642M [00:05&amp;lt;01:00, 9.16MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  18%|\u2588\u258a        | 115M/642M [00:05&amp;lt;00:58, 9.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  18%|\u2588\u258a        | 116M/642M [00:05&amp;lt;00:57, 9.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  18%|\u2588\u258a        | 117M/642M [00:05&amp;lt;00:56, 9.70MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  18%|\u2588\u258a        | 118M/642M [00:05&amp;lt;00:55, 9.91MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u258a        | 119M/642M [00:05&amp;lt;00:53, 10.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u258a        | 120M/642M [00:05&amp;lt;00:53, 10.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u2589        | 121M/642M [00:05&amp;lt;00:52, 10.3MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u2589        | 122M/642M [00:05&amp;lt;00:52, 10.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u2589        | 123M/642M [00:06&amp;lt;00:53, 10.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u2589        | 124M/642M [00:06&amp;lt;00:56, 9.61MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2589        | 125M/642M [00:06&amp;lt;01:00, 8.98MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2589        | 126M/642M [00:06&amp;lt;01:04, 8.38MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2589        | 127M/642M [00:06&amp;lt;01:09, 7.78MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2589        | 128M/642M [00:06&amp;lt;01:12, 7.42MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2589        | 128M/642M [00:06&amp;lt;01:13, 7.33MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2588        | 129M/642M [00:06&amp;lt;01:14, 7.26MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2588        | 130M/642M [00:06&amp;lt;01:15, 7.12MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2588        | 130M/642M [00:07&amp;lt;01:16, 6.97MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2588        | 131M/642M [00:07&amp;lt;01:16, 7.00MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|\u2588\u2588        | 132M/642M [00:07&amp;lt;01:15, 7.12MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|\u2588\u2588        | 132M/642M [00:07&amp;lt;01:14, 7.18MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|\u2588\u2588        | 133M/642M [00:07&amp;lt;01:13, 7.22MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|\u2588\u2588        | 134M/642M [00:07&amp;lt;01:11, 7.42MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|\u2588\u2588        | 135M/642M [00:07&amp;lt;01:09, 7.66MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|\u2588\u2588        | 136M/642M [00:07&amp;lt;01:08, 7.80MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|\u2588\u2588\u258f       | 137M/642M [00:07&amp;lt;00:57, 9.19MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  22%|\u2588\u2588\u258f       | 143M/642M [00:08&amp;lt;00:19, 27.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  24%|\u2588\u2588\u258e       | 151M/642M [00:08&amp;lt;00:11, 43.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  24%|\u2588\u2588\u258d       | 156M/642M [00:08&amp;lt;00:11, 44.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  25%|\u2588\u2588\u258d       | 160M/642M [00:08&amp;lt;00:12, 41.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  26%|\u2588\u2588\u258c       | 164M/642M [00:08&amp;lt;00:13, 38.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  26%|\u2588\u2588\u258c       | 168M/642M [00:08&amp;lt;00:14, 33.3MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  27%|\u2588\u2588\u258b       | 171M/642M [00:08&amp;lt;00:16, 29.3MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  27%|\u2588\u2588\u258b       | 174M/642M [00:08&amp;lt;00:19, 25.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  27%|\u2588\u2588\u258b       | 176M/642M [00:09&amp;lt;00:21, 22.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  28%|\u2588\u2588\u258a       | 179M/642M [00:09&amp;lt;00:24, 19.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  28%|\u2588\u2588\u258a       | 181M/642M [00:09&amp;lt;00:25, 18.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  28%|\u2588\u2588\u258a       | 183M/642M [00:09&amp;lt;00:27, 17.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  29%|\u2588\u2588\u258a       | 184M/642M [00:09&amp;lt;00:28, 16.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  29%|\u2588\u2588\u2589       | 186M/642M [00:09&amp;lt;00:31, 15.3MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  29%|\u2588\u2588\u2589       | 187M/642M [00:09&amp;lt;00:33, 14.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  29%|\u2588\u2588\u2589       | 189M/642M [00:10&amp;lt;00:36, 12.9MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2589       | 190M/642M [00:10&amp;lt;00:40, 11.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2589       | 191M/642M [00:10&amp;lt;00:43, 10.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2589       | 192M/642M [00:10&amp;lt;00:49, 9.60MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2588       | 193M/642M [00:10&amp;lt;00:53, 8.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2588       | 194M/642M [00:10&amp;lt;00:56, 8.34MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2588       | 195M/642M [00:10&amp;lt;01:00, 7.79MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2588       | 195M/642M [00:11&amp;lt;01:03, 7.32MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 196M/642M [00:11&amp;lt;01:08, 6.78MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 197M/642M [00:11&amp;lt;01:13, 6.35MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 197M/642M [00:11&amp;lt;01:17, 6.02MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 198M/642M [00:11&amp;lt;01:23, 5.60MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 198M/642M [00:11&amp;lt;01:24, 5.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 199M/642M [00:11&amp;lt;01:27, 5.31MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 199M/642M [00:11&amp;lt;01:32, 5.01MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 200M/642M [00:12&amp;lt;01:36, 4.81MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 200M/642M [00:12&amp;lt;01:41, 4.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588\u258f      | 201M/642M [00:12&amp;lt;01:46, 4.33MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588\u258f      | 201M/642M [00:12&amp;lt;01:48, 4.24MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588\u258f      | 202M/642M [00:12&amp;lt;01:49, 4.23MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588\u258f      | 202M/642M [00:12&amp;lt;01:47, 4.29MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 202M/642M [00:12&amp;lt;01:50, 4.15MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 203M/642M [00:12&amp;lt;01:54, 4.00MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 203M/642M [00:12&amp;lt;01:57, 3.90MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 204M/642M [00:13&amp;lt;02:01, 3.77MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 204M/642M [00:13&amp;lt;02:05, 3.65MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 204M/642M [00:13&amp;lt;02:11, 3.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 205M/642M [00:13&amp;lt;02:13, 3.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 205M/642M [00:13&amp;lt;02:18, 3.32MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 205M/642M [00:13&amp;lt;02:22, 3.20MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 206M/642M [00:13&amp;lt;02:31, 3.03MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 206M/642M [00:13&amp;lt;02:38, 2.89MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 206M/642M [00:13&amp;lt;02:38, 2.88MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 206M/642M [00:14&amp;lt;02:39, 2.87MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 207M/642M [00:14&amp;lt;02:42, 2.80MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 207M/642M [00:14&amp;lt;02:42, 2.80MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 207M/642M [00:14&amp;lt;02:39, 2.85MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 208M/642M [00:14&amp;lt;02:40, 2.84MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 208M/642M [00:14&amp;lt;02:40, 2.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 208M/642M [00:14&amp;lt;02:38, 2.87MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 208M/642M [00:14&amp;lt;02:37, 2.88MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 209M/642M [00:14&amp;lt;02:39, 2.85MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 209M/642M [00:14&amp;lt;02:45, 2.75MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 209M/642M [00:15&amp;lt;02:44, 2.75MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 210M/642M [00:15&amp;lt;02:40, 2.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 210M/642M [00:15&amp;lt;02:34, 2.92MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 210M/642M [00:15&amp;lt;02:30, 3.01MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 210M/642M [00:15&amp;lt;02:23, 3.15MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 211M/642M [00:15&amp;lt;02:23, 3.16MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 211M/642M [00:15&amp;lt;02:15, 3.32MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 211M/642M [00:15&amp;lt;02:17, 3.28MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 212M/642M [00:15&amp;lt;02:20, 3.22MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 212M/642M [00:15&amp;lt;02:26, 3.07MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 212M/642M [00:16&amp;lt;02:29, 3.01MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 213M/642M [00:16&amp;lt;02:33, 2.94MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 213M/642M [00:16&amp;lt;02:33, 2.93MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 213M/642M [00:16&amp;lt;02:39, 2.82MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 214M/642M [00:16&amp;lt;02:38, 2.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 214M/642M [00:16&amp;lt;02:38, 2.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 214M/642M [00:16&amp;lt;02:32, 2.93MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 214M/642M [00:16&amp;lt;02:30, 2.98MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 215M/642M [00:16&amp;lt;02:29, 3.00MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258e      | 215M/642M [00:17&amp;lt;02:30, 2.98MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258e      | 215M/642M [00:17&amp;lt;02:31, 2.96MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258e      | 216M/642M [00:17&amp;lt;02:23, 3.11MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258e      | 216M/642M [00:17&amp;lt;02:25, 3.08MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258e      | 216M/642M [00:17&amp;lt;02:26, 3.04MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258e      | 216M/642M [00:17&amp;lt;02:26, 3.04MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258d      | 217M/642M [00:17&amp;lt;02:20, 3.17MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258d      | 217M/642M [00:17&amp;lt;02:22, 3.13MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258d      | 217M/642M [00:17&amp;lt;02:20, 3.16MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258d      | 218M/642M [00:18&amp;lt;03:17, 2.26MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258d      | 218M/642M [00:18&amp;lt;02:09, 3.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258d      | 221M/642M [00:18&amp;lt;00:50, 8.77MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  35%|\u2588\u2588\u2588\u258d      | 223M/642M [00:18&amp;lt;00:35, 12.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  35%|\u2588\u2588\u2588\u258c      | 225M/642M [00:18&amp;lt;00:28, 15.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  35%|\u2588\u2588\u2588\u258c      | 227M/642M [00:18&amp;lt;00:26, 16.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  36%|\u2588\u2588\u2588\u258c      | 229M/642M [00:18&amp;lt;00:26, 16.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  36%|\u2588\u2588\u2588\u258c      | 230M/642M [00:18&amp;lt;00:26, 16.3MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  36%|\u2588\u2588\u2588\u258c      | 232M/642M [00:18&amp;lt;00:27, 15.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  36%|\u2588\u2588\u2588\u258b      | 234M/642M [00:19&amp;lt;00:26, 16.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  37%|\u2588\u2588\u2588\u258b      | 235M/642M [00:19&amp;lt;00:28, 15.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  37%|\u2588\u2588\u2588\u258b      | 237M/642M [00:19&amp;lt;00:30, 14.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  37%|\u2588\u2588\u2588\u258b      | 238M/642M [00:19&amp;lt;00:32, 13.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  37%|\u2588\u2588\u2588\u258b      | 239M/642M [00:19&amp;lt;00:32, 13.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  38%|\u2588\u2588\u2588\u258a      | 241M/642M [00:19&amp;lt;00:32, 12.9MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  38%|\u2588\u2588\u2588\u258a      | 242M/642M [00:19&amp;lt;00:34, 12.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  38%|\u2588\u2588\u2588\u258a      | 243M/642M [00:19&amp;lt;00:33, 12.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  38%|\u2588\u2588\u2588\u258a      | 245M/642M [00:19&amp;lt;00:31, 13.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  38%|\u2588\u2588\u2588\u258a      | 246M/642M [00:20&amp;lt;00:31, 13.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|\u2588\u2588\u2588\u258a      | 247M/642M [00:20&amp;lt;00:32, 12.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|\u2588\u2588\u2588\u258a      | 248M/642M [00:20&amp;lt;00:35, 11.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|\u2588\u2588\u2588\u2589      | 249M/642M [00:20&amp;lt;00:37, 11.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|\u2588\u2588\u2588\u2589      | 251M/642M [00:20&amp;lt;00:37, 10.9MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|\u2588\u2588\u2588\u2589      | 252M/642M [00:20&amp;lt;00:38, 10.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|\u2588\u2588\u2588\u2589      | 253M/642M [00:20&amp;lt;00:38, 10.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  40%|\u2588\u2588\u2588\u2589      | 254M/642M [00:20&amp;lt;00:36, 11.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  40%|\u2588\u2588\u2588\u2589      | 255M/642M [00:20&amp;lt;00:35, 11.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  40%|\u2588\u2588\u2588\u2589      | 256M/642M [00:21&amp;lt;00:35, 11.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  40%|\u2588\u2588\u2588\u2588      | 257M/642M [00:21&amp;lt;00:34, 11.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  40%|\u2588\u2588\u2588\u2588      | 258M/642M [00:21&amp;lt;00:35, 11.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  40%|\u2588\u2588\u2588\u2588      | 259M/642M [00:21&amp;lt;00:35, 11.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588      | 260M/642M [00:21&amp;lt;00:36, 10.9MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588      | 262M/642M [00:21&amp;lt;00:38, 10.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588      | 263M/642M [00:21&amp;lt;00:39, 10.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588      | 263M/642M [00:21&amp;lt;00:41, 9.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588      | 264M/642M [00:21&amp;lt;00:42, 9.33MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588\u258f     | 265M/642M [00:22&amp;lt;00:43, 9.13MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588\u258f     | 266M/642M [00:22&amp;lt;00:43, 8.96MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  42%|\u2588\u2588\u2588\u2588\u258f     | 267M/642M [00:22&amp;lt;00:44, 8.74MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  42%|\u2588\u2588\u2588\u2588\u258f     | 268M/642M [00:22&amp;lt;00:47, 8.31MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  42%|\u2588\u2588\u2588\u2588\u258f     | 269M/642M [00:22&amp;lt;00:50, 7.75MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  42%|\u2588\u2588\u2588\u2588\u258f     | 269M/642M [00:22&amp;lt;00:51, 7.51MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  42%|\u2588\u2588\u2588\u2588\u258f     | 270M/642M [00:22&amp;lt;00:52, 7.46MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  42%|\u2588\u2588\u2588\u2588\u258f     | 271M/642M [00:22&amp;lt;00:52, 7.37MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  42%|\u2588\u2588\u2588\u2588\u258f     | 272M/642M [00:22&amp;lt;00:53, 7.19MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  42%|\u2588\u2588\u2588\u2588\u258f     | 272M/642M [00:23&amp;lt;00:55, 7.00MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 273M/642M [00:23&amp;lt;00:54, 7.12MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 274M/642M [00:23&amp;lt;00:53, 7.18MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 274M/642M [00:23&amp;lt;00:54, 7.11MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 275M/642M [00:23&amp;lt;00:54, 7.01MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 276M/642M [00:23&amp;lt;00:55, 6.97MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 276M/642M [00:23&amp;lt;00:55, 6.95MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 277M/642M [00:23&amp;lt;00:55, 6.90MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 278M/642M [00:23&amp;lt;00:55, 6.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 278M/642M [00:23&amp;lt;00:56, 6.77MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 279M/642M [00:24&amp;lt;00:57, 6.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258e     | 280M/642M [00:24&amp;lt;00:58, 6.52MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258e     | 280M/642M [00:24&amp;lt;00:58, 6.49MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258d     | 281M/642M [00:24&amp;lt;00:59, 6.36MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258d     | 281M/642M [00:24&amp;lt;00:59, 6.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258d     | 282M/642M [00:24&amp;lt;00:58, 6.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258d     | 283M/642M [00:24&amp;lt;00:58, 6.38MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258d     | 283M/642M [00:24&amp;lt;01:00, 6.19MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258d     | 284M/642M [00:24&amp;lt;01:03, 5.87MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258d     | 284M/642M [00:25&amp;lt;01:06, 5.65MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258d     | 285M/642M [00:25&amp;lt;01:10, 5.30MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258d     | 286M/642M [00:25&amp;lt;01:14, 5.03MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258d     | 286M/642M [00:25&amp;lt;01:16, 4.89MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258d     | 286M/642M [00:25&amp;lt;01:17, 4.81MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258d     | 287M/642M [00:25&amp;lt;01:18, 4.73MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258d     | 287M/642M [00:25&amp;lt;01:18, 4.75MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258d     | 288M/642M [00:25&amp;lt;01:15, 4.93MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258d     | 288M/642M [00:25&amp;lt;01:11, 5.15MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258c     | 289M/642M [00:26&amp;lt;01:09, 5.32MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258c     | 290M/642M [00:26&amp;lt;01:08, 5.42MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258c     | 290M/642M [00:26&amp;lt;01:08, 5.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258c     | 291M/642M [00:26&amp;lt;01:07, 5.47MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258c     | 291M/642M [00:26&amp;lt;01:06, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258c     | 292M/642M [00:26&amp;lt;01:05, 5.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 292M/642M [00:26&amp;lt;01:04, 5.66MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 293M/642M [00:26&amp;lt;01:03, 5.77MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 293M/642M [00:26&amp;lt;01:03, 5.77MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 294M/642M [00:26&amp;lt;01:04, 5.68MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 295M/642M [00:27&amp;lt;01:04, 5.66MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 295M/642M [00:27&amp;lt;01:05, 5.53MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 296M/642M [00:27&amp;lt;01:05, 5.52MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 296M/642M [00:27&amp;lt;01:05, 5.51MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 297M/642M [00:27&amp;lt;01:04, 5.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258b     | 297M/642M [00:27&amp;lt;01:04, 5.63MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258b     | 298M/642M [00:27&amp;lt;01:03, 5.71MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 298M/642M [00:27&amp;lt;01:02, 5.76MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 299M/642M [00:27&amp;lt;01:02, 5.73MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 300M/642M [00:27&amp;lt;01:02, 5.70MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 300M/642M [00:28&amp;lt;01:05, 5.49MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 301M/642M [00:28&amp;lt;01:04, 5.52MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 301M/642M [00:28&amp;lt;01:15, 4.72MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 302M/642M [00:28&amp;lt;01:21, 4.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 305M/642M [00:28&amp;lt;00:29, 11.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  48%|\u2588\u2588\u2588\u2588\u258a     | 308M/642M [00:28&amp;lt;00:19, 18.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  49%|\u2588\u2588\u2588\u2588\u258a     | 311M/642M [00:28&amp;lt;00:15, 22.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  49%|\u2588\u2588\u2588\u2588\u2589     | 314M/642M [00:28&amp;lt;00:14, 23.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  49%|\u2588\u2588\u2588\u2588\u2589     | 316M/642M [00:28&amp;lt;00:13, 24.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  50%|\u2588\u2588\u2588\u2588\u2589     | 319M/642M [00:29&amp;lt;00:14, 23.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  50%|\u2588\u2588\u2588\u2588\u2588     | 321M/642M [00:29&amp;lt;00:15, 22.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  50%|\u2588\u2588\u2588\u2588\u2588     | 323M/642M [00:29&amp;lt;00:16, 20.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  51%|\u2588\u2588\u2588\u2588\u2588     | 325M/642M [00:29&amp;lt;00:17, 19.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  51%|\u2588\u2588\u2588\u2588\u2588     | 327M/642M [00:29&amp;lt;00:18, 17.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 329M/642M [00:29&amp;lt;00:19, 16.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 331M/642M [00:29&amp;lt;00:21, 15.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 332M/642M [00:29&amp;lt;00:22, 14.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 333M/642M [00:30&amp;lt;00:23, 13.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 335M/642M [00:30&amp;lt;00:24, 13.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 336M/642M [00:30&amp;lt;00:25, 12.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 337M/642M [00:30&amp;lt;00:26, 11.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 338M/642M [00:30&amp;lt;00:27, 11.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 339M/642M [00:30&amp;lt;00:28, 11.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 341M/642M [00:30&amp;lt;00:29, 10.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 342M/642M [00:30&amp;lt;00:30, 10.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 343M/642M [00:31&amp;lt;00:30, 10.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 344M/642M [00:31&amp;lt;00:31, 9.82MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 344M/642M [00:31&amp;lt;00:32, 9.49MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 345M/642M [00:31&amp;lt;00:34, 9.12MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 346M/642M [00:31&amp;lt;00:34, 8.90MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 347M/642M [00:31&amp;lt;00:35, 8.70MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 348M/642M [00:31&amp;lt;00:36, 8.47MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 349M/642M [00:31&amp;lt;00:36, 8.32MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 350M/642M [00:31&amp;lt;00:37, 8.11MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 350M/642M [00:32&amp;lt;00:38, 7.84MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 351M/642M [00:32&amp;lt;00:40, 7.51MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 352M/642M [00:32&amp;lt;00:41, 7.24MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 352M/642M [00:32&amp;lt;00:42, 7.08MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 353M/642M [00:32&amp;lt;00:43, 6.91MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 354M/642M [00:32&amp;lt;00:45, 6.69MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 354M/642M [00:32&amp;lt;00:46, 6.47MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 355M/642M [00:32&amp;lt;00:48, 6.25MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 356M/642M [00:32&amp;lt;00:50, 5.98MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 356M/642M [00:33&amp;lt;00:51, 5.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 357M/642M [00:33&amp;lt;00:51, 5.75MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 357M/642M [00:33&amp;lt;00:52, 5.71MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 358M/642M [00:33&amp;lt;00:52, 5.62MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 358M/642M [00:33&amp;lt;00:53, 5.54MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 359M/642M [00:33&amp;lt;00:54, 5.46MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 359M/642M [00:33&amp;lt;00:56, 5.27MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 360M/642M [00:33&amp;lt;00:58, 5.06MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 360M/642M [00:33&amp;lt;01:00, 4.89MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 361M/642M [00:34&amp;lt;01:01, 4.77MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 361M/642M [00:34&amp;lt;01:02, 4.73MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 362M/642M [00:34&amp;lt;01:02, 4.73MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 362M/642M [00:34&amp;lt;01:02, 4.69MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 363M/642M [00:34&amp;lt;01:03, 4.63MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 363M/642M [00:34&amp;lt;01:05, 4.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 364M/642M [00:34&amp;lt;01:06, 4.36MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 364M/642M [00:34&amp;lt;01:06, 4.37MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 364M/642M [00:34&amp;lt;01:06, 4.34MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 365M/642M [00:34&amp;lt;01:06, 4.36MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 365M/642M [00:35&amp;lt;01:06, 4.36MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 366M/642M [00:35&amp;lt;01:06, 4.34MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 366M/642M [00:35&amp;lt;01:05, 4.39MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 367M/642M [00:35&amp;lt;01:05, 4.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 367M/642M [00:35&amp;lt;01:05, 4.41MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 367M/642M [00:35&amp;lt;01:05, 4.42MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 368M/642M [00:35&amp;lt;01:04, 4.44MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 368M/642M [00:35&amp;lt;01:05, 4.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 369M/642M [00:35&amp;lt;01:06, 4.29MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 369M/642M [00:35&amp;lt;01:08, 4.18MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 369M/642M [00:36&amp;lt;01:10, 4.08MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 370M/642M [00:36&amp;lt;01:12, 3.93MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 370M/642M [00:36&amp;lt;01:17, 3.65MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 371M/642M [00:36&amp;lt;01:18, 3.60MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 371M/642M [00:36&amp;lt;01:18, 3.62MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 371M/642M [00:36&amp;lt;01:21, 3.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 372M/642M [00:36&amp;lt;01:18, 3.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 372M/642M [00:36&amp;lt;01:15, 3.74MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 372M/642M [00:36&amp;lt;01:15, 3.74MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 373M/642M [00:37&amp;lt;01:14, 3.80MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 373M/642M [00:37&amp;lt;01:14, 3.76MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 374M/642M [00:37&amp;lt;01:13, 3.80MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 374M/642M [00:37&amp;lt;01:11, 3.91MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 374M/642M [00:37&amp;lt;01:11, 3.90MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 375M/642M [00:37&amp;lt;01:09, 4.00MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 375M/642M [00:37&amp;lt;01:07, 4.15MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 376M/642M [00:37&amp;lt;01:07, 4.14MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 376M/642M [00:37&amp;lt;01:07, 4.14MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 376M/642M [00:37&amp;lt;01:08, 4.07MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 377M/642M [00:38&amp;lt;01:08, 4.04MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 377M/642M [00:38&amp;lt;01:11, 3.88MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 378M/642M [00:38&amp;lt;01:14, 3.70MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 378M/642M [00:38&amp;lt;01:15, 3.66MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 378M/642M [00:38&amp;lt;01:22, 3.34MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 379M/642M [00:38&amp;lt;01:24, 3.28MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 379M/642M [00:38&amp;lt;01:17, 3.54MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 380M/642M [00:38&amp;lt;01:06, 4.10MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 380M/642M [00:38&amp;lt;01:00, 4.55MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 381M/642M [00:39&amp;lt;00:56, 4.81MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 381M/642M [00:39&amp;lt;00:54, 5.03MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 382M/642M [00:39&amp;lt;00:52, 5.17MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 382M/642M [00:39&amp;lt;00:49, 5.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 383M/642M [00:39&amp;lt;00:48, 5.60MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 383M/642M [00:39&amp;lt;00:48, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 384M/642M [00:39&amp;lt;00:50, 5.39MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 384M/642M [00:39&amp;lt;00:48, 5.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 385M/642M [00:39&amp;lt;00:47, 5.64MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 386M/642M [00:39&amp;lt;00:47, 5.62MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 386M/642M [00:40&amp;lt;00:47, 5.62MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 387M/642M [00:40&amp;lt;00:48, 5.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 387M/642M [00:40&amp;lt;00:49, 5.44MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 388M/642M [00:40&amp;lt;00:49, 5.35MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 388M/642M [00:40&amp;lt;00:49, 5.32MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 389M/642M [00:40&amp;lt;00:49, 5.37MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 389M/642M [00:40&amp;lt;00:49, 5.37MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 390M/642M [00:40&amp;lt;00:49, 5.29MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 390M/642M [00:40&amp;lt;00:52, 5.01MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 391M/642M [00:41&amp;lt;00:57, 4.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 391M/642M [00:41&amp;lt;01:00, 4.36MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 392M/642M [00:41&amp;lt;01:00, 4.34MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 392M/642M [00:41&amp;lt;01:01, 4.23MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 392M/642M [00:41&amp;lt;01:01, 4.24MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 393M/642M [00:41&amp;lt;01:00, 4.33MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 393M/642M [00:41&amp;lt;00:55, 4.67MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 394M/642M [00:41&amp;lt;00:51, 5.02MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 395M/642M [00:41&amp;lt;00:49, 5.25MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 395M/642M [00:42&amp;lt;00:48, 5.30MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 396M/642M [00:42&amp;lt;00:48, 5.29MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 396M/642M [00:42&amp;lt;00:51, 4.98MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 397M/642M [00:42&amp;lt;00:57, 4.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 397M/642M [00:42&amp;lt;01:00, 4.23MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 397M/642M [00:42&amp;lt;01:05, 3.92MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 398M/642M [00:42&amp;lt;01:06, 3.84MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 398M/642M [00:42&amp;lt;01:09, 3.68MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 399M/642M [00:42&amp;lt;01:09, 3.69MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 399M/642M [00:43&amp;lt;01:05, 3.89MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 399M/642M [00:43&amp;lt;01:03, 4.00MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 400M/642M [00:43&amp;lt;01:02, 4.07MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 400M/642M [00:43&amp;lt;01:03, 3.97MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 401M/642M [00:43&amp;lt;01:02, 4.02MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 401M/642M [00:43&amp;lt;01:03, 4.00MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 401M/642M [00:43&amp;lt;01:02, 4.02MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 402M/642M [00:43&amp;lt;01:03, 3.98MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 402M/642M [00:43&amp;lt;01:02, 4.02MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 403M/642M [00:43&amp;lt;00:59, 4.20MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 403M/642M [00:44&amp;lt;00:52, 4.79MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 404M/642M [00:44&amp;lt;00:47, 5.29MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 404M/642M [00:44&amp;lt;00:44, 5.64MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 405M/642M [00:44&amp;lt;00:43, 5.66MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 406M/642M [00:44&amp;lt;00:42, 5.80MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 406M/642M [00:44&amp;lt;00:43, 5.66MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 407M/642M [00:44&amp;lt;00:44, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 407M/642M [00:44&amp;lt;00:44, 5.55MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 408M/642M [00:44&amp;lt;00:42, 5.71MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 409M/642M [00:45&amp;lt;00:40, 6.03MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 409M/642M [00:45&amp;lt;00:40, 6.06MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 410M/642M [00:45&amp;lt;00:42, 5.76MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 410M/642M [00:45&amp;lt;00:44, 5.49MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 411M/642M [00:45&amp;lt;00:46, 5.16MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 411M/642M [00:45&amp;lt;00:48, 5.03MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 412M/642M [00:45&amp;lt;00:47, 5.12MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 412M/642M [00:45&amp;lt;00:45, 5.32MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 413M/642M [00:45&amp;lt;00:42, 5.70MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 414M/642M [00:46&amp;lt;00:38, 6.16MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 414M/642M [00:46&amp;lt;00:36, 6.60MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 415M/642M [00:46&amp;lt;00:35, 6.66MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 416M/642M [00:46&amp;lt;00:35, 6.62MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 416M/642M [00:46&amp;lt;00:35, 6.60MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 417M/642M [00:46&amp;lt;00:35, 6.63MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 418M/642M [00:46&amp;lt;00:35, 6.68MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 419M/642M [00:46&amp;lt;00:31, 7.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 420M/642M [00:46&amp;lt;00:28, 8.13MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 420M/642M [00:46&amp;lt;00:27, 8.55MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 421M/642M [00:47&amp;lt;00:26, 8.76MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 422M/642M [00:47&amp;lt;00:26, 8.84MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 423M/642M [00:47&amp;lt;00:25, 8.94MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 424M/642M [00:47&amp;lt;00:25, 9.07MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 425M/642M [00:47&amp;lt;00:24, 9.14MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 426M/642M [00:47&amp;lt;00:24, 9.21MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 427M/642M [00:47&amp;lt;00:25, 8.99MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 428M/642M [00:47&amp;lt;00:25, 8.70MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 428M/642M [00:47&amp;lt;00:26, 8.53MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 429M/642M [00:47&amp;lt;00:26, 8.47MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 430M/642M [00:48&amp;lt;00:26, 8.28MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 431M/642M [00:48&amp;lt;00:26, 8.24MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 432M/642M [00:48&amp;lt;00:27, 8.13MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 432M/642M [00:48&amp;lt;00:27, 7.88MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 433M/642M [00:48&amp;lt;00:29, 7.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 434M/642M [00:48&amp;lt;00:29, 7.28MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 435M/642M [00:48&amp;lt;00:33, 6.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 435M/642M [00:48&amp;lt;00:39, 5.51MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 436M/642M [00:49&amp;lt;00:31, 6.94MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 437M/642M [00:49&amp;lt;00:26, 8.18MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 439M/642M [00:49&amp;lt;00:23, 9.03MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 440M/642M [00:49&amp;lt;00:22, 9.61MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 441M/642M [00:49&amp;lt;00:21, 9.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 442M/642M [00:49&amp;lt;00:22, 9.31MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 442M/642M [00:49&amp;lt;00:23, 8.98MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 443M/642M [00:49&amp;lt;00:24, 8.51MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 444M/642M [00:49&amp;lt;00:25, 8.21MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 445M/642M [00:50&amp;lt;00:25, 8.11MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 446M/642M [00:50&amp;lt;00:25, 7.93MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 446M/642M [00:50&amp;lt;00:26, 7.82MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 447M/642M [00:50&amp;lt;00:26, 7.80MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 448M/642M [00:50&amp;lt;00:26, 7.77MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 449M/642M [00:50&amp;lt;00:26, 7.58MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 449M/642M [00:50&amp;lt;00:27, 7.44MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 450M/642M [00:50&amp;lt;00:27, 7.18MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 451M/642M [00:50&amp;lt;00:28, 7.11MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 452M/642M [00:50&amp;lt;00:28, 6.94MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 452M/642M [00:51&amp;lt;00:28, 6.93MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 453M/642M [00:51&amp;lt;00:28, 6.94MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 454M/642M [00:51&amp;lt;00:28, 6.94MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 454M/642M [00:51&amp;lt;00:28, 7.00MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 455M/642M [00:51&amp;lt;00:27, 7.00MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 456M/642M [00:51&amp;lt;00:28, 6.93MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 456M/642M [00:51&amp;lt;00:29, 6.69MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 457M/642M [00:51&amp;lt;00:29, 6.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 458M/642M [00:51&amp;lt;00:29, 6.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 458M/642M [00:52&amp;lt;00:29, 6.58MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 459M/642M [00:52&amp;lt;00:29, 6.61MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 459M/642M [00:52&amp;lt;00:29, 6.55MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 460M/642M [00:52&amp;lt;00:29, 6.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 461M/642M [00:52&amp;lt;00:30, 6.28MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 461M/642M [00:52&amp;lt;00:31, 6.08MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 462M/642M [00:52&amp;lt;00:32, 5.76MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 462M/642M [00:52&amp;lt;00:33, 5.62MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 463M/642M [00:52&amp;lt;00:33, 5.58MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 463M/642M [00:52&amp;lt;00:34, 5.49MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 464M/642M [00:53&amp;lt;00:34, 5.45MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 465M/642M [00:53&amp;lt;00:34, 5.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 465M/642M [00:53&amp;lt;00:33, 5.46MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 466M/642M [00:53&amp;lt;00:33, 5.51MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 466M/642M [00:53&amp;lt;00:33, 5.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 467M/642M [00:53&amp;lt;00:32, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 467M/642M [00:53&amp;lt;00:32, 5.58MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 468M/642M [00:53&amp;lt;00:32, 5.65MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 468M/642M [00:53&amp;lt;00:32, 5.64MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 469M/642M [00:53&amp;lt;00:32, 5.60MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 469M/642M [00:54&amp;lt;00:32, 5.61MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 470M/642M [00:54&amp;lt;00:32, 5.63MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 470M/642M [00:54&amp;lt;00:32, 5.58MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 471M/642M [00:54&amp;lt;00:31, 5.64MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 472M/642M [00:54&amp;lt;00:31, 5.64MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 472M/642M [00:54&amp;lt;00:31, 5.69MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 473M/642M [00:54&amp;lt;00:31, 5.68MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 473M/642M [00:54&amp;lt;00:31, 5.65MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 474M/642M [00:54&amp;lt;00:32, 5.45MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 474M/642M [00:55&amp;lt;00:33, 5.21MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 475M/642M [00:55&amp;lt;00:35, 4.98MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 475M/642M [00:55&amp;lt;00:36, 4.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 476M/642M [00:55&amp;lt;00:36, 4.73MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 476M/642M [00:55&amp;lt;00:37, 4.58MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 477M/642M [00:55&amp;lt;00:38, 4.51MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 477M/642M [00:55&amp;lt;00:38, 4.52MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 477M/642M [00:55&amp;lt;00:38, 4.47MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 478M/642M [00:55&amp;lt;00:39, 4.29MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 478M/642M [00:56&amp;lt;00:40, 4.26MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 479M/642M [00:56&amp;lt;00:39, 4.31MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 479M/642M [00:56&amp;lt;00:39, 4.30MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 480M/642M [00:56&amp;lt;00:39, 4.35MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 480M/642M [00:56&amp;lt;00:38, 4.38MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 480M/642M [00:56&amp;lt;00:38, 4.34MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 481M/642M [00:56&amp;lt;00:38, 4.34MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 481M/642M [00:56&amp;lt;00:38, 4.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 482M/642M [00:56&amp;lt;00:37, 4.42MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 482M/642M [00:56&amp;lt;00:37, 4.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 483M/642M [00:57&amp;lt;00:37, 4.45MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 483M/642M [00:57&amp;lt;00:37, 4.46MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 484M/642M [00:57&amp;lt;00:37, 4.47MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 484M/642M [00:57&amp;lt;00:36, 4.49MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 484M/642M [00:57&amp;lt;00:36, 4.55MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 485M/642M [00:57&amp;lt;00:36, 4.51MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 485M/642M [00:57&amp;lt;00:37, 4.38MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 486M/642M [00:57&amp;lt;00:37, 4.37MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 486M/642M [00:57&amp;lt;00:37, 4.39MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 487M/642M [00:57&amp;lt;00:37, 4.35MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 487M/642M [00:58&amp;lt;00:37, 4.36MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 487M/642M [00:58&amp;lt;00:36, 4.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 488M/642M [00:58&amp;lt;00:36, 4.38MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 488M/642M [00:58&amp;lt;00:36, 4.41MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 489M/642M [00:58&amp;lt;00:36, 4.41MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 489M/642M [00:58&amp;lt;00:36, 4.34MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 490M/642M [00:58&amp;lt;00:36, 4.31MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 490M/642M [00:58&amp;lt;00:36, 4.30MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 490M/642M [00:58&amp;lt;00:35, 4.46MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 491M/642M [00:59&amp;lt;00:37, 4.22MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 491M/642M [00:59&amp;lt;00:38, 4.06MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 492M/642M [00:59&amp;lt;00:34, 4.58MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 492M/642M [00:59&amp;lt;00:29, 5.23MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 493M/642M [00:59&amp;lt;00:27, 5.76MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 494M/642M [00:59&amp;lt;00:25, 6.13MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 495M/642M [00:59&amp;lt;00:24, 6.39MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 495M/642M [00:59&amp;lt;00:23, 6.44MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 496M/642M [00:59&amp;lt;00:23, 6.38MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 496M/642M [00:59&amp;lt;00:24, 6.25MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 497M/642M [01:00&amp;lt;00:24, 6.15MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 498M/642M [01:00&amp;lt;00:25, 5.90MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 498M/642M [01:00&amp;lt;00:25, 5.82MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 499M/642M [01:00&amp;lt;00:26, 5.74MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 499M/642M [01:00&amp;lt;00:26, 5.64MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 500M/642M [01:00&amp;lt;00:26, 5.70MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 500M/642M [01:00&amp;lt;00:26, 5.69MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 501M/642M [01:00&amp;lt;00:25, 5.72MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 501M/642M [01:00&amp;lt;00:25, 5.75MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 502M/642M [01:00&amp;lt;00:25, 5.68MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 503M/642M [01:01&amp;lt;00:25, 5.67MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 503M/642M [01:01&amp;lt;00:25, 5.61MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 504M/642M [01:01&amp;lt;00:26, 5.55MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 504M/642M [01:01&amp;lt;00:26, 5.54MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 505M/642M [01:01&amp;lt;00:26, 5.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 505M/642M [01:01&amp;lt;00:26, 5.45MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 506M/642M [01:01&amp;lt;00:26, 5.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 506M/642M [01:01&amp;lt;00:25, 5.51MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 507M/642M [01:01&amp;lt;00:24, 5.75MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 508M/642M [01:02&amp;lt;00:23, 5.88MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 508M/642M [01:02&amp;lt;00:23, 6.03MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 509M/642M [01:02&amp;lt;00:23, 5.97MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 509M/642M [01:02&amp;lt;00:23, 5.87MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 510M/642M [01:02&amp;lt;00:23, 5.76MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 510M/642M [01:02&amp;lt;00:24, 5.60MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 511M/642M [01:02&amp;lt;00:25, 5.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 511M/642M [01:02&amp;lt;00:25, 5.31MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 512M/642M [01:02&amp;lt;00:26, 5.19MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 512M/642M [01:02&amp;lt;00:26, 5.04MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 513M/642M [01:03&amp;lt;00:28, 4.76MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 513M/642M [01:03&amp;lt;00:28, 4.76MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 514M/642M [01:03&amp;lt;00:29, 4.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 514M/642M [01:03&amp;lt;00:29, 4.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 515M/642M [01:03&amp;lt;00:30, 4.42MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 515M/642M [01:03&amp;lt;00:30, 4.41MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 516M/642M [01:03&amp;lt;00:30, 4.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 516M/642M [01:03&amp;lt;00:31, 4.22MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 516M/642M [01:03&amp;lt;00:31, 4.17MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 517M/642M [01:04&amp;lt;00:33, 3.95MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 517M/642M [01:04&amp;lt;00:33, 3.89MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 518M/642M [01:04&amp;lt;00:36, 3.61MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 518M/642M [01:04&amp;lt;00:37, 3.45MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 518M/642M [01:04&amp;lt;00:38, 3.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 519M/642M [01:04&amp;lt;00:38, 3.32MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 519M/642M [01:04&amp;lt;00:40, 3.17MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 519M/642M [01:04&amp;lt;00:41, 3.07MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 519M/642M [01:04&amp;lt;00:44, 2.88MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 520M/642M [01:05&amp;lt;00:45, 2.81MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 520M/642M [01:05&amp;lt;00:44, 2.86MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 520M/642M [01:05&amp;lt;00:41, 3.05MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 521M/642M [01:05&amp;lt;00:41, 3.07MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 521M/642M [01:05&amp;lt;00:41, 3.08MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 521M/642M [01:05&amp;lt;00:39, 3.21MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 522M/642M [01:05&amp;lt;00:38, 3.27MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 522M/642M [01:05&amp;lt;00:37, 3.34MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 522M/642M [01:05&amp;lt;00:37, 3.36MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 523M/642M [01:06&amp;lt;00:37, 3.37MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 523M/642M [01:06&amp;lt;00:36, 3.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 523M/642M [01:06&amp;lt;00:34, 3.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 524M/642M [01:06&amp;lt;00:32, 3.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 524M/642M [01:06&amp;lt;00:29, 4.11MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 525M/642M [01:06&amp;lt;00:27, 4.46MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 525M/642M [01:06&amp;lt;00:25, 4.80MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 526M/642M [01:06&amp;lt;00:24, 5.04MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 526M/642M [01:06&amp;lt;00:23, 5.17MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 527M/642M [01:06&amp;lt;00:22, 5.26MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 527M/642M [01:07&amp;lt;00:22, 5.29MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 528M/642M [01:07&amp;lt;00:22, 5.38MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 529M/642M [01:07&amp;lt;00:21, 5.52MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 529M/642M [01:07&amp;lt;00:21, 5.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 530M/642M [01:07&amp;lt;00:21, 5.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 530M/642M [01:07&amp;lt;00:20, 5.65MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 531M/642M [01:07&amp;lt;00:20, 5.70MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 531M/642M [01:07&amp;lt;00:20, 5.72MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 532M/642M [01:07&amp;lt;00:20, 5.73MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 532M/642M [01:07&amp;lt;00:20, 5.72MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 533M/642M [01:08&amp;lt;00:20, 5.61MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 534M/642M [01:08&amp;lt;00:20, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 534M/642M [01:08&amp;lt;00:20, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 535M/642M [01:08&amp;lt;00:20, 5.54MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 535M/642M [01:08&amp;lt;00:20, 5.54MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 536M/642M [01:08&amp;lt;00:19, 5.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 536M/642M [01:08&amp;lt;00:18, 5.91MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 537M/642M [01:08&amp;lt;00:18, 6.07MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 538M/642M [01:08&amp;lt;00:17, 6.28MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 538M/642M [01:08&amp;lt;00:17, 6.23MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 539M/642M [01:09&amp;lt;00:17, 6.09MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 539M/642M [01:09&amp;lt;00:19, 5.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 540M/642M [01:09&amp;lt;00:21, 4.87MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 541M/642M [01:09&amp;lt;00:18, 5.84MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 542M/642M [01:09&amp;lt;00:15, 6.63MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 542M/642M [01:09&amp;lt;00:14, 7.06MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 543M/642M [01:09&amp;lt;00:13, 7.41MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 544M/642M [01:09&amp;lt;00:13, 7.71MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 545M/642M [01:09&amp;lt;00:12, 7.89MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 546M/642M [01:10&amp;lt;00:12, 8.06MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 546M/642M [01:10&amp;lt;00:12, 8.08MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 547M/642M [01:10&amp;lt;00:12, 7.98MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 548M/642M [01:10&amp;lt;00:12, 7.75MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 549M/642M [01:10&amp;lt;00:12, 7.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 549M/642M [01:10&amp;lt;00:12, 7.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 550M/642M [01:10&amp;lt;00:12, 7.42MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 551M/642M [01:10&amp;lt;00:13, 7.20MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 552M/642M [01:10&amp;lt;00:13, 6.92MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 552M/642M [01:11&amp;lt;00:14, 6.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 553M/642M [01:11&amp;lt;00:15, 6.14MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 553M/642M [01:11&amp;lt;00:15, 6.00MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 554M/642M [01:11&amp;lt;00:15, 5.98MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 555M/642M [01:11&amp;lt;00:15, 5.96MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 555M/642M [01:11&amp;lt;00:15, 5.89MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 556M/642M [01:11&amp;lt;00:15, 5.69MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 556M/642M [01:11&amp;lt;00:15, 5.69MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 557M/642M [01:11&amp;lt;00:15, 5.62MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 557M/642M [01:12&amp;lt;00:16, 5.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 558M/642M [01:12&amp;lt;00:15, 5.53MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 558M/642M [01:12&amp;lt;00:15, 5.65MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 559M/642M [01:12&amp;lt;00:15, 5.63MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 560M/642M [01:12&amp;lt;00:15, 5.71MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 560M/642M [01:12&amp;lt;00:14, 5.73MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 561M/642M [01:12&amp;lt;00:14, 5.75MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 561M/642M [01:12&amp;lt;00:14, 5.71MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 562M/642M [01:12&amp;lt;00:15, 5.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 562M/642M [01:12&amp;lt;00:15, 5.46MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 563M/642M [01:13&amp;lt;00:15, 5.39MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 563M/642M [01:13&amp;lt;00:15, 5.39MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 564M/642M [01:13&amp;lt;00:14, 5.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 564M/642M [01:13&amp;lt;00:14, 5.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 565M/642M [01:13&amp;lt;00:14, 5.63MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 566M/642M [01:13&amp;lt;00:14, 5.60MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 566M/642M [01:13&amp;lt;00:14, 5.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 567M/642M [01:13&amp;lt;00:14, 5.57MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 567M/642M [01:13&amp;lt;00:14, 5.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 568M/642M [01:13&amp;lt;00:14, 5.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 568M/642M [01:14&amp;lt;00:14, 5.26MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 569M/642M [01:14&amp;lt;00:14, 5.31MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 569M/642M [01:14&amp;lt;00:13, 5.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 570M/642M [01:14&amp;lt;00:13, 5.53MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 570M/642M [01:14&amp;lt;00:13, 5.58MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 571M/642M [01:14&amp;lt;00:13, 5.60MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 571M/642M [01:14&amp;lt;00:13, 5.62MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 572M/642M [01:14&amp;lt;00:13, 5.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 573M/642M [01:14&amp;lt;00:12, 5.64MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 573M/642M [01:15&amp;lt;00:12, 5.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 574M/642M [01:15&amp;lt;00:12, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 574M/642M [01:15&amp;lt;00:12, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 575M/642M [01:15&amp;lt;00:12, 5.53MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 575M/642M [01:15&amp;lt;00:12, 5.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 576M/642M [01:15&amp;lt;00:12, 5.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 576M/642M [01:15&amp;lt;00:12, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 577M/642M [01:15&amp;lt;00:12, 5.55MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 577M/642M [01:15&amp;lt;00:12, 5.54MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 578M/642M [01:15&amp;lt;00:11, 5.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 578M/642M [01:16&amp;lt;00:11, 5.55MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 579M/642M [01:16&amp;lt;00:12, 5.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 579M/642M [01:16&amp;lt;00:12, 5.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 580M/642M [01:16&amp;lt;00:11, 5.50MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 581M/642M [01:16&amp;lt;00:11, 5.53MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 581M/642M [01:16&amp;lt;00:11, 5.52MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 582M/642M [01:16&amp;lt;00:11, 5.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 582M/642M [01:16&amp;lt;00:11, 5.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 583M/642M [01:16&amp;lt;00:11, 5.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 583M/642M [01:16&amp;lt;00:11, 5.48MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 584M/642M [01:17&amp;lt;00:11, 5.47MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 584M/642M [01:17&amp;lt;00:10, 5.52MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 585M/642M [01:17&amp;lt;00:10, 5.44MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 585M/642M [01:17&amp;lt;00:10, 5.44MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 586M/642M [01:17&amp;lt;00:10, 5.40MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 586M/642M [01:17&amp;lt;00:10, 5.45MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 587M/642M [01:17&amp;lt;00:10, 5.49MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 587M/642M [01:17&amp;lt;00:10, 5.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 588M/642M [01:17&amp;lt;00:10, 5.43MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 589M/642M [01:17&amp;lt;00:10, 5.53MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 589M/642M [01:18&amp;lt;00:09, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 590M/642M [01:18&amp;lt;00:09, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 590M/642M [01:18&amp;lt;00:09, 5.54MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 591M/642M [01:18&amp;lt;00:09, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 591M/642M [01:18&amp;lt;00:09, 5.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 592M/642M [01:18&amp;lt;00:09, 5.55MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 592M/642M [01:18&amp;lt;00:09, 5.46MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 593M/642M [01:18&amp;lt;00:09, 5.39MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 593M/642M [01:18&amp;lt;00:09, 5.39MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 594M/642M [01:18&amp;lt;00:09, 5.44MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 594M/642M [01:19&amp;lt;00:08, 5.55MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 595M/642M [01:19&amp;lt;00:08, 5.45MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 596M/642M [01:19&amp;lt;00:08, 5.45MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 596M/642M [01:19&amp;lt;00:08, 5.62MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 597M/642M [01:19&amp;lt;00:09, 4.73MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 597M/642M [01:19&amp;lt;00:08, 5.24MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 598M/642M [01:19&amp;lt;00:07, 6.30MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 599M/642M [01:19&amp;lt;00:06, 7.09MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 600M/642M [01:19&amp;lt;00:05, 7.54MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 601M/642M [01:20&amp;lt;00:05, 7.83MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 602M/642M [01:20&amp;lt;00:05, 8.18MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 603M/642M [01:20&amp;lt;00:04, 8.84MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 604M/642M [01:20&amp;lt;00:04, 9.22MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 604M/642M [01:20&amp;lt;00:04, 9.28MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 605M/642M [01:20&amp;lt;00:04, 9.21MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 606M/642M [01:20&amp;lt;00:04, 9.25MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 607M/642M [01:20&amp;lt;00:03, 9.14MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 608M/642M [01:20&amp;lt;00:03, 9.11MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 609M/642M [01:20&amp;lt;00:03, 9.30MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 610M/642M [01:21&amp;lt;00:03, 9.77MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 611M/642M [01:21&amp;lt;00:03, 10.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 612M/642M [01:21&amp;lt;00:03, 10.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 613M/642M [01:21&amp;lt;00:03, 10.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 614M/642M [01:21&amp;lt;00:02, 9.94MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 615M/642M [01:21&amp;lt;00:02, 10.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 616M/642M [01:21&amp;lt;00:02, 10.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 617M/642M [01:21&amp;lt;00:02, 9.76MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 618M/642M [01:21&amp;lt;00:02, 9.91MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 619M/642M [01:21&amp;lt;00:02, 9.70MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 620M/642M [01:22&amp;lt;00:02, 9.45MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 621M/642M [01:22&amp;lt;00:02, 8.99MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 621M/642M [01:22&amp;lt;00:02, 8.71MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 622M/642M [01:22&amp;lt;00:02, 8.39MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 623M/642M [01:22&amp;lt;00:02, 8.02MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 624M/642M [01:22&amp;lt;00:02, 7.85MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 625M/642M [01:22&amp;lt;00:02, 7.69MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 625M/642M [01:22&amp;lt;00:02, 7.53MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 626M/642M [01:22&amp;lt;00:02, 7.30MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 627M/642M [01:23&amp;lt;00:02, 7.21MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 627M/642M [01:23&amp;lt;00:02, 6.99MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 628M/642M [01:23&amp;lt;00:02, 6.81MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 629M/642M [01:23&amp;lt;00:02, 6.62MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 629M/642M [01:23&amp;lt;00:02, 6.36MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 630M/642M [01:23&amp;lt;00:01, 6.18MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 631M/642M [01:23&amp;lt;00:01, 5.97MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 631M/642M [01:23&amp;lt;00:01, 5.72MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 632M/642M [01:23&amp;lt;00:01, 5.70MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 632M/642M [01:24&amp;lt;00:01, 5.52MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 633M/642M [01:24&amp;lt;00:01, 5.46MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 633M/642M [01:24&amp;lt;00:01, 5.46MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 634M/642M [01:24&amp;lt;00:01, 5.33MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 634M/642M [01:24&amp;lt;00:01, 5.32MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 635M/642M [01:24&amp;lt;00:01, 5.23MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 635M/642M [01:24&amp;lt;00:01, 5.31MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 636M/642M [01:24&amp;lt;00:01, 5.33MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 636M/642M [01:24&amp;lt;00:01, 5.26MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 637M/642M [01:25&amp;lt;00:00, 5.24MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 637M/642M [01:25&amp;lt;00:00, 5.21MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 638M/642M [01:25&amp;lt;00:00, 5.25MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 638M/642M [01:25&amp;lt;00:00, 5.08MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 639M/642M [01:25&amp;lt;00:00, 4.87MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 639M/642M [01:25&amp;lt;00:00, 4.72MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 640M/642M [01:25&amp;lt;00:00, 4.59MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 640M/642M [01:25&amp;lt;00:00, 4.56MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 641M/642M [01:25&amp;lt;00:00, 4.51MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 641M/642M [01:25&amp;lt;00:00, 4.47MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 642M/642M [01:26&amp;lt;00:00, 4.36MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642M/642M [01:26&amp;lt;00:00, 7.81MB/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:18,  2.10s/it]\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:07,  1.03it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:04,  1.52it/s]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.47it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:04&amp;lt;00:04,  1.15it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:05,  1.39s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:07&amp;lt;00:03,  1.05s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:07&amp;lt;00:01,  1.29it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:08&amp;lt;00:00,  1.20it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.64it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.18it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    1         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  8.50      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.18      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          230.70    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         326.51    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          557.21    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             3.85      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3270.29   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3545.40   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          30.37     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        24.26     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           53.77     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          11.55     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        11.98     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           13.37     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           11.72     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.95     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            14.34     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 100.72280478477478 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 1\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 8.495853965170681\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.1770447139270126\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 230.70076392969446\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 326.5122036433533\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3270.2904696576297\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3545.402384363115\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1900.9774820061969\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6038.386561041699\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 30.374120362102985\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 24.2576920427382\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 12.718030757385312\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 53.76895607449114\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 11.554640946977461\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 11.984970061138803\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.4664064802308425\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 13.374347250077905\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 11.721820218755436\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.952333316206932\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.9289297785335124\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 14.338939925655719\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.8492781103164004\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 0 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 1.0027644634246826 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 2.005537986755371 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 3.00883412361145 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 4.011110067367554 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 5.013396978378296 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 6.016204118728638 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 7.0183422565460205 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 8.021005153656006 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 9.024004220962524 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 10.026884078979492 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 11.029734373092651 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 12.03339672088623 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 13.036310911178589 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 14.039863348007202 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 15.044543504714966 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 16.04663372039795 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 17.049084663391113 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 18.052741289138794 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 19.055694818496704 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 20.058908224105835 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 21.062407970428467 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 22.065725088119507 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 23.073348999023438 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 24.076596975326538 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 25.079514980316162 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 26.08271288871765 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 27.0849187374115 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 28.087331533432007 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 29.09062957763672 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 30.093969106674194 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 31.096092224121094 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 32.098259925842285 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 33.10020351409912 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 34.10257911682129 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 35.10529041290283 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 36.10723090171814 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 37.109349489212036 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 38.1116259098053 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 39.11374092102051 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 40.11586785316467 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 41.118582010269165 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 42.12103724479675 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 43.12319540977478 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 44.12515425682068 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 45.12720561027527 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 46.12935662269592 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 47.13183069229126 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 48.133991718292236 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 49.13620734214783 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 50.138524770736694 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 51.14052772521973 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 52.142560958862305 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 53.14522337913513 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 54.14732527732849 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 55.149657011032104 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 56.15199160575867 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 57.15406537055969 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 58.156192779541016 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 59.15878653526306 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 60.16103219985962 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 61.163318395614624 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 62.16525983810425 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 63.167245864868164 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 64.1698248386383 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 65.17213368415833 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 66.17452359199524 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 67.17691826820374 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 68.17936849594116 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 69.18149089813232 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 70.18439936637878 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 71.18666243553162 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 72.18893694877625 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 73.19117951393127 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 74.19313359260559 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 75.19576644897461 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 76.19833612442017 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 77.20049715042114 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 78.20435619354248 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 79.20632457733154 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 80.2082450389862 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 81.21024990081787 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 82.21247148513794 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 83.21441888809204 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 84.21637320518494 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 85.21903896331787 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 86.22158765792847 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 87.22428059577942 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 88.22724509239197 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 89.22956919670105 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 90.23157835006714 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 91.23390245437622 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 92.23608303070068 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 93.23812937736511 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 94.24120903015137 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 95.24327397346497 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 96.2458381652832 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 97.24825119972229 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 98.25093483924866 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 99.25294971466064 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 100.25548052787781 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 101.25751781463623 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 102.25949025154114 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 103.26156711578369 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 104.26377248764038 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 105.26615071296692 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 106.26816177368164 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 107.27009868621826 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 108.27200222015381 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 109.27393198013306 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 110.27661728858948 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 111.27906799316406 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 112.28118348121643 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 113.28320384025574 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 114.28534841537476 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 115.28734397888184 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 116.28973627090454 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 117.29232597351074 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 118.2943127155304 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 119.29629921913147 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 120.29865336418152 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 121.30116868019104 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 122.30377912521362 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 123.30740165710449 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 124.31108808517456 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 125.31548953056335 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 126.31857442855835 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 127.32106804847717 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 128.3253276348114 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 129.32917594909668 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 130.33185362815857 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 131.3350248336792 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 132.33720326423645 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 133.33984470367432 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 134.34301328659058 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 135.34635257720947 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 136.34887647628784 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 137.3522493839264 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 138.35579633712769 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 139.358295917511 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 140.36085295677185 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 141.36370849609375 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 142.36598944664001 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 143.36933064460754 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 144.37130403518677 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 145.37449288368225 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 146.37700176239014 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 147.3802375793457 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 148.38250184059143 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 149.3860569000244 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 150.38816237449646 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 151.3904893398285 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 152.3929557800293 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 153.39507055282593 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 154.39732837677002 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 155.3993000984192 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 156.4015018939972 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 157.40350580215454 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 158.40657830238342 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 159.4092080593109 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 160.41207098960876 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 161.4147527217865 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 162.41681790351868 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 163.41901397705078 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 164.42175388336182 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 165.42370223999023 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 166.4259991645813 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 167.42848825454712 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 168.43075299263 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 169.43307185173035 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 170.43538284301758 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 171.43756771087646 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 172.43955254554749 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 173.44178104400635 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 174.44429540634155 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 175.44649934768677 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 176.44899606704712 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 177.45134973526 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 178.4532699584961 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 179.45522141456604 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 180.45733952522278 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 181.45929837226868 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 182.46165084838867 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 183.4640531539917 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 184.4660894870758 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 185.4686324596405 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 186.4707133769989 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 187.47362422943115 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 188.47581458091736 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 189.4778106212616 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 190.48050332069397 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 191.48263144493103 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 192.48473238945007 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 193.48723816871643 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 194.48975229263306 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 195.49191188812256 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 196.49439454078674 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 197.4964690208435 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 198.49919033050537 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 199.50177788734436 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 200.50390458106995 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 201.50603199005127 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 202.50806856155396 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 203.51003217697144 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 204.51197934150696 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 205.51428079605103 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 206.51628255844116 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 207.5182421207428 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 208.52018928527832 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 209.52216410636902 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 210.52410292625427 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 211.52660512924194 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 212.52859234809875 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 213.53052520751953 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 214.53261947631836 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 215.53467178344727 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 216.53704929351807 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 217.53932571411133 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 218.5414218902588 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 219.54398107528687 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 220.54659509658813 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 221.54925847053528 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 222.551691532135 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 223.5549201965332 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 224.5569200515747 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 225.55911374092102 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 226.56166434288025 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 227.563782453537 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 228.5663137435913 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 229.56835508346558 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 230.5703706741333 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 231.57227158546448 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 232.57412815093994 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 233.57616901397705 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 234.57857728004456 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 235.58051705360413 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 236.5827236175537 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 237.58505535125732 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 238.5871295928955 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 239.5890917778015 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 240.5917627811432 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 241.5936849117279 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 242.5959689617157 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 243.5989866256714 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 244.60097765922546 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 245.60298871994019 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 246.6053764820099 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 247.60878205299377 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 248.6108250617981 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0\nRequest Rate: 1\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821db2c-3ad8b3563950890026038cbf;a550943b-1281-41b4-9cb5-fccdfd069f35)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0&amp;#x27;, request_rate=1, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/sglang_10_1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    1         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  8.50      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.18      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          230.70    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         326.51    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          557.21    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             3.85      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3270.29   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3545.40   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          30.37     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        24.26     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           53.77     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          11.55     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        11.98     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           13.37     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           11.72     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.95     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            14.34     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 100.72280478477478 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.495853965170681\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.1770447139270126\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 230.70076392969446\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 326.5122036433533\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3270.2904696576297\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3545.402384363115\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1900.9774820061969\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6038.386561041699\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 30.374120362102985\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 24.2576920427382\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 12.718030757385312\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 53.76895607449114\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 11.554640946977461\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 11.984970061138803\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.4664064802308425\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 13.374347250077905\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 11.721820218755436\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.952333316206932\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.9289297785335124\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 14.338939925655719\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.8492781103164004\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1\nRequest Rate: 2\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821db91-4dee6a4f25bca6933c35f224;5eb3118b-5a57-4ed4-a936-7ee067ece5c3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1&amp;#x27;, request_rate=2, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/sglang_10_2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:11,  1.27s/it]\r 20%|\u2588\u2588        | 2/10 [00:01&amp;lt;00:04,  1.71it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:07,  1.02s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.14it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.53it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:04,  1.18s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:01,  1.58it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.48it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.55it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.33it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    2         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  7.53      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.33      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          260.13    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         368.17    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          628.30    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             4.72      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3556.32   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3969.97   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          30.40     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        25.47     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           46.18     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          13.03     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.84     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           15.31     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.76     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         13.18     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            27.77     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 11.854221820831299 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 2\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 7.534644202329218\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.3272026828962482\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 260.13172584766465\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 368.16602423541923\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3556.3241513445973\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3969.9653452262282\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1944.9830224288855\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6257.719596885145\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 30.403116159141064\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 25.47102142125368\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 9.259286162874158\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 46.17720420472324\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 13.032179248841588\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.842418130690595\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.0169617252645353\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 15.310583312794504\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.756570648366933\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 13.178114779293537\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.925960689246034\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 27.772135138511654\nINFO:sglang_benchmarks.utils:CONCURRENCY: 4.719962954913273\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1\nRequest Rate: 2\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821db91-4dee6a4f25bca6933c35f224;5eb3118b-5a57-4ed4-a936-7ee067ece5c3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1&amp;#x27;, request_rate=2, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/sglang_10_2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    2         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  7.53      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.33      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          260.13    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         368.17    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          628.30    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             4.72      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3556.32   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3969.97   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          30.40     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        25.47     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           46.18     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.03     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.84     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           15.31     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.76     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.18     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            27.77     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.854221820831299 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 7.534644202329218\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.3272026828962482\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 260.13172584766465\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 368.16602423541923\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3556.3241513445973\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3969.9653452262282\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1944.9830224288855\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6257.719596885145\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 30.403116159141064\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 25.47102142125368\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 9.259286162874158\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 46.17720420472324\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.032179248841588\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.842418130690595\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.0169617252645353\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 15.310583312794504\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.756570648366933\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 13.178114779293537\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.925960689246034\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 27.772135138511654\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 4.719962954913273\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2\nRequest Rate: 4\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821db9e-4c823658506d63fc499a0a6a;ff69d401-3b49-4db3-9edd-ca1b01c10ebb)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2&amp;#x27;, request_rate=4, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/sglang_10_4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:06,  1.32it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:03,  2.52it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:08,  1.26s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.17it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.60it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:04,  1.05s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.71it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.65it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.43it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    4         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  7.00      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.43      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          280.05    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         396.36    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          676.41    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.23      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3662.60   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 4118.19   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          42.70     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        27.55     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           146.76    \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          13.69     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        13.51     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           15.94     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           13.10     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         13.25     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            28.97     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 11.481121301651001 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 4\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.99874594155699\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.428827404724357\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 280.05017132597396\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 396.3567220705366\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3662.6043010503054\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 4118.192200548947\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1957.078145133885\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6322.114320285618\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 42.70070008933544\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 27.55246264860034\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 39.588421292286576\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 146.76222863607111\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 13.693701221782728\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 13.509819200421651\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.1606930172827947\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 15.941016896860674\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 13.096596339755244\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 13.251043390482664\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.9655673817902755\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 28.971827905625105\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.233229398001975\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2\nRequest Rate: 4\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821db9e-4c823658506d63fc499a0a6a;ff69d401-3b49-4db3-9edd-ca1b01c10ebb)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2&amp;#x27;, request_rate=4, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/sglang_10_4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    4         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  7.00      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.43      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          280.05    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         396.36    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          676.41    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.23      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3662.60   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4118.19   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          42.70     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        27.55     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           146.76    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.69     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.51     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           15.94     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           13.10     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.25     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            28.97     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.481121301651001 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.99874594155699\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.428827404724357\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 280.05017132597396\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 396.3567220705366\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3662.6043010503054\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4118.192200548947\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1957.078145133885\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6322.114320285618\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 42.70070008933544\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 27.55246264860034\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 39.588421292286576\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 146.76222863607111\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.693701221782728\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 13.509819200421651\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.1606930172827947\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 15.941016896860674\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 13.096596339755244\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 13.251043390482664\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.9655673817902755\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 28.971827905625105\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.233229398001975\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3\nRequest Rate: 8\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821dbaa-7f20123b688c9ed84c9bc574;7cc9d6e9-b4fe-42ee-adf2-efc6ecf28dba)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3&amp;#x27;, request_rate=8, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/sglang_10_8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  1.99it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.12it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:09,  1.31s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.19it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.50it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:03,  1.08it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.96it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.69it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.50it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    8         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.65      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.50      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          294.65    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         417.02    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          711.67    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.50      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3659.53   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 4145.27   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          29.40     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        25.82     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           45.00     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          14.26     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        13.50     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           19.76     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           13.13     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         13.34     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            27.50     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 11.285817623138428 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 8\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.651932094246149\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.5033226224076903\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 294.6512339919073\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 417.0216954558933\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3659.5263960771263\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 4145.272179041058\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1916.7869460384895\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6262.70306263119\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 29.40055150538683\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 25.821512565016747\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 8.942155965855902\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 45.000438541173935\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 14.26497791765006\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 13.496801154230607\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.2635817575692743\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 19.756505034724253\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 13.133580269434862\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 13.339450117200613\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 3.2186211712329773\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 27.504693334922194\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.50144881852083\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3\nRequest Rate: 8\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821dbaa-7f20123b688c9ed84c9bc574;7cc9d6e9-b4fe-42ee-adf2-efc6ecf28dba)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3&amp;#x27;, request_rate=8, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/sglang_10_8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    8         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.65      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.50      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          294.65    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         417.02    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          711.67    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.50      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3659.53   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4145.27   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          29.40     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        25.82     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           45.00     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          14.26     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.50     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           19.76     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           13.13     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.34     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            27.50     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.285817623138428 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.651932094246149\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.5033226224076903\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 294.6512339919073\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 417.0216954558933\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3659.5263960771263\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4145.272179041058\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1916.7869460384895\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6262.70306263119\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 29.40055150538683\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 25.821512565016747\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 8.942155965855902\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 45.000438541173935\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 14.26497791765006\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 13.496801154230607\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.2635817575692743\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 19.756505034724253\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 13.133580269434862\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 13.339450117200613\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.2186211712329773\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 27.504693334922194\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.50144881852083\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:11", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:11&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4\nRequest Rate: 16\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821dbb6-3197f1560ee4eb3b70a48663;385f3301-dea0-4448-a87a-fd911e45ea05)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4&amp;#x27;, request_rate=16, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/sglang_10_16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:03,  2.87it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.64it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:09,  1.29s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.11it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.48it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.15it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:00,  2.12it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.04it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.64it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.54it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    16        \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.50      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.54      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          301.57    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         426.81    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          728.38    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.65      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3669.78   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 4162.77   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          33.65     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        31.27     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           48.84     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          14.25     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        13.57     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           19.74     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           13.16     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         13.38     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            14.04     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 10.925732135772705 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 16\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.499377998523414\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.5386087718350723\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 301.56731927967417\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 426.81007330704904\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3669.7824522852898\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 4162.766019813716\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1919.7865550622676\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6257.928727157414\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 33.65161996334791\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 31.27406258136034\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 11.244481139607844\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 48.84092506021261\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 14.245179117060161\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 13.569755023063077\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.2069103914728707\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 19.737829688191415\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 13.155302743787315\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 13.379468582570553\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 3.9046654158750584\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 14.043545294553041\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.646359471812569\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4\nRequest Rate: 16\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821dbb6-3197f1560ee4eb3b70a48663;385f3301-dea0-4448-a87a-fd911e45ea05)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4&amp;#x27;, request_rate=16, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/sglang_10_16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    16        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.50      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.54      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          301.57    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         426.81    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          728.38    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.65      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3669.78   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4162.77   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          33.65     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        31.27     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           48.84     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          14.25     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.57     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           19.74     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           13.16     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.38     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            14.04     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.925732135772705 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.499377998523414\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.5386087718350723\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 301.56731927967417\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 426.81007330704904\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3669.7824522852898\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4162.766019813716\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1919.7865550622676\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6257.928727157414\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 33.65161996334791\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 31.27406258136034\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 11.244481139607844\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 48.84092506021261\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 14.245179117060161\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 13.569755023063077\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.2069103914728707\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 19.737829688191415\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 13.155302743787315\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 13.379468582570553\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.9046654158750584\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 14.043545294553041\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.646359471812569\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5\nRequest Rate: 32\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821dbc1-0474bbae441c298444300011;c2cac09a-d20b-4809-ba31-0e79b5e5c4f3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5&amp;#x27;, request_rate=32, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/sglang_10_32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.74it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:01,  4.16it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:08,  1.27s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.08it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.47it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.17it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.16it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.08it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.65it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.56it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    32        \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.41      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.56      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          305.83    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         432.85    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          738.68    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.71      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3662.23   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 4165.46   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          44.59     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        44.55     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           61.43     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          13.82     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        13.49     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           16.96     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           13.09     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         13.33     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            14.39     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 11.018979787826538 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 32\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.408732526935637\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.5603709404270525\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 305.8327043237023\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 432.84689887446433\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3662.22692579031\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 4165.455515496433\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1923.8360184279911\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6244.5180048421025\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 44.5944438688457\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 44.5512211881578\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 13.949627419428966\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 61.433558939024806\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 13.822379352830517\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 13.490408219350007\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.3850055645531234\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 16.958008152013644\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 13.088377889760604\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 13.326107524335384\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 3.6204910116614086\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 14.389855852350593\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.714432472252699\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5\nRequest Rate: 32\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6821dbc1-0474bbae441c298444300011;c2cac09a-d20b-4809-ba31-0e79b5e5c4f3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5&amp;#x27;, request_rate=32, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/sglang_10_32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    32        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.41      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.56      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          305.83    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         432.85    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          738.68    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.71      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3662.23   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4165.46   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          44.59     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        44.55     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           61.43     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.82     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.49     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           16.96     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           13.09     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.33     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            14.39     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.018979787826538 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.408732526935637\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.5603709404270525\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 305.8327043237023\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 432.84689887446433\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3662.22692579031\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4165.455515496433\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1923.8360184279911\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6244.5180048421025\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 44.5944438688457\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 44.5512211881578\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 13.949627419428966\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 61.433558939024806\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.822379352830517\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 13.490408219350007\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.3850055645531234\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 16.958008152013644\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 13.088377889760604\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 13.326107524335384\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.6204910116614086\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 14.389855852350593\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.714432472252699\n\n"}]}, "renderCollapsed": ["passed"], "initialSort": "result", "title": "shortfin_index.html"}' id="data-container"></div>
<script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
</footer>
</html>