<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<title id="head-title">index.html</title>
<style type="text/css">body {
  font-family: Helvetica, Arial, sans-serif;
  font-size: 12px;
  /* do not increase min-width as some may use split screens */
  min-width: 800px;
  color: #999;
}

h1 {
  font-size: 24px;
  color: black;
}

h2 {
  font-size: 16px;
  color: black;
}

p {
  color: black;
}

a {
  color: #999;
}

table {
  border-collapse: collapse;
}

/******************************
 * SUMMARY INFORMATION
 ******************************/
#environment td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  vertical-align: top;
}
#environment tr:nth-child(odd) {
  background-color: #f6f6f6;
}
#environment ul {
  margin: 0;
  padding: 0 20px;
}

/******************************
 * TEST RESULT COLORS
 ******************************/
span.passed,
.passed .col-result {
  color: green;
}

span.skipped,
span.xfailed,
span.rerun,
.skipped .col-result,
.xfailed .col-result,
.rerun .col-result {
  color: orange;
}

span.error,
span.failed,
span.xpassed,
.error .col-result,
.failed .col-result,
.xpassed .col-result {
  color: red;
}

.col-links__extra {
  margin-right: 3px;
}

/******************************
 * RESULTS TABLE
 *
 * 1. Table Layout
 * 2. Extra
 * 3. Sorting items
 *
 ******************************/
/*------------------
 * 1. Table Layout
 *------------------*/
#results-table {
  border: 1px solid #e6e6e6;
  color: #999;
  font-size: 12px;
  width: 100%;
}
#results-table th,
#results-table td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  text-align: left;
}
#results-table th {
  font-weight: bold;
}

/*------------------
 * 2. Extra
 *------------------*/
.logwrapper {
  max-height: 230px;
  overflow-y: scroll;
  background-color: #e6e6e6;
}
.logwrapper.expanded {
  max-height: none;
}
.logwrapper.expanded .logexpander:after {
  content: "collapse [-]";
}
.logwrapper .logexpander {
  z-index: 1;
  position: sticky;
  top: 10px;
  width: max-content;
  border: 1px solid;
  border-radius: 3px;
  padding: 5px 7px;
  margin: 10px 0 10px calc(100% - 80px);
  cursor: pointer;
  background-color: #e6e6e6;
}
.logwrapper .logexpander:after {
  content: "expand [+]";
}
.logwrapper .logexpander:hover {
  color: #000;
  border-color: #000;
}
.logwrapper .log {
  min-height: 40px;
  position: relative;
  top: -50px;
  height: calc(100% + 50px);
  border: 1px solid #e6e6e6;
  color: black;
  display: block;
  font-family: "Courier New", Courier, monospace;
  padding: 5px;
  padding-right: 80px;
  white-space: pre-wrap;
}

div.media {
  border: 1px solid #e6e6e6;
  float: right;
  height: 240px;
  margin: 0 5px;
  overflow: hidden;
  width: 320px;
}

.media-container {
  display: grid;
  grid-template-columns: 25px auto 25px;
  align-items: center;
  flex: 1 1;
  overflow: hidden;
  height: 200px;
}

.media-container--fullscreen {
  grid-template-columns: 0px auto 0px;
}

.media-container__nav--right,
.media-container__nav--left {
  text-align: center;
  cursor: pointer;
}

.media-container__viewport {
  cursor: pointer;
  text-align: center;
  height: inherit;
}
.media-container__viewport img,
.media-container__viewport video {
  object-fit: cover;
  width: 100%;
  max-height: 100%;
}

.media__name,
.media__counter {
  display: flex;
  flex-direction: row;
  justify-content: space-around;
  flex: 0 0 25px;
  align-items: center;
}

.collapsible td:not(.col-links) {
  cursor: pointer;
}
.collapsible td:not(.col-links):hover::after {
  color: #bbb;
  font-style: italic;
  cursor: pointer;
}

.col-result {
  width: 130px;
}
.col-result:hover::after {
  content: " (hide details)";
}

.col-result.collapsed:hover::after {
  content: " (show details)";
}

#environment-header h2:hover::after {
  content: " (hide details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

#environment-header.collapsed h2:hover::after {
  content: " (show details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

/*------------------
 * 3. Sorting items
 *------------------*/
.sortable {
  cursor: pointer;
}
.sortable.desc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: -12.5px;
  border: 10px solid #4caf50;
  border-bottom: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}
.sortable.asc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: 12.5px;
  border: 10px solid #4caf50;
  border-top: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}

.hidden, .summary__reload__button.hidden {
  display: none;
}

.summary__data {
  flex: 0 0 550px;
}
.summary__reload {
  flex: 1 1;
  display: flex;
  justify-content: center;
}
.summary__reload__button {
  flex: 0 0 300px;
  display: flex;
  color: white;
  font-weight: bold;
  background-color: #4caf50;
  text-align: center;
  justify-content: center;
  align-items: center;
  border-radius: 3px;
  cursor: pointer;
}
.summary__reload__button:hover {
  background-color: #46a049;
}
.summary__spacer {
  flex: 0 0 550px;
}

.controls {
  display: flex;
  justify-content: space-between;
}

.filters,
.collapse {
  display: flex;
  align-items: center;
}
.filters button,
.collapse button {
  color: #999;
  border: none;
  background: none;
  cursor: pointer;
  text-decoration: underline;
}
.filters button:hover,
.collapse button:hover {
  color: #ccc;
}

.filter__label {
  margin-right: 10px;
}

      </style>
</head>
<body>
<h1 id="title">index.html</h1>
<p>Report generated on 05-Jun-2025 at 11:10:28 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
<div id="environment-header">
<h2>Environment</h2>
</div>
<table id="environment"></table>
<!-- TEMPLATES -->
<template id="template_environment_row">
<tr>
<td></td>
<td></td>
</tr>
</template>
<template id="template_results-table__body--empty">
<tbody class="results-table-row">
<tr id="not-found-message">
<td colspan="4">No results found. Check the filters.
</td></tr>
</tbody></template>
<template id="template_results-table__tbody">
<tbody class="results-table-row">
<tr class="collapsible">
</tr>
<tr class="extras-row">
<td class="extra" colspan="4">
<div class="extraHTML"></div>
<div class="media">
<div class="media-container">
<div class="media-container__nav--left">&lt;</div>
<div class="media-container__viewport">
<img src=""/>
<video controls="">
<source src="" type="video/mp4"/>
</video>
</div>
<div class="media-container__nav--right">&gt;</div>
</div>
<div class="media__name"></div>
<div class="media__counter"></div>
</div>
<div class="logwrapper">
<div class="logexpander"></div>
<div class="log"></div>
</div>
</td>
</tr>
</tbody>
</template>
<!-- END TEMPLATES -->
<div class="summary">
<div class="summary__data">
<h2>Summary</h2>
<div class="additional-summary prefix">
</div>
<p class="run-count">18 tests ran in 462 seconds</p>
<p class="filter">(Un)check the boxes to filter the results.</p>
<div class="summary__reload">
<div class="summary__reload__button hidden" onclick="location.reload()">
<div>There are still tests running. <br/>Reload this page to get the latest results!</div>
</div>
</div>
<div class="summary__spacer"></div>
<div class="controls">
<div class="filters">
<input checked="true" class="filter" data-test-result="failed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="failed">0 Failed,</span>
<input checked="true" class="filter" data-test-result="passed" name="filter_checkbox" type="checkbox"/>
<span class="passed">18 Passed,</span>
<input checked="true" class="filter" data-test-result="skipped" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="skipped">0 Skipped,</span>
<input checked="true" class="filter" data-test-result="xfailed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xfailed">0 Expected failures,</span>
<input checked="true" class="filter" data-test-result="xpassed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xpassed">0 Unexpected passes,</span>
<input checked="true" class="filter" data-test-result="error" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="error">0 Errors,</span>
<input checked="true" class="filter" data-test-result="rerun" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="rerun">0 Reruns</span>
</div>
<div class="collapse">
<button id="show_all_details">Show all details</button> / <button id="hide_all_details">Hide all details</button>
</div>
</div>
</div>
<div class="additional-summary summary">
</div>
<div class="additional-summary postfix">
</div>
</div>
<table id="results-table">
<thead id="results-table-head">
<tr>
<th class="sortable" data-column-type="result">Result</th>
<th class="sortable" data-column-type="testId">Test</th>
<th class="sortable" data-column-type="duration">Duration</th>
<th>Links</th>
</tr>
</thead>
</table>
</body>
<footer>
<div data-jsonblob='{"environment": {"Python": "3.11.13", "Platform": "Linux-6.8.0-57-generic-x86_64-with-glibc2.35", "Packages": {"pytest": "8.0.0", "pluggy": "1.6.0"}, "Plugins": {"asyncio": "0.23.8", "anyio": "4.9.0", "metadata": "3.1.1", "timeout": "2.4.0", "xdist": "3.5.0", "html": "4.1.1"}, "CI": "true"}, "tests": {"reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]", "duration": "00:02:59", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:59&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stderr setup -----------------------------\nINFO:integration_tests.llm.model_management:Downloading model SanctumAI/Meta-Llama-3.1-8B-Instruct-GGUF from HuggingFace\nINFO:integration_tests.llm.model_management:Downloading tokenizer NousResearch/Meta-Llama-3.1-8B using transformers\nINFO:integration_tests.llm.model_management:Exporting model with following settings:\n  MLIR Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\n  Config Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json\n  Batch Sizes: 4\nINFO:integration_tests.llm.model_management:Running export command: python -m sharktank.examples.export_paged_llm_v1 --use-attention-mask --block-seq-stride=16 --gguf-file=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/meta-llama-3.1-8b-instruct.f16.gguf --output-mlir=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir --output-config=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json --bs-prefill=4 --bs-decode=4\nINFO:integration_tests.llm.model_management:Export succeeded.\nINFO:integration_tests.llm.model_management:Model successfully exported to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\nINFO:integration_tests.llm.model_management:Compiling model to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\nINFO:integration_tests.llm.model_management:Running compiler command: iree-compile /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir -o /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb --iree-hal-target-device=hip --iree-hip-target=gfx942\nINFO:integration_tests.llm.model_management:Compilation succeeded\nINFO:integration_tests.llm.model_management:Model successfully compiled to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\n[2025-06-05 11:07:13] Started server process [4418]\n[2025-06-05 11:07:13] Waiting for application startup.\n[2025-06-05 11:07:17] Application startup complete.\n[2025-06-05 11:07:17] Uvicorn running on http://0.0.0.0:47585 (Press CTRL+C to quit)\n[2025-06-05 11:07:17] 127.0.0.1:33018 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\n\n------------------------------ Captured log setup ------------------------------\nINFO     integration_tests.llm.model_management:model_management.py:282 Downloading model SanctumAI/Meta-Llama-3.1-8B-Instruct-GGUF from HuggingFace\nINFO     integration_tests.llm.model_management:model_management.py:404 Downloading tokenizer NousResearch/Meta-Llama-3.1-8B using transformers\nINFO     integration_tests.llm.model_management:model_management.py:471 Exporting model with following settings:\n  MLIR Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\n  Config Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json\n  Batch Sizes: 4\nINFO     integration_tests.llm.model_management:model_management.py:502 Running export command: python -m sharktank.examples.export_paged_llm_v1 --use-attention-mask --block-seq-stride=16 --gguf-file=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/meta-llama-3.1-8b-instruct.f16.gguf --output-mlir=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir --output-config=/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json --bs-prefill=4 --bs-decode=4\nINFO     integration_tests.llm.model_management:model_management.py:508 Export succeeded.\nINFO     integration_tests.llm.model_management:model_management.py:515 Model successfully exported to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\nINFO     integration_tests.llm.model_management:model_management.py:521 Compiling model to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\nINFO     integration_tests.llm.model_management:model_management.py:532 Running compiler command: iree-compile /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir -o /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb --iree-hal-target-device=hip --iree-hip-target=gfx942\nINFO     integration_tests.llm.model_management:model_management.py:537 Compilation succeeded\nINFO     integration_tests.llm.model_management:model_management.py:544 Model successfully compiled to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\n\n----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-1...\n::group::Benchmark run on llama31_8b_none-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:47585&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=1, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_1_llama31_8b_none-1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json\n\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 0.00/642M [00:00&amp;lt;?, ?B/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 525k/642M [00:00&amp;lt;02:05, 5.38MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   1%|          | 5.60M/642M [00:00&amp;lt;00:19, 33.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   3%|\u258e         | 19.6M/642M [00:00&amp;lt;00:07, 85.3MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   5%|\u258c         | 33.8M/642M [00:00&amp;lt;00:05, 110MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   7%|\u258b         | 48.0M/642M [00:00&amp;lt;00:05, 124MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  10%|\u2589         | 62.2M/642M [00:00&amp;lt;00:04, 133MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  12%|\u2588\u258f        | 76.4M/642M [00:00&amp;lt;00:04, 138MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  14%|\u2588\u258d        | 90.6M/642M [00:00&amp;lt;00:04, 141MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|\u2588\u258b        | 105M/642M [00:00&amp;lt;00:03, 144MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u258a        | 119M/642M [00:01&amp;lt;00:03, 145MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|\u2588\u2588        | 133M/642M [00:01&amp;lt;00:03, 147MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  23%|\u2588\u2588\u258e       | 147M/642M [00:01&amp;lt;00:03, 147MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  25%|\u2588\u2588\u258c       | 162M/642M [00:01&amp;lt;00:03, 148MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  27%|\u2588\u2588\u258b       | 176M/642M [00:01&amp;lt;00:03, 148MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2589       | 190M/642M [00:01&amp;lt;00:03, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  32%|\u2588\u2588\u2588\u258f      | 204M/642M [00:01&amp;lt;00:03, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  34%|\u2588\u2588\u2588\u258d      | 219M/642M [00:01&amp;lt;00:02, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  36%|\u2588\u2588\u2588\u258b      | 233M/642M [00:01&amp;lt;00:02, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|\u2588\u2588\u2588\u258a      | 247M/642M [00:01&amp;lt;00:02, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588      | 261M/642M [00:02&amp;lt;00:02, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 276M/642M [00:02&amp;lt;00:02, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258c     | 290M/642M [00:02&amp;lt;00:02, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 304M/642M [00:02&amp;lt;00:02, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  50%|\u2588\u2588\u2588\u2588\u2589     | 318M/642M [00:02&amp;lt;00:02, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 333M/642M [00:02&amp;lt;00:02, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 347M/642M [00:02&amp;lt;00:02, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 361M/642M [00:02&amp;lt;00:01, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 375M/642M [00:02&amp;lt;00:01, 148MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 389M/642M [00:02&amp;lt;00:01, 144MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 403M/642M [00:03&amp;lt;00:01, 145MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 418M/642M [00:03&amp;lt;00:01, 146MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 432M/642M [00:03&amp;lt;00:01, 147MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 446M/642M [00:03&amp;lt;00:01, 148MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 460M/642M [00:03&amp;lt;00:01, 148MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 475M/642M [00:03&amp;lt;00:01, 148MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 489M/642M [00:03&amp;lt;00:01, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 503M/642M [00:03&amp;lt;00:00, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 517M/642M [00:03&amp;lt;00:00, 148MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 532M/642M [00:03&amp;lt;00:00, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 546M/642M [00:04&amp;lt;00:00, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 560M/642M [00:04&amp;lt;00:00, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 574M/642M [00:04&amp;lt;00:00, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 588M/642M [00:04&amp;lt;00:00, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 603M/642M [00:04&amp;lt;00:00, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 617M/642M [00:04&amp;lt;00:00, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 631M/642M [00:04&amp;lt;00:00, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642M/642M [00:04&amp;lt;00:00, 143MB/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:07:27] 127.0.0.1:41436 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:07:28] 127.0.0.1:41446 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:07:29] 127.0.0.1:41456 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:07:30] 127.0.0.1:41084 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:07:30] 127.0.0.1:41092 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:07:31] 127.0.0.1:41094 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:19,  2.18s/it][2025-06-05 11:07:31] 127.0.0.1:41104 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:07,  1.01it/s][2025-06-05 11:07:31] 127.0.0.1:41112 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:07:31] 127.0.0.1:41128 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:02&amp;lt;00:02,  2.14it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:02&amp;lt;00:02,  2.34it/s][2025-06-05 11:07:32] 127.0.0.1:41136 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:07:32] 127.0.0.1:41152 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:05,  1.45s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:08&amp;lt;00:04,  1.57s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:09&amp;lt;00:02,  1.36s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:16&amp;lt;00:02,  2.98s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:18&amp;lt;00:00,  2.64s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:18&amp;lt;00:00,  1.80s/it]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    1         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  18.04     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      1467      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1518      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    143       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.33      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          81.34     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         84.16     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          165.50    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.84      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   8537.81   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 7856.96   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          306.04    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        300.01    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           481.18    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          31.55     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        31.99     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           36.49     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           32.53     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         33.03     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            81.69     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 29.130565881729126 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 1\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 18.03606490138918\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1467\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1518\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 143\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.33266680025851236\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 81.33703266320627\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 84.16470046540363\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 8537.806802972531\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 7856.9610335398465\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 4370.815982171876\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 14395.49094135873\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 306.03627433689934\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 300.0125156249851\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 148.68199387629096\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 481.18301033973694\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 31.550059921665632\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 31.985946311335457\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 3.5050605824392176\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 36.489133382296764\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 32.52807924001231\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 33.03180634975433\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 14.79775868884864\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 81.68525346089149\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.8402448703702308\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-1...\n::group::Benchmark run on llama31_8b_none-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 29.130565881729126 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 18.03606490138918\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1467\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1518\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 143\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.33266680025851236\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 81.33703266320627\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 84.16470046540363\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 8537.806802972531\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 7856.9610335398465\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 4370.815982171876\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 14395.49094135873\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 306.03627433689934\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 300.0125156249851\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 148.68199387629096\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 481.18301033973694\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 31.550059921665632\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 31.985946311335457\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 3.5050605824392176\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 36.489133382296764\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 32.52807924001231\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 33.03180634975433\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 14.79775868884864\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 81.68525346089149\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.8402448703702308\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-2...\n::group::Benchmark run on llama31_8b_none-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:47585&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=2, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_2_llama31_8b_none-2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:07:50] 127.0.0.1:54238 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:07:52] 127.0.0.1:54242 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:07:52] 127.0.0.1:54248 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:07:53] 127.0.0.1:54252 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:07:53] 127.0.0.1:54254 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:07:53] 127.0.0.1:54260 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:07:53] 127.0.0.1:54264 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:10,  1.19s/it][2025-06-05 11:07:53] 127.0.0.1:54276 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:07:53] 127.0.0.1:54288 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:01&amp;lt;00:01,  3.81it/s][2025-06-05 11:07:53] 127.0.0.1:54302 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:07:54] 127.0.0.1:54312 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:01&amp;lt;00:00,  4.00it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:02&amp;lt;00:00,  3.89it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:02,  1.10s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.05it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.32it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.55it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    2         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  6.45      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    90        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.62      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          139.47    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         116.05    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          255.52    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.84      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   4574.89   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5480.50   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          310.60    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        312.69    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           475.16    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          23.96     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        22.80     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           27.44     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           22.80     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         22.07     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            29.56     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 11.870187520980835 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 2\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.445653910748661\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 90\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.620573188599169\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 139.47382413766323\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 116.04718626804461\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 4574.8937097378075\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5480.495346011594\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1993.8876575193806\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6158.996903426014\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 310.6010132469237\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 312.69135046750307\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 164.22445805107583\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 475.16042567789555\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 23.96273396778884\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 22.801488785817337\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.0879303816463834\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 27.435672119308702\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 22.80218096420089\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 22.0681291539222\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 12.616277838715055\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 29.55966463778167\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.8390563769542725\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-2...\n::group::Benchmark run on llama31_8b_none-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 11.870187520980835 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.445653910748661\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 90\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.620573188599169\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 139.47382413766323\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 116.04718626804461\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 4574.8937097378075\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5480.495346011594\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1993.8876575193806\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6158.996903426014\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 310.6010132469237\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 312.69135046750307\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 164.22445805107583\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 475.16042567789555\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 23.96273396778884\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 22.801488785817337\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.0879303816463834\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 27.435672119308702\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 22.80218096420089\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 22.0681291539222\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 12.616277838715055\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 29.55966463778167\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.8390563769542725\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]", "duration": "00:00:15", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:15&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-4...\n::group::Benchmark run on llama31_8b_none-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:47585&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=4, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_4_llama31_8b_none-4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:08:06] 127.0.0.1:47032 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:08:07] 127.0.0.1:47044 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:08] 127.0.0.1:47058 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:08] 127.0.0.1:47072 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:08] 127.0.0.1:47078 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:08] 127.0.0.1:47094 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  1.81it/s][2025-06-05 11:08:08] 127.0.0.1:47102 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  2.97it/s][2025-06-05 11:08:08] 127.0.0.1:47104 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:08] 127.0.0.1:47120 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:08] 127.0.0.1:47134 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:08] 127.0.0.1:47136 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:00&amp;lt;00:00,  9.16it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:01,  1.05it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.36it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.54it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  6.48      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    95        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.62      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          138.74    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         115.44    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          254.18    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.93      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   4753.86   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5687.74   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          376.08    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        376.14    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           611.68    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          24.38     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        23.73     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           27.32     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           23.41     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         22.16     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            28.27     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 15.432485580444336 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 4\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.479746149852872\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 95\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6173081332963674\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 138.74000295835856\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 115.4366209264207\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 4753.862676327117\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5687.7444428391755\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2022.9450670328765\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6331.215561027639\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 376.079756882973\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 376.1390333529562\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 235.49305599261797\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 611.6788829490542\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 24.379673931176566\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 23.72688981077964\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.8195067727647596\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 27.323164876778684\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 23.408236704831495\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 22.15794613584876\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 18.185959446055353\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 28.267717519775033\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.9345980946707657\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-4...\n::group::Benchmark run on llama31_8b_none-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 15.432485580444336 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.479746149852872\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 95\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6173081332963674\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 138.74000295835856\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 115.4366209264207\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 4753.862676327117\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5687.7444428391755\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2022.9450670328765\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6331.215561027639\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 376.079756882973\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 376.1390333529562\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 235.49305599261797\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 611.6788829490542\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 24.379673931176566\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 23.72688981077964\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.8195067727647596\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 27.323164876778684\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 23.408236704831495\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 22.15794613584876\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 18.185959446055353\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 28.267717519775033\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.9345980946707657\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]", "duration": "00:00:15", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:15&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-8...\n::group::Benchmark run on llama31_8b_none-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:47585&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=8, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_8_llama31_8b_none-8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:08:21] 127.0.0.1:45156 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:08:23] 127.0.0.1:45158 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:23] 127.0.0.1:45160 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:23] 127.0.0.1:45176 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:23] 127.0.0.1:45178 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:23] 127.0.0.1:45180 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.57it/s][2025-06-05 11:08:23] 127.0.0.1:45196 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:23] 127.0.0.1:45212 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 30%|\u2588\u2588\u2588       | 3/10 [00:00&amp;lt;00:00,  7.22it/s][2025-06-05 11:08:23] 127.0.0.1:45228 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:23] 127.0.0.1:45230 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:23] 127.0.0.1:45232 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01&amp;lt;00:00,  5.13it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:01,  1.04it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.19it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.38it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.61it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    8         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  6.22      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    93        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.64      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          144.49    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         120.22    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          264.71    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.95      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   4591.25   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5548.77   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          272.48    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        290.59    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           431.75    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          24.13     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        23.29     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           27.23     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           23.09     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         22.09     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            29.47     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 14.88376259803772 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 8\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.221795368008316\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 93\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6429012468920938\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 144.4920552389981\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 120.22253316882156\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 4591.252844431438\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5548.771898495033\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2035.2464897934128\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6143.795209764503\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 272.4799335701391\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 290.5938031617552\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 160.7832148136329\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 431.7476056050509\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 24.12719905962983\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 23.288331176726995\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.8864843686483899\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 27.232051029566247\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 23.09256404722956\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 22.08695886656642\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 12.294376462283822\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 29.4676573947072\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.9517221784818437\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-8...\n::group::Benchmark run on llama31_8b_none-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 14.88376259803772 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.221795368008316\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 93\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6429012468920938\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 144.4920552389981\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 120.22253316882156\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 4591.252844431438\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5548.771898495033\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2035.2464897934128\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6143.795209764503\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 272.4799335701391\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 290.5938031617552\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 160.7832148136329\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 431.7476056050509\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 24.12719905962983\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 23.288331176726995\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.8864843686483899\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 27.232051029566247\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 23.09256404722956\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 22.08695886656642\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 12.294376462283822\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 29.4676573947072\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.9517221784818437\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-16...\n::group::Benchmark run on llama31_8b_none-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:47585&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=16, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_16_llama31_8b_none-16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:08:33] 127.0.0.1:55282 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:08:34] 127.0.0.1:55296 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:34] 127.0.0.1:55308 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:35] 127.0.0.1:55322 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:35] 127.0.0.1:55338 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:35] 127.0.0.1:55346 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:01,  6.94it/s][2025-06-05 11:08:35] 127.0.0.1:55358 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:35] 127.0.0.1:55362 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:35] 127.0.0.1:55372 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:35] 127.0.0.1:55376 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:35] 127.0.0.1:55380 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01&amp;lt;00:00,  5.38it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:01,  1.14it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.25it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.42it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.60it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    16        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  6.26      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    91        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.64      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          143.51    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         119.41    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          262.92    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.96      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   4642.96   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5585.95   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          309.41    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        317.67    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           483.98    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          24.22     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        23.37     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           27.37     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           23.17     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         22.06     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            30.97     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 11.886303424835205 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 16\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.264159459155053\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 91\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.638553348790317\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 143.51486514062375\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 119.40947622378928\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 4642.96133746393\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5585.953651927412\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2032.7053890792706\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6218.521155132912\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 309.4102768227458\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 317.6748943515122\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 174.64025603104662\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 483.9784974511713\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 24.21828110915613\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 23.368031548841255\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.9115895174122493\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 27.36655345664008\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 23.170741001657028\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 22.058465518057346\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 12.304107158145596\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 30.970669398084265\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.964778510341562\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-16...\n::group::Benchmark run on llama31_8b_none-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 11.886303424835205 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.264159459155053\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 91\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.638553348790317\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 143.51486514062375\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 119.40947622378928\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 4642.96133746393\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5585.953651927412\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2032.7053890792706\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6218.521155132912\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 309.4102768227458\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 317.6748943515122\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 174.64025603104662\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 483.9784974511713\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 24.21828110915613\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 23.368031548841255\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.9115895174122493\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 27.36655345664008\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 23.170741001657028\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 22.058465518057346\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 12.304107158145596\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 30.970669398084265\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.964778510341562\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_none-32...\n::group::Benchmark run on llama31_8b_none-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_none-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:47585&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=32, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_32_llama31_8b_none-32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:08:44] 127.0.0.1:53734 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:08:46] 127.0.0.1:53744 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:46] 127.0.0.1:53746 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:46] 127.0.0.1:53754 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:46] 127.0.0.1:53766 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:08:46] 127.0.0.1:53772 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  4.03it/s][2025-06-05 11:08:46] 127.0.0.1:53788 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:46] 127.0.0.1:53800 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:46] 127.0.0.1:53814 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:46] 127.0.0.1:53828 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:08:46] 127.0.0.1:53836 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01&amp;lt;00:00,  6.39it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:01,  1.18it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.25it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.42it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.61it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    32        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  6.19      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    93        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.65      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          145.16    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         120.78    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          265.94    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.94      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   4550.30   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5482.27   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          398.54    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        396.24    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           425.39    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          23.48     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        22.30     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           27.27     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           22.20     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         21.87     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            27.96     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 11.574238300323486 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 32\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.193074896000326\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 93\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.645882710474456\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 145.162139179134\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 120.78006685872327\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 4550.297478679568\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5482.274122070521\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2046.3137420420464\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6162.683826084249\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 398.5413237242028\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 396.23839943669736\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 22.048761126075494\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 425.38955259136856\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 23.479492242228012\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 22.303083368494672\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.2841637074470102\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 27.26881131560032\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 22.19978931738452\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 21.86923404224217\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.4545211176290165\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 27.961841905489557\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.938958468994642\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-32...\n::group::Benchmark run on llama31_8b_none-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47585\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 11.574238300323486 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.193074896000326\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 93\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.645882710474456\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 145.162139179134\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 120.78006685872327\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 4550.297478679568\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5482.274122070521\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2046.3137420420464\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6162.683826084249\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 398.5413237242028\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 396.23839943669736\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 22.048761126075494\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 425.38955259136856\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 23.479492242228012\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 22.303083368494672\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.2841637074470102\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 27.26881131560032\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 22.19978931738452\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 21.86923404224217\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.4545211176290165\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 27.961841905489557\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.938958468994642\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]", "duration": "00:00:30", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:30&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stderr setup -----------------------------\n[2025-06-05 11:08:52] Shutting down\n[2025-06-05 11:08:53] Waiting for application shutdown.\n[2025-06-05 11:08:53] Application shutdown complete.\n[2025-06-05 11:08:53] Finished server process [4418]\n[2025-06-05 11:08:54] Started server process [5221]\n[2025-06-05 11:08:54] Waiting for application startup.\n[2025-06-05 11:08:58] Application startup complete.\n[2025-06-05 11:08:58] Uvicorn running on http://0.0.0.0:36669 (Press CTRL+C to quit)\n[2025-06-05 11:08:59] 127.0.0.1:53382 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\n\n----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-1...\n::group::Benchmark run on llama31_8b_trie-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36669&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=1, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_1_llama31_8b_trie-1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:09:02] 127.0.0.1:44964 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:09:04] 127.0.0.1:44970 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:05] 127.0.0.1:44976 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:06] 127.0.0.1:44980 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:06] 127.0.0.1:44990 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:06] 127.0.0.1:45004 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:19,  2.18s/it][2025-06-05 11:09:06] 127.0.0.1:45012 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:07,  1.01it/s][2025-06-05 11:09:06] 127.0.0.1:45028 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:09:07] 127.0.0.1:45032 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:02&amp;lt;00:02,  2.15it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:02&amp;lt;00:02,  2.34it/s][2025-06-05 11:09:07] 127.0.0.1:45048 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:08] 127.0.0.1:45056 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:05,  1.41s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:08&amp;lt;00:04,  1.55s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:09&amp;lt;00:02,  1.35s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:16&amp;lt;00:02,  2.95s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:17&amp;lt;00:00,  2.61s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:17&amp;lt;00:00,  1.79s/it]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    1         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     6         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  17.85     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      1467      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  1518      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    137       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.34      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          82.17     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         85.02     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          167.19    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.83      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   8427.88   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 7772.64   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          309.05    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        300.46    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           487.37    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          31.13     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        31.61     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           35.99     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           32.08     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         32.72     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            60.25     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 23.258362770080566 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 1\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 17.85369771067053\nINFO:sglang_benchmarks.utils:COMPLETED: 6\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1467\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 1518\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 137\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.33606483638479046\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 82.16785249608127\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 85.02440360535199\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 8427.879200549796\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 7772.641640854999\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 4312.244655650707\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 14212.93117434252\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 309.0509707884242\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 300.4642950836569\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 150.1511260474599\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 487.36856386531144\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 31.12836105027261\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 31.607659868918393\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 3.4790999764250428\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 35.98577181598759\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 32.08135424288571\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 32.71821094676852\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 17.770253149553774\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 60.251910351216786\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.832313844603546\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-1...\n::group::Benchmark run on llama31_8b_trie-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 23.258362770080566 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 17.85369771067053\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 6\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1467\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 1518\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 137\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.33606483638479046\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 82.16785249608127\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 85.02440360535199\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 8427.879200549796\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 7772.641640854999\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 4312.244655650707\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 14212.93117434252\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 309.0509707884242\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 300.4642950836569\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 150.1511260474599\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 487.36856386531144\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 31.12836105027261\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 31.607659868918393\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 3.4790999764250428\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 35.98577181598759\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 32.08135424288571\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 32.71821094676852\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 17.770253149553774\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 60.251910351216786\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.832313844603546\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]", "duration": "00:00:15", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:15&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-2...\n::group::Benchmark run on llama31_8b_trie-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36669&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=2, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_2_llama31_8b_trie-2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:09:29] 127.0.0.1:40046 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:09:31] 127.0.0.1:40058 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:31] 127.0.0.1:40062 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:32] 127.0.0.1:40064 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:32] 127.0.0.1:40076 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:32] 127.0.0.1:40090 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:10,  1.16s/it][2025-06-05 11:09:32] 127.0.0.1:40102 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:09:32] 127.0.0.1:40110 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:09:32] 127.0.0.1:40118 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:01&amp;lt;00:01,  3.78it/s][2025-06-05 11:09:32] 127.0.0.1:40128 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:09:33] 127.0.0.1:40130 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:01&amp;lt;00:01,  3.98it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:02&amp;lt;00:00,  3.86it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:02,  1.08s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.07it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.35it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.58it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    2         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  6.35      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    94        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.63      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          141.65    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         117.86    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          259.50    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.85      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   4515.07   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5407.07   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          314.16    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        314.39    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           483.07    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          23.69     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        22.45     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           27.38     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           22.46     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         21.69     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            30.44     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 15.417144536972046 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 2\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.346699237357825\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 94\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6302488664430911\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 141.64843273308472\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 117.85653802485804\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 4515.070174122229\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5407.07260556519\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1954.3852859664405\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6060.557867209427\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 314.1630112659186\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 314.3878437113017\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 168.67090009496482\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 483.0692939693108\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 23.686908055658957\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 22.447134887887046\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.219409083843731\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 27.378719794216206\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 22.46148474228115\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 21.689354442059994\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 13.17850741926512\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 30.44228910002857\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.845617859151545\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-2...\n::group::Benchmark run on llama31_8b_trie-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 15.417144536972046 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.346699237357825\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 94\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6302488664430911\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 141.64843273308472\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 117.85653802485804\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 4515.070174122229\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5407.07260556519\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1954.3852859664405\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6060.557867209427\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 314.1630112659186\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 314.3878437113017\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 168.67090009496482\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 483.0692939693108\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 23.686908055658957\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 22.447134887887046\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.219409083843731\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 27.378719794216206\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 22.46148474228115\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 21.689354442059994\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 13.17850741926512\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 30.44228910002857\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.845617859151545\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]", "duration": "00:00:15", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:15&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-4...\n::group::Benchmark run on llama31_8b_trie-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36669&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=4, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_4_llama31_8b_trie-4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:09:44] 127.0.0.1:44362 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:09:46] 127.0.0.1:44372 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:46] 127.0.0.1:44380 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:46] 127.0.0.1:44386 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:46] 127.0.0.1:44394 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:47] 127.0.0.1:44396 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  1.81it/s][2025-06-05 11:09:47] 127.0.0.1:44410 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:09:47] 127.0.0.1:44416 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  2.79it/s][2025-06-05 11:09:47] 127.0.0.1:44418 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:09:47] 127.0.0.1:44420 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:09:47] 127.0.0.1:44424 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:00&amp;lt;00:00,  9.37it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:01,  1.10it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.42it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.60it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  6.24      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    89        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.64      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          143.99    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         119.80    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          263.79    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.91      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   4544.88   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5459.54   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          311.25    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        311.62    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           477.61    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          23.71     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        22.79     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           26.95     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           22.64     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         21.57     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            28.05     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 14.94369101524353 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 4\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.24365843180567\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 89\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.640650036142864\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 143.98609562310867\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 119.80155675871556\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 4544.880591565743\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5459.542399970815\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1984.8248322703955\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6094.858034476638\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 311.25004030764103\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 311.61956838332117\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 166.07907006680384\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 477.6062311604619\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 23.71257825054673\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 22.789443208377147\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.9580172175396047\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 26.949774679475173\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 22.637452215534\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 21.56579354777932\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 12.382453269421738\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 28.04784589912742\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.911677915251594\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-4...\n::group::Benchmark run on llama31_8b_trie-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 14.94369101524353 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.24365843180567\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 89\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.640650036142864\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 143.98609562310867\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 119.80155675871556\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 4544.880591565743\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5459.542399970815\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1984.8248322703955\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6094.858034476638\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 311.25004030764103\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 311.61956838332117\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 166.07907006680384\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 477.6062311604619\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 23.71257825054673\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 22.789443208377147\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.9580172175396047\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 26.949774679475173\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 22.637452215534\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 21.56579354777932\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 12.382453269421738\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 28.04784589912742\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.911677915251594\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-8...\n::group::Benchmark run on llama31_8b_trie-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36669&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=8, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_8_llama31_8b_trie-8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:09:57] 127.0.0.1:50112 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:09:58] 127.0.0.1:50114 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:58] 127.0.0.1:50128 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:59] 127.0.0.1:50136 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:59] 127.0.0.1:50152 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:09:59] 127.0.0.1:50158 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.57it/s][2025-06-05 11:09:59] 127.0.0.1:50160 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:09:59] 127.0.0.1:50172 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:09:59] 127.0.0.1:50184 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00&amp;lt;00:00, 10.10it/s][2025-06-05 11:09:59] 127.0.0.1:50186 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:09:59] 127.0.0.1:50200 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01&amp;lt;00:00,  4.83it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:01,  1.01it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.16it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.37it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.63it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    8         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  6.15      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    91        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.65      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          146.25    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         121.68    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          267.93    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.96      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   4542.07   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5484.03   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          279.54    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        296.40    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           440.42    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          23.88     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        22.98     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           27.18     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           22.79     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         21.71     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            30.11     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 12.267482995986938 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 8\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.147024984005839\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 91\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6507212855662278\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 146.2496089310097\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 121.6848804008846\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 4542.065562447533\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5484.0334604959935\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2001.8947872363665\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6069.044129415415\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 279.5358835719526\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 296.40008392743766\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 162.26252614341206\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 440.4176577599719\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 23.882796986891254\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 22.979516103831905\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.002024481879607\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 27.181332381204015\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 22.790753119500643\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 21.705285413190722\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 12.574052477395231\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 30.112138111144247\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.95561874192195\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-8...\n::group::Benchmark run on llama31_8b_trie-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 12.267482995986938 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.147024984005839\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 91\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6507212855662278\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 146.2496089310097\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 121.6848804008846\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 4542.065562447533\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5484.0334604959935\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2001.8947872363665\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6069.044129415415\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 279.5358835719526\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 296.40008392743766\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 162.26252614341206\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 440.4176577599719\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 23.882796986891254\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 22.979516103831905\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.002024481879607\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 27.181332381204015\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 22.790753119500643\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 21.705285413190722\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 12.574052477395231\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 30.112138111144247\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.95561874192195\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-16...\n::group::Benchmark run on llama31_8b_trie-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36669&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=16, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_16_llama31_8b_trie-16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:10:08] 127.0.0.1:52818 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:10:10] 127.0.0.1:40660 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:10:10] 127.0.0.1:40662 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:10:10] 127.0.0.1:40678 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:10:10] 127.0.0.1:40694 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:10:10] 127.0.0.1:40702 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:01,  6.98it/s][2025-06-05 11:10:10] 127.0.0.1:40716 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:10:10] 127.0.0.1:40724 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:10:10] 127.0.0.1:40734 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:10:10] 127.0.0.1:40746 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:10:10] 127.0.0.1:40752 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01&amp;lt;00:00,  4.90it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:01,  1.13it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.25it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.44it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.59it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    16        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  6.28      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    90        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.64      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          143.25    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         119.19    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          262.44    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             3.00      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   4706.30   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5645.72   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          371.92    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        381.57    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           610.86    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          24.17     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        23.51     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           27.20     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           23.18     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         21.78     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            27.64     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 11.567306995391846 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 16\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.275810891762376\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 90\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6373678348483057\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 143.2484208821567\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 119.18778511663317\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 4706.2950391555205\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5645.717927021906\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1996.2033482743802\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6231.21674222406\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 371.9242212828249\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 381.57031615264714\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 238.8713495260846\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 610.8576276758686\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 24.17070479345719\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 23.508054075533845\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.8764496638404626\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 27.19933178030168\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 23.176573334575536\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 21.77530131302774\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 18.585467330595968\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 27.64339752495289\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.9996410792638764\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-16...\n::group::Benchmark run on llama31_8b_trie-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 11.567306995391846 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.275810891762376\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 90\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6373678348483057\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 143.2484208821567\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 119.18778511663317\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 4706.2950391555205\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5645.717927021906\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1996.2033482743802\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6231.21674222406\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 371.9242212828249\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 381.57031615264714\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 238.8713495260846\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 610.8576276758686\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 24.17070479345719\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 23.508054075533845\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.8764496638404626\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 27.19933178030168\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 23.176573334575536\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 21.77530131302774\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 18.585467330595968\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 27.64339752495289\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.9996410792638764\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting benchmark run on llama31_8b_trie-32...\n::group::Benchmark run on llama31_8b_trie-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Running SGLang Benchmark with the following settings:\nINFO:sglang_benchmarks.shortfin_benchmark_test:Test parameterization: llama31_8b_trie-32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO:sglang_benchmarks.shortfin_benchmark_test:Fail to load tokenizer config with error=None is not a local folder and is not a valid model identifier listed on &amp;#x27;https://huggingface.co/models&amp;#x27;\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=&amp;lt;your_token&amp;gt;`\nINFO:sglang_benchmarks.shortfin_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:36669&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF&amp;#x27;, request_rate=32, backend=&amp;#x27;shortfin&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/shortfin_10_32_llama31_8b_trie-32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=None, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.shortfin_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.shortfin_benchmark_test:Starting initial single prompt test run...\n[2025-06-05 11:10:20] 127.0.0.1:50542 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\nINFO:sglang_benchmarks.shortfin_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-06-05 11:10:21] 127.0.0.1:50554 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:10:21] 127.0.0.1:50570 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:10:21] 127.0.0.1:50584 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:10:21] 127.0.0.1:50596 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-06-05 11:10:21] 127.0.0.1:50600 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:10:22] 127.0.0.1:50614 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:10:22] 127.0.0.1:50628 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:00,  9.64it/s][2025-06-05 11:10:22] 127.0.0.1:50640 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:10:22] 127.0.0.1:50644 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n[2025-06-05 11:10:22] 127.0.0.1:50658 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 503\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01&amp;lt;00:00,  6.16it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:05&amp;lt;00:01,  1.15it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.23it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.41it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.64it/s]\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.shortfin_benchmark_test:Backend:                                 shortfin  \nINFO:sglang_benchmarks.shortfin_benchmark_test:Traffic request rate:                    32        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Successful requests:                     4         \nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark duration (s):                  6.10      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total input tokens:                      899       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens:                  748       \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total generated tokens (retokenized):    84        \nINFO:sglang_benchmarks.shortfin_benchmark_test:Request throughput (req/s):              0.66      \nINFO:sglang_benchmarks.shortfin_benchmark_test:Input token throughput (tok/s):          147.34    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Output token throughput (tok/s):         122.59    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Total token throughput (tok/s):          269.93    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Concurrency:                             2.94      \nINFO:sglang_benchmarks.shortfin_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean E2E Latency (ms):                   4488.83   \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median E2E Latency (ms):                 5414.97   \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TTFT (ms):                          402.49    \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TTFT (ms):                        398.36    \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TTFT (ms):                           434.93    \nINFO:sglang_benchmarks.shortfin_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean TPOT (ms):                          23.02     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median TPOT (ms):                        21.98     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 TPOT (ms):                           26.44     \nINFO:sglang_benchmarks.shortfin_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.shortfin_benchmark_test:Mean ITL (ms):                           21.85     \nINFO:sglang_benchmarks.shortfin_benchmark_test:Median ITL (ms):                         21.58     \nINFO:sglang_benchmarks.shortfin_benchmark_test:P99 ITL (ms):                            27.88     \nINFO:sglang_benchmarks.shortfin_benchmark_test:==================================================\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run completed in 11.337730884552002 seconds\nINFO:sglang_benchmarks.shortfin_benchmark_test:\n\n======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: shortfin\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 32\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.101560450159013\nINFO:sglang_benchmarks.utils:COMPLETED: 4\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 899\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 748\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 84\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 0.6555700025713513\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 147.3393580779112\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 122.5915904808427\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 4488.82998409681\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 5414.972343714908\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 2021.9835457015035\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6072.7500882186\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 402.4877641350031\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 398.3570069540292\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 25.19513182041247\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 434.9274232937023\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 23.018239883517573\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 21.981376187293684\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.0656785932520947\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 26.44141934187292\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 21.84954777056162\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 21.57899527810514\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.6316689344148276\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 27.882632082328197\nINFO:sglang_benchmarks.utils:CONCURRENCY: 2.942742284216705\nINFO:sglang_benchmarks.shortfin_benchmark_test:Benchmark run successful\n::endgroup::\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-32...\n::group::Benchmark run on llama31_8b_trie-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:36669\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 11.337730884552002 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.101560450159013\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 4\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 899\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 748\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 84\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.6555700025713513\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 147.3393580779112\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 122.5915904808427\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 4488.82998409681\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 5414.972343714908\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 2021.9835457015035\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6072.7500882186\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 402.4877641350031\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 398.3570069540292\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 25.19513182041247\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 434.9274232937023\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 23.018239883517573\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 21.981376187293684\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.0656785932520947\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 26.44141934187292\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 21.84954777056162\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 21.57899527810514\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.6316689344148276\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 27.882632082328197\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.942742284216705\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n--------------------------- Captured stderr teardown ---------------------------\n[2025-06-05 11:10:28] Shutting down\n[2025-06-05 11:10:28] Waiting for application shutdown.\n[2025-06-05 11:10:28] Application shutdown complete.\n[2025-06-05 11:10:28] Finished server process [5221]\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]", "duration": "00:01:04", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:01:04&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 0 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 1.0016756057739258 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 2.0027945041656494 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 3.004002094268799 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 4.005511522293091 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 5.0068933963775635 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 6.010197877883911 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 7.011489629745483 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 8.012831211090088 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 9.014634370803833 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 10.016640424728394 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 11.018638372421265 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 12.020830154418945 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 13.022769927978516 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 14.02427887916565 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 15.026044845581055 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 16.0284423828125 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 17.03034520149231 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 18.032459497451782 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 19.033847332000732 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 20.035146236419678 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 21.03713607788086 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 22.039060831069946 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 23.040993452072144 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 24.042552709579468 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 25.043938398361206 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 26.045408248901367 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 27.04672598838806 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 28.047852516174316 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 29.049161195755005 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 30.050281524658203 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 31.051339626312256 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 32.05244493484497 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 33.05352711677551 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 34.0546452999115 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 35.05593013763428 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 36.056971073150635 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 37.05802392959595 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server has not started yet; waited 38.059107065200806 seconds; timeout: 600 seconds.\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0\nRequest Rate: 1\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417baf-1370ce745a4b0ed418c08da0;f460188d-d037-4e12-9969-587fc89b7dd0)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0&amp;#x27;, request_rate=1, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/sglang_10_1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json\n\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 0.00/642M [00:00&amp;lt;?, ?B/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   2%|\u258f         | 14.2M/642M [00:00&amp;lt;00:04, 149MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   5%|\u258d         | 30.4M/642M [00:00&amp;lt;00:03, 161MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   7%|\u258b         | 46.7M/642M [00:00&amp;lt;00:03, 166MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  10%|\u2589         | 63.1M/642M [00:00&amp;lt;00:03, 168MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  12%|\u2588\u258f        | 79.5M/642M [00:00&amp;lt;00:03, 169MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  15%|\u2588\u258d        | 95.8M/642M [00:00&amp;lt;00:03, 170MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 112M/642M [00:00&amp;lt;00:03, 171MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2588        | 129M/642M [00:00&amp;lt;00:03, 171MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  23%|\u2588\u2588\u258e       | 145M/642M [00:00&amp;lt;00:03, 171MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  25%|\u2588\u2588\u258c       | 161M/642M [00:01&amp;lt;00:02, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  28%|\u2588\u2588\u258a       | 178M/642M [00:01&amp;lt;00:02, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2588       | 194M/642M [00:01&amp;lt;00:02, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 211M/642M [00:01&amp;lt;00:02, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  35%|\u2588\u2588\u2588\u258c      | 227M/642M [00:01&amp;lt;00:02, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  38%|\u2588\u2588\u2588\u258a      | 244M/642M [00:01&amp;lt;00:02, 167MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588      | 260M/642M [00:01&amp;lt;00:02, 169MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  43%|\u2588\u2588\u2588\u2588\u258e     | 276M/642M [00:01&amp;lt;00:02, 170MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 293M/642M [00:01&amp;lt;00:02, 171MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  48%|\u2588\u2588\u2588\u2588\u258a     | 309M/642M [00:01&amp;lt;00:02, 171MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  51%|\u2588\u2588\u2588\u2588\u2588     | 326M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 342M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 359M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 375M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 392M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 408M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 425M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 441M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 457M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 474M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 490M/642M [00:03&amp;lt;00:00, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 507M/642M [00:03&amp;lt;00:00, 167MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 523M/642M [00:03&amp;lt;00:00, 168MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 539M/642M [00:03&amp;lt;00:00, 169MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 556M/642M [00:03&amp;lt;00:00, 170MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 572M/642M [00:03&amp;lt;00:00, 171MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 589M/642M [00:03&amp;lt;00:00, 171MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 605M/642M [00:03&amp;lt;00:00, 171MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 621M/642M [00:03&amp;lt;00:00, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 638M/642M [00:03&amp;lt;00:00, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642M/642M [00:03&amp;lt;00:00, 171MB/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:18,  2.10s/it]\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:08,  1.11s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.40it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:04&amp;lt;00:03,  1.25it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:05,  1.25s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:07&amp;lt;00:02,  1.03it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:07&amp;lt;00:01,  1.33it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:08&amp;lt;00:00,  1.18it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.59it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.18it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    1         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  8.51      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.18      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          230.34    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         326.00    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          556.34    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             3.85      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3279.05   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3592.57   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          26.11     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        20.11     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           43.31     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          11.65     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        11.92     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           12.76     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           11.77     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.48     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            22.57     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 22.01105046272278 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 1\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 8.509195054881275\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.1751992915315215\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 230.33906114017822\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 326.00028347084407\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3279.04610959813\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3592.5659304484725\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1861.8051417395043\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6051.701207850128\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 26.11044580116868\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 20.10571933351457\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 9.840997657302038\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 43.30802531912923\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 11.650745721463315\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 11.91529516676369\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.9471707398082891\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 12.756933688842034\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 11.768921965452831\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.478152522817254\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.4217957635714784\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 22.568755992688235\nINFO:sglang_benchmarks.utils:CONCURRENCY: 3.853532664898914\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 0 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 1.0016756057739258 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 2.0027945041656494 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 3.004002094268799 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 4.005511522293091 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 5.0068933963775635 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 6.010197877883911 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 7.011489629745483 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 8.012831211090088 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 9.014634370803833 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 10.016640424728394 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 11.018638372421265 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 12.020830154418945 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 13.022769927978516 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 14.02427887916565 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 15.026044845581055 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 16.0284423828125 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 17.03034520149231 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 18.032459497451782 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 19.033847332000732 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 20.035146236419678 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 21.03713607788086 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 22.039060831069946 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 23.040993452072144 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 24.042552709579468 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 25.043938398361206 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 26.045408248901367 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 27.04672598838806 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 28.047852516174316 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 29.049161195755005 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 30.050281524658203 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 31.051339626312256 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 32.05244493484497 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 33.05352711677551 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 34.0546452999115 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 35.05593013763428 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 36.056971073150635 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 37.05802392959595 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 38.059107065200806 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0\nRequest Rate: 1\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417baf-1370ce745a4b0ed418c08da0;f460188d-d037-4e12-9969-587fc89b7dd0)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0&amp;#x27;, request_rate=1, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/sglang_10_1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    1         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  8.51      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.18      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          230.34    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         326.00    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          556.34    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             3.85      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3279.05   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3592.57   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          26.11     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        20.11     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           43.31     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          11.65     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        11.92     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           12.76     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           11.77     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.48     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            22.57     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 22.01105046272278 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.509195054881275\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.1751992915315215\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 230.33906114017822\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 326.00028347084407\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3279.04610959813\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3592.5659304484725\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1861.8051417395043\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6051.701207850128\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 26.11044580116868\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 20.10571933351457\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 9.840997657302038\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 43.30802531912923\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 11.650745721463315\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 11.91529516676369\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.9471707398082891\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 12.756933688842034\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 11.768921965452831\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.478152522817254\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.4217957635714784\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 22.568755992688235\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.853532664898914\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1\nRequest Rate: 2\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417bc6-0ed101cc7539bd6b772ebd8c;94b5d54c-7e0c-4232-bafd-f5b31b378fca)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1&amp;#x27;, request_rate=2, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/sglang_10_2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:11,  1.28s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:06,  1.05it/s]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.22it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.66it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:04,  1.07s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:05&amp;lt;00:02,  1.31it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.59it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.60it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.35it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    2         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  7.40      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.35      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          264.75    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         374.70    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          639.45    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             4.63      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3427.94   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3815.58   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          27.49     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        22.91     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           46.54     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          12.62     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.55     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           14.42     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.30     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.55     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            25.79     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 11.940057516098022 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 2\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 7.40322248917073\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.3507631324909901\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 264.7495739682341\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 374.70169295300065\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3427.9366713482887\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3815.575029933825\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1864.635845946703\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6124.082752810791\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 27.49231355264783\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 22.912779124453664\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 11.063409329325193\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 46.5360256517306\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 12.617976086374433\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.550585236130317\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.7547204253228019\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 14.41689373067589\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.302604570476722\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.553937966004014\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.473863956767091\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 25.794487539678805\nINFO:sglang_benchmarks.utils:CONCURRENCY: 4.630330476171152\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1\nRequest Rate: 2\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417bc6-0ed101cc7539bd6b772ebd8c;94b5d54c-7e0c-4232-bafd-f5b31b378fca)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1&amp;#x27;, request_rate=2, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/sglang_10_2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    2         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  7.40      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.35      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          264.75    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         374.70    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          639.45    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             4.63      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3427.94   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3815.58   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          27.49     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        22.91     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           46.54     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.62     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.55     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           14.42     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.30     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.55     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            25.79     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.940057516098022 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 7.40322248917073\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.3507631324909901\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 264.7495739682341\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 374.70169295300065\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3427.9366713482887\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3815.575029933825\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1864.635845946703\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6124.082752810791\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 27.49231355264783\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 22.912779124453664\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 11.063409329325193\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 46.5360256517306\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 12.617976086374433\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.550585236130317\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.7547204253228019\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 14.41689373067589\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.302604570476722\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.553937966004014\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.473863956767091\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 25.794487539678805\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 4.630330476171152\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2\nRequest Rate: 4\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417bd2-40da635c3b8265390b9006f6;d2f18ad7-a8cb-4457-8aae-433bf377843f)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2&amp;#x27;, request_rate=4, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/sglang_10_4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:06,  1.33it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:03,  2.57it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:08,  1.22s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.25it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:02,  1.67it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:03,  1.03it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.81it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.62it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.46it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    4         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.83      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.46      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          287.12    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         406.36    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          693.48    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.09      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3476.52   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3890.14   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          26.21     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        21.64     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           48.02     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          13.02     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.69     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           15.31     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.48     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.56     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            27.59     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 11.3082115650177 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 4\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.82647730410099\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.4648843839255898\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 287.1173392494156\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 406.3589281009586\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3476.5227844007313\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3890.139969298616\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1862.2731081325396\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6136.697365203872\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 26.208577305078506\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 21.64365048520267\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 11.674284270796841\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 48.022642666473985\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 13.016057887127197\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.687813039006041\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 1.0332347003932512\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 15.307875842554495\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.483030874194515\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.563369702547789\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.429892056013879\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 27.5862485403195\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.092703937230141\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2\nRequest Rate: 4\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417bd2-40da635c3b8265390b9006f6;d2f18ad7-a8cb-4457-8aae-433bf377843f)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2&amp;#x27;, request_rate=4, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/sglang_10_4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    4         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.83      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.46      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          287.12    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         406.36    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          693.48    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.09      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3476.52   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3890.14   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          26.21     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        21.64     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           48.02     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.02     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.69     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           15.31     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.48     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.56     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            27.59     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.3082115650177 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.82647730410099\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.4648843839255898\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 287.1173392494156\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 406.3589281009586\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3476.5227844007313\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3890.139969298616\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1862.2731081325396\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6136.697365203872\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 26.208577305078506\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 21.64365048520267\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 11.674284270796841\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 48.022642666473985\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.016057887127197\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.687813039006041\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.0332347003932512\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 15.307875842554495\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.483030874194515\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.563369702547789\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.429892056013879\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 27.5862485403195\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.092703937230141\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3\nRequest Rate: 8\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417bde-5bf4f42f56df99d07ff32ff5;be98852b-2718-417d-8655-6e024cd6f2a1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3&amp;#x27;, request_rate=8, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/sglang_10_8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  2.03it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.21it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:08,  1.24s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.24it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.58it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.15it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.00it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.66it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.53it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    8         \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.53      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.53      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          300.15    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         424.81    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          724.96    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.36      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3500.89   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3931.46   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          27.52     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        25.72     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           42.20     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          13.66     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.87     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           19.05     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.57     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.59     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            25.23     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 11.056481838226318 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 8\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.530010065995157\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.5313912075074307\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 300.1526766714564\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 424.80792096256124\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3500.8858112152666\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3931.4618434291333\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1851.019849688418\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6135.072856908664\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 27.52349143847823\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 25.715744821354747\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 9.895182652100873\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 42.195881768129766\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 13.656059603182852\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.865352948099023\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.2105018947992336\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 19.049490623222663\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.566414499802276\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.587945209816098\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 2.9879236071103223\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 25.233953753486265\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.361225749782578\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3\nRequest Rate: 8\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417bde-5bf4f42f56df99d07ff32ff5;be98852b-2718-417d-8655-6e024cd6f2a1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3&amp;#x27;, request_rate=8, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/sglang_10_8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    8         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.53      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.53      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          300.15    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         424.81    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          724.96    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.36      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3500.89   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3931.46   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          27.52     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        25.72     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           42.20     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.66     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.87     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           19.05     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.57     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.59     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            25.23     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.056481838226318 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.530010065995157\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.5313912075074307\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 300.1526766714564\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 424.80792096256124\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3500.8858112152666\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3931.4618434291333\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1851.019849688418\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6135.072856908664\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 27.52349143847823\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 25.715744821354747\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 9.895182652100873\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 42.195881768129766\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.656059603182852\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.865352948099023\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.2105018947992336\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 19.049490623222663\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.566414499802276\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.587945209816098\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.9879236071103223\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 25.233953753486265\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.361225749782578\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:11", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:11&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4\nRequest Rate: 16\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417be9-626036d20f06244a6346c8ff;ef1b54ad-ba9d-4d49-870e-9c6fab94afaa)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4&amp;#x27;, request_rate=16, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/sglang_10_16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:03,  2.94it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.79it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:08,  1.23s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.17it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.55it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.22it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.23it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.06it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.59it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.57it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    16        \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.38      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.57      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          307.21    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         434.80    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          742.01    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.50      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3506.75   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3944.45   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          32.87     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        30.20     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           48.13     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          13.57     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.86     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           18.83     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.57     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.65     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            13.30     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 10.72655177116394 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 16\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.3799853241071105\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.567401724611258\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 307.2107380238065\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 434.7972384071629\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3506.752058630809\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3944.4474736228585\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1854.104954778056\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6129.645007336513\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 32.870350731536746\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 30.202964320778847\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 10.98248705373083\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 48.125458913855255\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 13.572438524591227\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.855337346796016\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 2.0800634299877783\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 18.82968267807737\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.568297360952291\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.645500712096691\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 3.671312489169875\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 13.297903011552986\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.496489224482009\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4\nRequest Rate: 16\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417be9-626036d20f06244a6346c8ff;ef1b54ad-ba9d-4d49-870e-9c6fab94afaa)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4&amp;#x27;, request_rate=16, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/sglang_10_16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    16        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.38      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.57      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          307.21    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         434.80    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          742.01    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.50      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3506.75   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3944.45   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          32.87     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        30.20     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           48.13     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.57     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.86     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           18.83     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.57     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.65     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            13.30     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.72655177116394 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.3799853241071105\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.567401724611258\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 307.2107380238065\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 434.7972384071629\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3506.752058630809\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3944.4474736228585\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1854.104954778056\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6129.645007336513\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 32.870350731536746\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 30.202964320778847\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 10.98248705373083\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 48.125458913855255\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.572438524591227\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.855337346796016\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.0800634299877783\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 18.82968267807737\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.568297360952291\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.645500712096691\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.671312489169875\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 13.297903011552986\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.496489224482009\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:11", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:11&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json...\nINFO:sglang_benchmarks.sglang_benchmark_test:Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO:sglang_benchmarks.sglang_benchmark_test:Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json\nINFO:sglang_benchmarks.sglang_benchmark_test:Beginning SGLang benchmark test...\nINFO:sglang_benchmarks.sglang_benchmark_test:Waiting for server to start at http://localhost:30000...\nINFO:sglang_benchmarks.sglang_benchmark_test:Server successfully started\nINFO:sglang_benchmarks.sglang_benchmark_test:Running SGLang Benchmark with the following args:\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5\nRequest Rate: 32\nINFO:sglang_benchmarks.sglang_benchmark_test:Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417bf4-56706f777c36843003238958;edd82232-6efd-4e92-a719-b1cf2b0a753c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO:sglang_benchmarks.sglang_benchmark_test:\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO:sglang_benchmarks.sglang_benchmark_test:Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5&amp;#x27;, request_rate=32, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/sglang_10_32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO:sglang_benchmarks.sglang_benchmark_test:#Input tokens: 1960\nINFO:sglang_benchmarks.sglang_benchmark_test:#Output tokens: 2774\nINFO:sglang_benchmarks.sglang_benchmark_test:Starting initial single prompt test run...\nINFO:sglang_benchmarks.sglang_benchmark_test:Initial test run completed. Starting main benchmark run...\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.79it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:01,  4.32it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:08,  1.20s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.16it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.56it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.25it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.28it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.10it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.60it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.59it/s]\nINFO:sglang_benchmarks.sglang_benchmark_test:\n============ Serving Benchmark Result ============\nINFO:sglang_benchmarks.sglang_benchmark_test:Backend:                                 sglang    \nINFO:sglang_benchmarks.sglang_benchmark_test:Traffic request rate:                    32        \nINFO:sglang_benchmarks.sglang_benchmark_test:Max reqeuest concurrency:                not set   \nINFO:sglang_benchmarks.sglang_benchmark_test:Successful requests:                     10        \nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark duration (s):                  6.28      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total input tokens:                      1960      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens:                  2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Total generated tokens (retokenized):    2774      \nINFO:sglang_benchmarks.sglang_benchmark_test:Request throughput (req/s):              1.59      \nINFO:sglang_benchmarks.sglang_benchmark_test:Input token throughput (tok/s):          312.31    \nINFO:sglang_benchmarks.sglang_benchmark_test:Output token throughput (tok/s):         442.02    \nINFO:sglang_benchmarks.sglang_benchmark_test:Total token throughput (tok/s):          754.33    \nINFO:sglang_benchmarks.sglang_benchmark_test:Concurrency:                             5.55      \nINFO:sglang_benchmarks.sglang_benchmark_test:----------------End-to-End Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean E2E Latency (ms):                   3484.35   \nINFO:sglang_benchmarks.sglang_benchmark_test:Median E2E Latency (ms):                 3921.27   \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Time to First Token----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TTFT (ms):                          52.30     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TTFT (ms):                        51.08     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TTFT (ms):                           80.07     \nINFO:sglang_benchmarks.sglang_benchmark_test:-----Time per Output Token (excl. 1st token)------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean TPOT (ms):                          12.89     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median TPOT (ms):                        12.70     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 TPOT (ms):                           14.66     \nINFO:sglang_benchmarks.sglang_benchmark_test:---------------Inter-token Latency----------------\nINFO:sglang_benchmarks.sglang_benchmark_test:Mean ITL (ms):                           12.42     \nINFO:sglang_benchmarks.sglang_benchmark_test:Median ITL (ms):                         12.54     \nINFO:sglang_benchmarks.sglang_benchmark_test:P99 ITL (ms):                            13.18     \nINFO:sglang_benchmarks.sglang_benchmark_test:==================================================\nINFO:sglang_benchmarks.sglang_benchmark_test:Benchmark run completed in 10.769965887069702 seconds\nINFO:sglang_benchmarks.sglang_benchmark_test:======== RESULTS ========\nINFO:sglang_benchmarks.utils:BACKEND: sglang\nINFO:sglang_benchmarks.utils:DATASET_NAME: sharegpt\nINFO:sglang_benchmarks.utils:REQUEST_RATE: 32\nINFO:sglang_benchmarks.utils:MAX_CONCURRENCY: None\nINFO:sglang_benchmarks.utils:SHAREGPT_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_INPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_OUTPUT_LEN: None\nINFO:sglang_benchmarks.utils:RANDOM_RANGE_RATIO: 0.0\nINFO:sglang_benchmarks.utils:DURATION: 6.275754652917385\nINFO:sglang_benchmarks.utils:COMPLETED: 10\nINFO:sglang_benchmarks.utils:TOTAL_INPUT_TOKENS: 1960\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS: 2774\nINFO:sglang_benchmarks.utils:TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO:sglang_benchmarks.utils:REQUEST_THROUGHPUT: 1.593433866212622\nINFO:sglang_benchmarks.utils:INPUT_THROUGHPUT: 312.3130377776739\nINFO:sglang_benchmarks.utils:OUTPUT_THROUGHPUT: 442.0185544873813\nINFO:sglang_benchmarks.utils:MEAN_E2E_LATENCY_MS: 3484.348489716649\nINFO:sglang_benchmarks.utils:MEDIAN_E2E_LATENCY_MS: 3921.2676817551255\nINFO:sglang_benchmarks.utils:STD_E2E_LATENCY_MS: 1851.577155738505\nINFO:sglang_benchmarks.utils:P99_E2E_LATENCY_MS: 6104.295963542536\nINFO:sglang_benchmarks.utils:MEAN_TTFT_MS: 52.29690163396299\nINFO:sglang_benchmarks.utils:MEDIAN_TTFT_MS: 51.08101572841406\nINFO:sglang_benchmarks.utils:STD_TTFT_MS: 21.131855118648467\nINFO:sglang_benchmarks.utils:P99_TTFT_MS: 80.07441461551934\nINFO:sglang_benchmarks.utils:MEAN_TPOT_MS: 12.892374536446233\nINFO:sglang_benchmarks.utils:MEDIAN_TPOT_MS: 12.702393596707614\nINFO:sglang_benchmarks.utils:STD_TPOT_MS: 0.871303226880051\nINFO:sglang_benchmarks.utils:P99_TPOT_MS: 14.657671384350396\nINFO:sglang_benchmarks.utils:MEAN_ITL_MS: 12.416957930645566\nINFO:sglang_benchmarks.utils:MEDIAN_ITL_MS: 12.538734590634704\nINFO:sglang_benchmarks.utils:STD_ITL_MS: 3.304180261965568\nINFO:sglang_benchmarks.utils:P99_ITL_MS: 13.18158451002091\nINFO:sglang_benchmarks.utils:CONCURRENCY: 5.55207888520131\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5\nRequest Rate: 32\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-68417bf4-56706f777c36843003238958;edd82232-6efd-4e92-a719-b1cf2b0a753c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5&amp;#x27;, request_rate=32, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/sglang_10_32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    32        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.28      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.59      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          312.31    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         442.02    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          754.33    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.55      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3484.35   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3921.27   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          52.30     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        51.08     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           80.07     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.89     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.70     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           14.66     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.42     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.54     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            13.18     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.769965887069702 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.275754652917385\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.593433866212622\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 312.3130377776739\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 442.0185544873813\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3484.348489716649\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3921.2676817551255\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1851.577155738505\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6104.295963542536\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 52.29690163396299\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 51.08101572841406\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 21.131855118648467\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 80.07441461551934\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 12.892374536446233\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.702393596707614\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.871303226880051\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 14.657671384350396\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.416957930645566\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.538734590634704\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.304180261965568\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 13.18158451002091\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.55207888520131\n\n"}]}, "renderCollapsed": ["passed"], "initialSort": "result", "title": "shortfin_index.html"}' id="data-container"></div>
<script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
</footer>
</html>