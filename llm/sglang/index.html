<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<title id="head-title">index.html</title>
<style type="text/css">body {
  font-family: Helvetica, Arial, sans-serif;
  font-size: 12px;
  /* do not increase min-width as some may use split screens */
  min-width: 800px;
  color: #999;
}

h1 {
  font-size: 24px;
  color: black;
}

h2 {
  font-size: 16px;
  color: black;
}

p {
  color: black;
}

a {
  color: #999;
}

table {
  border-collapse: collapse;
}

/******************************
 * SUMMARY INFORMATION
 ******************************/
#environment td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  vertical-align: top;
}
#environment tr:nth-child(odd) {
  background-color: #f6f6f6;
}
#environment ul {
  margin: 0;
  padding: 0 20px;
}

/******************************
 * TEST RESULT COLORS
 ******************************/
span.passed,
.passed .col-result {
  color: green;
}

span.skipped,
span.xfailed,
span.rerun,
.skipped .col-result,
.xfailed .col-result,
.rerun .col-result {
  color: orange;
}

span.error,
span.failed,
span.xpassed,
.error .col-result,
.failed .col-result,
.xpassed .col-result {
  color: red;
}

.col-links__extra {
  margin-right: 3px;
}

/******************************
 * RESULTS TABLE
 *
 * 1. Table Layout
 * 2. Extra
 * 3. Sorting items
 *
 ******************************/
/*------------------
 * 1. Table Layout
 *------------------*/
#results-table {
  border: 1px solid #e6e6e6;
  color: #999;
  font-size: 12px;
  width: 100%;
}
#results-table th,
#results-table td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  text-align: left;
}
#results-table th {
  font-weight: bold;
}

/*------------------
 * 2. Extra
 *------------------*/
.logwrapper {
  max-height: 230px;
  overflow-y: scroll;
  background-color: #e6e6e6;
}
.logwrapper.expanded {
  max-height: none;
}
.logwrapper.expanded .logexpander:after {
  content: "collapse [-]";
}
.logwrapper .logexpander {
  z-index: 1;
  position: sticky;
  top: 10px;
  width: max-content;
  border: 1px solid;
  border-radius: 3px;
  padding: 5px 7px;
  margin: 10px 0 10px calc(100% - 80px);
  cursor: pointer;
  background-color: #e6e6e6;
}
.logwrapper .logexpander:after {
  content: "expand [+]";
}
.logwrapper .logexpander:hover {
  color: #000;
  border-color: #000;
}
.logwrapper .log {
  min-height: 40px;
  position: relative;
  top: -50px;
  height: calc(100% + 50px);
  border: 1px solid #e6e6e6;
  color: black;
  display: block;
  font-family: "Courier New", Courier, monospace;
  padding: 5px;
  padding-right: 80px;
  white-space: pre-wrap;
}

div.media {
  border: 1px solid #e6e6e6;
  float: right;
  height: 240px;
  margin: 0 5px;
  overflow: hidden;
  width: 320px;
}

.media-container {
  display: grid;
  grid-template-columns: 25px auto 25px;
  align-items: center;
  flex: 1 1;
  overflow: hidden;
  height: 200px;
}

.media-container--fullscreen {
  grid-template-columns: 0px auto 0px;
}

.media-container__nav--right,
.media-container__nav--left {
  text-align: center;
  cursor: pointer;
}

.media-container__viewport {
  cursor: pointer;
  text-align: center;
  height: inherit;
}
.media-container__viewport img,
.media-container__viewport video {
  object-fit: cover;
  width: 100%;
  max-height: 100%;
}

.media__name,
.media__counter {
  display: flex;
  flex-direction: row;
  justify-content: space-around;
  flex: 0 0 25px;
  align-items: center;
}

.collapsible td:not(.col-links) {
  cursor: pointer;
}
.collapsible td:not(.col-links):hover::after {
  color: #bbb;
  font-style: italic;
  cursor: pointer;
}

.col-result {
  width: 130px;
}
.col-result:hover::after {
  content: " (hide details)";
}

.col-result.collapsed:hover::after {
  content: " (show details)";
}

#environment-header h2:hover::after {
  content: " (hide details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

#environment-header.collapsed h2:hover::after {
  content: " (show details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

/*------------------
 * 3. Sorting items
 *------------------*/
.sortable {
  cursor: pointer;
}
.sortable.desc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: -12.5px;
  border: 10px solid #4caf50;
  border-bottom: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}
.sortable.asc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: 12.5px;
  border: 10px solid #4caf50;
  border-top: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}

.hidden, .summary__reload__button.hidden {
  display: none;
}

.summary__data {
  flex: 0 0 550px;
}
.summary__reload {
  flex: 1 1;
  display: flex;
  justify-content: center;
}
.summary__reload__button {
  flex: 0 0 300px;
  display: flex;
  color: white;
  font-weight: bold;
  background-color: #4caf50;
  text-align: center;
  justify-content: center;
  align-items: center;
  border-radius: 3px;
  cursor: pointer;
}
.summary__reload__button:hover {
  background-color: #46a049;
}
.summary__spacer {
  flex: 0 0 550px;
}

.controls {
  display: flex;
  justify-content: space-between;
}

.filters,
.collapse {
  display: flex;
  align-items: center;
}
.filters button,
.collapse button {
  color: #999;
  border: none;
  background: none;
  cursor: pointer;
  text-decoration: underline;
}
.filters button:hover,
.collapse button:hover {
  color: #ccc;
}

.filter__label {
  margin-right: 10px;
}

      </style>
</head>
<body>
<h1 id="title">index.html</h1>
<p>Report generated on 08-Jan-2025 at 12:13:47 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
<div id="environment-header">
<h2>Environment</h2>
</div>
<table id="environment"></table>
<!-- TEMPLATES -->
<template id="template_environment_row">
<tr>
<td></td>
<td></td>
</tr>
</template>
<template id="template_results-table__body--empty">
<tbody class="results-table-row">
<tr id="not-found-message">
<td colspan="4">No results found. Check the filters.
</td></tr>
</tbody></template>
<template id="template_results-table__tbody">
<tbody class="results-table-row">
<tr class="collapsible">
</tr>
<tr class="extras-row">
<td class="extra" colspan="4">
<div class="extraHTML"></div>
<div class="media">
<div class="media-container">
<div class="media-container__nav--left">&lt;</div>
<div class="media-container__viewport">
<img src=""/>
<video controls="">
<source src="" type="video/mp4"/>
</video>
</div>
<div class="media-container__nav--right">&gt;</div>
</div>
<div class="media__name"></div>
<div class="media__counter"></div>
</div>
<div class="logwrapper">
<div class="logexpander"></div>
<div class="log"></div>
</div>
</td>
</tr>
</tbody>
</template>
<!-- END TEMPLATES -->
<div class="summary">
<div class="summary__data">
<h2>Summary</h2>
<div class="additional-summary prefix">
</div>
<p class="run-count">18 tests ran in 3428 seconds</p>
<p class="filter">(Un)check the boxes to filter the results.</p>
<div class="summary__reload">
<div class="summary__reload__button hidden" onclick="location.reload()">
<div>There are still tests running. <br/>Reload this page to get the latest results!</div>
</div>
</div>
<div class="summary__spacer"></div>
<div class="controls">
<div class="filters">
<input checked="true" class="filter" data-test-result="failed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="failed">0 Failed,</span>
<input checked="true" class="filter" data-test-result="passed" name="filter_checkbox" type="checkbox"/>
<span class="passed">18 Passed,</span>
<input checked="true" class="filter" data-test-result="skipped" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="skipped">0 Skipped,</span>
<input checked="true" class="filter" data-test-result="xfailed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xfailed">0 Expected failures,</span>
<input checked="true" class="filter" data-test-result="xpassed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xpassed">0 Unexpected passes,</span>
<input checked="true" class="filter" data-test-result="error" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="error">0 Errors,</span>
<input checked="true" class="filter" data-test-result="rerun" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="rerun">0 Reruns</span>
</div>
<div class="collapse">
<button id="show_all_details">Show all details</button> / <button id="hide_all_details">Hide all details</button>
</div>
</div>
</div>
<div class="additional-summary summary">
</div>
<div class="additional-summary postfix">
</div>
</div>
<table id="results-table">
<thead id="results-table-head">
<tr>
<th class="sortable" data-column-type="result">Result</th>
<th class="sortable" data-column-type="testId">Test</th>
<th class="sortable" data-column-type="duration">Duration</th>
<th>Links</th>
</tr>
</thead>
</table>
</body>
<footer>
<div data-jsonblob='{"environment": {"Python": "3.11.10", "Platform": "Linux-5.15.0-130-generic-x86_64-with-glibc2.35", "Packages": {"pytest": "8.0.0", "pluggy": "1.5.0"}, "Plugins": {"timeout": "2.3.1", "html": "4.1.1", "xdist": "3.5.0", "metadata": "3.1.1", "anyio": "4.8.0"}, "CI": "true"}, "tests": {"reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-1-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-1-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:07:46", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-1-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:07:46&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stdout setup -----------------------------\nDownloading dataset llama3_8B_fp16\n  gguf: [PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/meta-llama-3.1-8b-instruct.f16.gguf&amp;#x27;)]\n  tokenizer_config.json: [PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/tokenizer_config.json&amp;#x27;), PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/tokenizer.json&amp;#x27;)]\nExporting prefill_bs1\nExporting decode_bs1\nExporting prefill_bs4\nExporting decode_bs4\nGENERATED!\nExporting\nSaving to &amp;#x27;/tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/model.mlir&amp;#x27;\n\n---------------------------- Captured stderr setup -----------------------------\n/home/nod/actions-runner-shark-ai/_work/shark-ai/shark-ai/sharktank/sharktank/types/gguf_interop/base.py:100: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n  data_tensor = torch.as_tensor(data.reshape(logical_shape))\n\n------------------------------ Captured log setup ------------------------------\nINFO     sglang_benchmarks.conftest:conftest.py:34 Preparing model artifacts...\n::group::Preparing model artifacts\nINFO     integration_tests.llm.utils:utils.py:50 Download model llama3_8B_fp16 with `hf_datasets` to /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0...\nINFO     integration_tests.llm.utils:utils.py:62 Model llama3_8B_fp16 successfully downloaded.\nINFO     integration_tests.llm.utils:utils.py:82 Exporting model with following settings:\n  MLIR Path: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/model.mlir\n  Config Path: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/config.json\n  Batch Sizes: 1,4\nINFO     integration_tests.llm.utils:utils.py:101 Model successfully exported to /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/model.mlir\nINFO     integration_tests.llm.utils:utils.py:105 Compiling model to /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/model.vmfb\nINFO     integration_tests.llm.utils:utils.py:116 Model successfully compiled to /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/model.vmfb\nINFO     sglang_benchmarks.conftest:conftest.py:61 Model artifacts setup successfully\n::endgroup::\nINFO     sglang_benchmarks.conftest:conftest.py:96 Saving edited config to: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/1_4_none.json\n\nINFO     sglang_benchmarks.conftest:conftest.py:97 Config: {\n  &amp;quot;module_name&amp;quot;: &amp;quot;module&amp;quot;,\n  &amp;quot;module_abi_version&amp;quot;: 1,\n  &amp;quot;max_seq_len&amp;quot;: 131072,\n  &amp;quot;attn_head_dim&amp;quot;: 128,\n  &amp;quot;prefill_batch_sizes&amp;quot;: [\n    1,\n    4\n  ],\n  &amp;quot;decode_batch_sizes&amp;quot;: [\n    1,\n    4\n  ],\n  &amp;quot;transformer_block_count&amp;quot;: 32,\n  &amp;quot;paged_kv_cache&amp;quot;: {\n    &amp;quot;attention_head_count_kv&amp;quot;: 8,\n    &amp;quot;block_seq_stride&amp;quot;: 16,\n    &amp;quot;device_block_count&amp;quot;: 256,\n    &amp;quot;prefix_sharing_algorithm&amp;quot;: &amp;quot;none&amp;quot;\n  }\n}\n\n----------------------------- Captured stderr call -----------------------------\n[2025-01-08 11:21:08] Started server process [1928851]\n[2025-01-08 11:21:08] Waiting for application startup.\n[2025-01-08 11:21:13] Application startup complete.\n[2025-01-08 11:21:13] Uvicorn running on http://0.0.0.0:44483 (Press CTRL+C to quit)\n[2025-01-08 11:21:14] 127.0.0.1:50262 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 11:21:17] 127.0.0.1:48868 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 11:23:22] 127.0.0.1:36008 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:23:22] 127.0.0.1:36014 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:23:23] 127.0.0.1:36016 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:23:23] 127.0.0.1:36024 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:23:24] 127.0.0.1:36026 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:23:24] 127.0.0.1:36034 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:23:24] 127.0.0.1:36040 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:23:24] 127.0.0.1:36056 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:23:25] 127.0.0.1:36062 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:23:25] 127.0.0.1:36068 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:34,  3.80s/it]\r 20%|\u2588\u2588        | 2/10 [00:05&amp;lt;00:21,  2.65s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:25&amp;lt;01:13, 10.52s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:26&amp;lt;00:40,  6.83s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:33&amp;lt;00:34,  6.92s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:35&amp;lt;00:20,  5.22s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:39&amp;lt;00:14,  4.68s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:40&amp;lt;00:45, 22.80s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:41&amp;lt;00:15, 15.98s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [03:05&amp;lt;00:00, 36.86s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [03:05&amp;lt;00:00, 18.55s/it]\n[2025-01-08 11:26:27] Shutting down\n[2025-01-08 11:26:27] Waiting for application shutdown.\n[2025-01-08 11:26:27] Application shutdown complete.\n[2025-01-08 11:26:27] Finished server process [1928851]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 44483\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 44483...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:44483...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0037617683410645 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.006080150604248 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0084455013275146 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.0109031200408936 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.0136942863464355 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.016026973724365 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.01839017868042 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:44483\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 313.2205276489258 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 1003\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 53820.54696709383\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 34475.66481749527\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 148.47599147469737\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.005408015567809343\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 14.95651106895769\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 185.47106254997198\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-2-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-2-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:03:50", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-2-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:03:50&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-08 11:26:31] Started server process [1929470]\n[2025-01-08 11:26:31] Waiting for application startup.\n[2025-01-08 11:26:36] Application startup complete.\n[2025-01-08 11:26:36] Uvicorn running on http://0.0.0.0:45727 (Press CTRL+C to quit)\n[2025-01-08 11:26:37] 127.0.0.1:39494 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 11:26:40] 127.0.0.1:41786 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 11:28:44] 127.0.0.1:49268 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:28:44] 127.0.0.1:49276 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:28:45] 127.0.0.1:49288 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:28:45] 127.0.0.1:49304 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:28:45] 127.0.0.1:49318 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:28:45] 127.0.0.1:49332 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:28:45] 127.0.0.1:49338 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:28:46] 127.0.0.1:49346 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:28:46] 127.0.0.1:49348 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:28:46] 127.0.0.1:49356 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:30,  3.37s/it]\r 20%|\u2588\u2588        | 2/10 [00:05&amp;lt;00:20,  2.56s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:26&amp;lt;01:16, 10.89s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:35&amp;lt;01:01, 10.24s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:41&amp;lt;00:43,  8.69s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:41&amp;lt;00:23,  5.80s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:42&amp;lt;00:12,  4.16s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:44&amp;lt;00:07,  3.62s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:32&amp;lt;00:17, 17.36s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:33&amp;lt;00:00, 12.30s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:33&amp;lt;00:00,  9.33s/it]\n[2025-01-08 11:30:18] Shutting down\n[2025-01-08 11:30:18] Waiting for application shutdown.\n[2025-01-08 11:30:18] Application shutdown complete.\n[2025-01-08 11:30:18] Finished server process [1929470]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 45727\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 45727...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:45727...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0037384033203125 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0053539276123047 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.007535934448242 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.009439945220947 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.011650323867798 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.01398491859436 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.016738653182983 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:45727\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 221.02719020843506 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 1206\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 41569.09577859333\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 40604.21173198847\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 328.7373915081844\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.00550801632925868\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 29.725004342427777\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 93.32210579497041\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-4-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-4-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:23", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-4-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:23&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-08 11:30:22] Started server process [1930096]\n[2025-01-08 11:30:22] Waiting for application startup.\n[2025-01-08 11:30:26] Application startup complete.\n[2025-01-08 11:30:26] Uvicorn running on http://0.0.0.0:40043 (Press CTRL+C to quit)\n[2025-01-08 11:30:27] 127.0.0.1:55478 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 11:30:30] 127.0.0.1:53432 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 11:32:35] 127.0.0.1:49470 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:32:35] 127.0.0.1:49482 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:32:35] 127.0.0.1:49494 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:32:35] 127.0.0.1:49502 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:32:36] 127.0.0.1:49514 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:32:36] 127.0.0.1:49520 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:32:36] 127.0.0.1:49536 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:32:36] 127.0.0.1:49542 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:32:36] 127.0.0.1:49554 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:32:36] 127.0.0.1:49556 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:27,  3.04s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.03s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:15&amp;lt;00:44,  6.41s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:31&amp;lt;00:59,  9.98s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:34&amp;lt;00:38,  7.60s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:35&amp;lt;00:21,  5.41s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:39&amp;lt;00:14,  4.85s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:52&amp;lt;00:14,  7.29s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:28&amp;lt;00:16, 16.29s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:05&amp;lt;00:00, 22.70s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:05&amp;lt;00:00, 12.53s/it]\n[2025-01-08 11:34:40] Shutting down\n[2025-01-08 11:34:40] Waiting for application shutdown.\n[2025-01-08 11:34:40] Application shutdown complete.\n[2025-01-08 11:34:40] Finished server process [1930096]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 40043\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 40043...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:40043...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0029494762420654 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0045642852783203 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.006801128387451 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.009943723678589 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.012106657028198 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.014359474182129 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.016603946685791 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:40043\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 253.41574358940125 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 805\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 42586.91571329837\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 35053.91911501647\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 637.896001979243\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.0036850105971097946\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 22.138398735146744\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 125.30264872300904\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-8-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-8-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:08", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-8-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:08&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-08 11:34:44] Started server process [1930712]\n[2025-01-08 11:34:44] Waiting for application startup.\n[2025-01-08 11:34:48] Application startup complete.\n[2025-01-08 11:34:48] Uvicorn running on http://0.0.0.0:47945 (Press CTRL+C to quit)\n[2025-01-08 11:34:49] 127.0.0.1:34690 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 11:34:53] 127.0.0.1:34698 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 11:36:57] 127.0.0.1:37920 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:36:57] 127.0.0.1:37930 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:36:57] 127.0.0.1:35758 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:36:57] 127.0.0.1:35770 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:36:58] 127.0.0.1:35780 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:36:58] 127.0.0.1:35796 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:36:58] 127.0.0.1:35798 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:36:58] 127.0.0.1:35812 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:36:58] 127.0.0.1:35814 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:36:58] 127.0.0.1:35826 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:26,  2.90s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:18,  2.37s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:15&amp;lt;00:44,  6.31s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:31&amp;lt;00:59,  9.92s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:37&amp;lt;00:42,  8.51s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:37&amp;lt;00:22,  5.72s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:12,  4.12s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:16&amp;lt;00:30, 15.05s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:22&amp;lt;00:12, 12.21s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:50&amp;lt;00:00, 17.08s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:50&amp;lt;00:00, 11.09s/it]\n[2025-01-08 11:38:48] Shutting down\n[2025-01-08 11:38:48] Waiting for application shutdown.\n[2025-01-08 11:38:48] Application shutdown complete.\n[2025-01-08 11:38:48] Finished server process [1930712]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 47945\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 47945...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:47945...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0027134418487549 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0048978328704834 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0070786476135254 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.0092761516571045 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.011893033981323 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.014073133468628 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.016199827194214 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47945\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 238.6458990573883 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 665\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 43673.893469100585\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 37350.720571528655\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 763.1710775021929\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.004396017175167799\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 25.0117156648008\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 110.90802554995753\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-16-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-16-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:08", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-16-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:08&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-08 11:38:52] Started server process [1931328]\n[2025-01-08 11:38:52] Waiting for application startup.\n[2025-01-08 11:38:56] Application startup complete.\n[2025-01-08 11:38:56] Uvicorn running on http://0.0.0.0:59955 (Press CTRL+C to quit)\n[2025-01-08 11:38:57] 127.0.0.1:38448 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 11:39:01] 127.0.0.1:38462 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 11:41:05] 127.0.0.1:51096 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:41:05] 127.0.0.1:51108 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:41:05] 127.0.0.1:51114 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:41:05] 127.0.0.1:51126 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:41:05] 127.0.0.1:51130 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:41:05] 127.0.0.1:51138 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:41:05] 127.0.0.1:51142 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:41:05] 127.0.0.1:51146 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:41:05] 127.0.0.1:51152 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:41:05] 127.0.0.1:51164 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:23,  2.64s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:17,  2.13s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:15&amp;lt;00:44,  6.31s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:20&amp;lt;00:34,  5.76s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:33&amp;lt;00:41,  8.26s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:39&amp;lt;00:29,  7.48s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:40&amp;lt;00:16,  5.39s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:55&amp;lt;00:16,  8.40s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:15&amp;lt;00:12, 12.03s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:51&amp;lt;00:00, 19.58s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:51&amp;lt;00:00, 11.17s/it]\n[2025-01-08 11:42:57] Shutting down\n[2025-01-08 11:42:57] Waiting for application shutdown.\n[2025-01-08 11:42:57] Application shutdown complete.\n[2025-01-08 11:42:57] Finished server process [1931328]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 59955\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 59955...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:59955...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0028588771820068 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0049843788146973 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0070927143096924 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.0092551708221436 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.011362552642822 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.013523578643799 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.0156848430633545 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:59955\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 239.47326159477234 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 574\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 39709.90258870297\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 36191.348391992506\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 839.9930144951213\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.0037060235626995564\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 24.832083883783667\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 111.71031851303997\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-32-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-32-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:05:11", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-32-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:05:11&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-08 11:43:00] Started server process [1931940]\n[2025-01-08 11:43:00] Waiting for application startup.\n[2025-01-08 11:43:05] Application startup complete.\n[2025-01-08 11:43:05] Uvicorn running on http://0.0.0.0:46141 (Press CTRL+C to quit)\n[2025-01-08 11:43:06] 127.0.0.1:55016 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 11:43:09] 127.0.0.1:52596 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 11:45:13] 127.0.0.1:48644 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:45:13] 127.0.0.1:48656 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:45:13] 127.0.0.1:48664 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:45:13] 127.0.0.1:48678 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:45:14] 127.0.0.1:48690 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:45:14] 127.0.0.1:48694 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:45:14] 127.0.0.1:48702 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:45:14] 127.0.0.1:48708 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:45:14] 127.0.0.1:48718 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:45:14] 127.0.0.1:48722 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:23,  2.57s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:17,  2.23s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:16&amp;lt;00:45,  6.46s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:26&amp;lt;00:48,  8.07s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:31&amp;lt;00:34,  6.89s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:35&amp;lt;00:24,  6.05s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:37&amp;lt;00:14,  4.69s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:48&amp;lt;00:13,  6.55s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:58&amp;lt;00:26, 26.57s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:53&amp;lt;00:00, 35.37s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:53&amp;lt;00:00, 17.39s/it]\n[2025-01-08 11:48:07] Shutting down\n[2025-01-08 11:48:07] Waiting for application shutdown.\n[2025-01-08 11:48:07] Application shutdown complete.\n[2025-01-08 11:48:07] Finished server process [1931940]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 46141\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 46141...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:46141...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0029127597808838 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0050461292266846 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0072314739227295 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.010164260864258 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.012413740158081 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.014650106430054 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.016912460327148 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:46141\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 301.6770031452179 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 319\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 49495.03338220529\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 33554.03945848229\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 878.2933369802777\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.004165980499237776\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 15.95064100943931\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 173.91150602401467\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-1-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-1-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:24", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-1-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:24&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "------------------------------ Captured log setup ------------------------------\nINFO     sglang_benchmarks.conftest:conftest.py:34 Preparing model artifacts...\n::group::Preparing model artifacts\nINFO     sglang_benchmarks.conftest:conftest.py:40 Reusing existing model artifacts directory: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\n::endgroup::\nINFO     sglang_benchmarks.conftest:conftest.py:96 Saving edited config to: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0/1_4_trie.json\n\nINFO     sglang_benchmarks.conftest:conftest.py:97 Config: {\n  &amp;quot;module_name&amp;quot;: &amp;quot;module&amp;quot;,\n  &amp;quot;module_abi_version&amp;quot;: 1,\n  &amp;quot;max_seq_len&amp;quot;: 131072,\n  &amp;quot;attn_head_dim&amp;quot;: 128,\n  &amp;quot;prefill_batch_sizes&amp;quot;: [\n    1,\n    4\n  ],\n  &amp;quot;decode_batch_sizes&amp;quot;: [\n    1,\n    4\n  ],\n  &amp;quot;transformer_block_count&amp;quot;: 32,\n  &amp;quot;paged_kv_cache&amp;quot;: {\n    &amp;quot;attention_head_count_kv&amp;quot;: 8,\n    &amp;quot;block_seq_stride&amp;quot;: 16,\n    &amp;quot;device_block_count&amp;quot;: 256,\n    &amp;quot;prefix_sharing_algorithm&amp;quot;: &amp;quot;trie&amp;quot;\n  }\n}\n\n----------------------------- Captured stderr call -----------------------------\n[2025-01-08 11:48:11] Started server process [1932660]\n[2025-01-08 11:48:11] Waiting for application startup.\n[2025-01-08 11:48:16] Application startup complete.\n[2025-01-08 11:48:16] Uvicorn running on http://0.0.0.0:53065 (Press CTRL+C to quit)\n[2025-01-08 11:48:16] 127.0.0.1:44532 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 11:48:20] 127.0.0.1:40348 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 11:50:24] 127.0.0.1:41958 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:50:25] 127.0.0.1:41968 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:50:26] 127.0.0.1:41982 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:50:26] 127.0.0.1:41998 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:50:26] 127.0.0.1:42004 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:50:26] 127.0.0.1:42008 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:50:26] 127.0.0.1:42016 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:50:27] 127.0.0.1:42030 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:50:27] 127.0.0.1:33106 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:50:28] 127.0.0.1:33114 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:35,  3.96s/it]\r 20%|\u2588\u2588        | 2/10 [00:06&amp;lt;00:24,  3.12s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:26&amp;lt;01:17, 11.04s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:29&amp;lt;00:47,  7.85s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:37&amp;lt;00:37,  7.59s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:39&amp;lt;00:23,  5.90s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:41&amp;lt;00:13,  4.61s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:50&amp;lt;00:11,  6.00s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:21&amp;lt;00:13, 13.83s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:06&amp;lt;00:00, 23.47s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:06&amp;lt;00:00, 12.67s/it]\n[2025-01-08 11:52:31] Shutting down\n[2025-01-08 11:52:31] Waiting for application shutdown.\n[2025-01-08 11:52:31] Application shutdown complete.\n[2025-01-08 11:52:31] Finished server process [1932660]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 53065\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 53065...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:53065...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.00357985496521 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.005716323852539 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.007888078689575 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.010165214538574 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.012422800064087 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.01475191116333 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.017505884170532 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:53065\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 254.5073046684265 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2437\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 42418.90779820387\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 35649.4637785072\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 162.85707103088498\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.009182957001030445\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 21.89247965734149\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 126.71017826296156\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-2-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-2-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:03:44", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-2-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:03:44&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-08 11:52:35] Started server process [1933308]\n[2025-01-08 11:52:35] Waiting for application startup.\n[2025-01-08 11:52:39] Application startup complete.\n[2025-01-08 11:52:39] Uvicorn running on http://0.0.0.0:57807 (Press CTRL+C to quit)\n[2025-01-08 11:52:40] 127.0.0.1:58690 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 11:52:43] 127.0.0.1:58702 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 11:54:48] 127.0.0.1:54446 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:54:48] 127.0.0.1:54448 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:54:48] 127.0.0.1:54454 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:54:48] 127.0.0.1:54460 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:54:49] 127.0.0.1:54468 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:54:49] 127.0.0.1:54476 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:54:49] 127.0.0.1:54480 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:54:49] 127.0.0.1:54492 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:54:49] 127.0.0.1:54496 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:54:49] 127.0.0.1:54504 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:24,  2.74s/it]\r 20%|\u2588\u2588        | 2/10 [00:05&amp;lt;00:20,  2.50s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:23&amp;lt;01:06,  9.56s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:25&amp;lt;00:39,  6.65s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:25&amp;lt;00:22,  4.51s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:28&amp;lt;00:15,  3.93s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:42&amp;lt;00:21,  7.23s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:47&amp;lt;00:12,  6.36s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:07&amp;lt;00:10, 10.79s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:26&amp;lt;00:00, 13.33s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:26&amp;lt;00:00,  8.68s/it]\n[2025-01-08 11:56:15] Shutting down\n[2025-01-08 11:56:15] Waiting for application shutdown.\n[2025-01-08 11:56:15] Application shutdown complete.\n[2025-01-08 11:56:15] Finished server process [1933308]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 57807\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 57807...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:57807...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0029239654541016 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0051000118255615 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0085809230804443 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.0106871128082275 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.012784719467163 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.014906883239746 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.017271041870117 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:57807\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 214.54838180541992 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2631\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 34515.77129489742\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 27205.679770500865\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 348.93328649923205\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.010695483069866896\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 31.955053148209586\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 86.80943158298032\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-4-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-4-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:03:49", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-4-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:03:49&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-08 11:56:18] Started server process [1933920]\n[2025-01-08 11:56:18] Waiting for application startup.\n[2025-01-08 11:56:23] Application startup complete.\n[2025-01-08 11:56:23] Uvicorn running on http://0.0.0.0:52589 (Press CTRL+C to quit)\n[2025-01-08 11:56:24] 127.0.0.1:42264 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 11:56:27] 127.0.0.1:42268 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 11:58:31] 127.0.0.1:53492 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:58:31] 127.0.0.1:53508 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:58:31] 127.0.0.1:53512 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:58:31] 127.0.0.1:53518 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:58:32] 127.0.0.1:53528 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:58:32] 127.0.0.1:53538 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:58:32] 127.0.0.1:53544 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:58:32] 127.0.0.1:53546 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:58:32] 127.0.0.1:53550 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 11:58:32] 127.0.0.1:53562 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:22,  2.47s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:17,  2.25s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:23&amp;lt;01:08,  9.85s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:26&amp;lt;00:42,  7.00s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:27&amp;lt;00:25,  5.03s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:41&amp;lt;00:32,  8.18s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:42&amp;lt;00:17,  5.69s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:50&amp;lt;00:13,  6.53s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:20&amp;lt;00:13, 13.73s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:32&amp;lt;00:00, 13.19s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:32&amp;lt;00:00,  9.24s/it]\n[2025-01-08 12:00:04] Shutting down\n[2025-01-08 12:00:04] Waiting for application shutdown.\n[2025-01-08 12:00:04] Application shutdown complete.\n[2025-01-08 12:00:04] Finished server process [1933920]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 52589\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 52589...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:52589...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0029120445251465 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0044972896575928 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0066401958465576 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.008836984634399 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.011582374572754 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.013722658157349 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.0158374309539795 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:52589\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 220.029278755188 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2055\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 38704.98225450865\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 34401.98871699977\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 638.5079910396598\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.003986002411693335\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 30.035382599091154\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 92.35773810598766\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-8-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-8-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:05:29", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-8-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:05:29&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-08 12:00:07] Started server process [1934528]\n[2025-01-08 12:00:07] Waiting for application startup.\n[2025-01-08 12:00:12] Application startup complete.\n[2025-01-08 12:00:12] Uvicorn running on http://0.0.0.0:32975 (Press CTRL+C to quit)\n[2025-01-08 12:00:13] 127.0.0.1:41660 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 12:00:16] 127.0.0.1:41666 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 12:02:20] 127.0.0.1:37952 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:02:20] 127.0.0.1:37968 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:02:20] 127.0.0.1:37970 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:02:20] 127.0.0.1:37984 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:02:21] 127.0.0.1:37996 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:02:21] 127.0.0.1:38010 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:02:21] 127.0.0.1:38022 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:02:21] 127.0.0.1:38034 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:02:21] 127.0.0.1:38038 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:02:21] 127.0.0.1:38050 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:25,  2.86s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.08s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:28&amp;lt;01:26, 12.32s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:31&amp;lt;00:50,  8.35s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:38&amp;lt;00:40,  8.04s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:40&amp;lt;00:23,  5.78s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:58&amp;lt;00:14,  7.39s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:08&amp;lt;00:08,  8.08s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [03:11&amp;lt;00:00, 39.39s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [03:11&amp;lt;00:00, 19.18s/it]\n[2025-01-08 12:05:32] Shutting down\n[2025-01-08 12:05:32] Waiting for application shutdown.\n[2025-01-08 12:05:32] Application shutdown complete.\n[2025-01-08 12:05:32] Finished server process [1934528]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 32975\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 32975...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:32975...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0029420852661133 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0057623386383057 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.007908582687378 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.009999513626099 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.012109279632568 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.014224052429199 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.016392707824707 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:32975\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 319.5426344871521 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 1808\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 50160.159072390525\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 39079.59860746632\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 798.2528809807263\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.007385533535853028\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 14.465278600279532\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 191.76955222600373\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-16-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-16-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-16-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-08 12:05:36] Started server process [1935147]\n[2025-01-08 12:05:36] Waiting for application startup.\n[2025-01-08 12:05:40] Application startup complete.\n[2025-01-08 12:05:40] Uvicorn running on http://0.0.0.0:38347 (Press CTRL+C to quit)\n[2025-01-08 12:05:41] 127.0.0.1:54126 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 12:05:45] 127.0.0.1:54136 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 12:07:49] 127.0.0.1:51420 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:07:49] 127.0.0.1:51426 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:07:49] 127.0.0.1:51440 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:07:49] 127.0.0.1:51444 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:07:49] 127.0.0.1:51458 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:07:49] 127.0.0.1:51460 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:07:49] 127.0.0.1:51462 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:07:49] 127.0.0.1:51466 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:07:49] 127.0.0.1:51480 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:07:49] 127.0.0.1:51492 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:20,  2.28s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.10s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:22&amp;lt;01:06,  9.46s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:24&amp;lt;00:40,  6.67s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:29&amp;lt;00:29,  5.85s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:34&amp;lt;00:22,  5.50s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:15,  5.04s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:04&amp;lt;00:23, 11.89s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:46&amp;lt;00:21, 21.32s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:56&amp;lt;00:00, 17.82s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:56&amp;lt;00:00, 11.68s/it]\n[2025-01-08 12:09:46] Shutting down\n[2025-01-08 12:09:46] Waiting for application shutdown.\n[2025-01-08 12:09:46] Application shutdown complete.\n[2025-01-08 12:09:46] Finished server process [1935147]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 38347\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 38347...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:38347...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0032501220703125 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0053045749664307 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0074899196624756 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.010725498199463 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.013118028640747 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.0154829025268555 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.0170207023620605 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:38347\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 244.45619535446167 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2475\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 44237.77664829977\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 31596.46638450795\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 829.809575487161\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.014281016774475574\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 23.757986818231768\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 116.76073487300891\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-32-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-32-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:00", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-32-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:00&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-08 12:09:49] Started server process [1935766]\n[2025-01-08 12:09:49] Waiting for application startup.\n[2025-01-08 12:09:54] Application startup complete.\n[2025-01-08 12:09:54] Uvicorn running on http://0.0.0.0:52041 (Press CTRL+C to quit)\n[2025-01-08 12:09:55] 127.0.0.1:37374 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-08 12:09:58] 127.0.0.1:38936 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-08 12:12:02] 127.0.0.1:47700 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:12:02] 127.0.0.1:47702 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:12:02] 127.0.0.1:47710 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:12:02] 127.0.0.1:47714 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:12:03] 127.0.0.1:47718 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:12:03] 127.0.0.1:47734 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:12:03] 127.0.0.1:47748 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:12:03] 127.0.0.1:47750 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:12:03] 127.0.0.1:47760 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-08 12:12:03] 127.0.0.1:47772 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:21,  2.39s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.01s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:26&amp;lt;01:20, 11.48s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:28&amp;lt;00:44,  7.41s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:29&amp;lt;00:25,  5.16s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:39&amp;lt;00:27,  6.78s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:41&amp;lt;00:16,  5.37s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:59&amp;lt;00:18,  9.50s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:28&amp;lt;00:15, 15.46s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:43&amp;lt;00:00, 15.32s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:43&amp;lt;00:00, 10.35s/it]\n[2025-01-08 12:13:46] Shutting down\n[2025-01-08 12:13:46] Waiting for application shutdown.\n[2025-01-08 12:13:46] Application shutdown complete.\n[2025-01-08 12:13:46] Finished server process [1935766]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 52041\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 52041...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:52041...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.003675937652588 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.005387306213379 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.007546901702881 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.009696006774902 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.011945009231567 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.014239311218262 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.017029047012329 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:52041\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-37/sglang_benchmark_test0\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 231.27530908584595 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 1495\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 42266.717039304785\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 34127.19368498074\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 838.3171675086487\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.00561797060072422\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 26.80089771318736\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 103.50399563799147\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:53", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:53&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:18,  2.09s/it]\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:07,  1.03it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:04,  1.51it/s]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.45it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:04&amp;lt;00:04,  1.15it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:05,  1.41s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:07&amp;lt;00:03,  1.06s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:07&amp;lt;00:01,  1.28it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:08&amp;lt;00:00,  1.19it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.62it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.17it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test0/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test0/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0023362636566162 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0040106773376465 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0059845447540283 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.0083746910095215 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.011556386947632 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.014429569244385 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.016237020492554 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 8.017908811569214 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 9.019571781158447 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 10.021363496780396 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 11.02454924583435 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 12.027303218841553 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 13.029052972793579 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 14.03149127960205 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 15.034172296524048 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 16.03603172302246 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 17.03846764564514 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 18.041083335876465 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 19.043755292892456 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 20.046499013900757 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 21.049118041992188 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 22.051833391189575 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 23.05493950843811 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 24.057555198669434 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 25.060238361358643 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 26.0629563331604 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 27.06564497947693 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 28.06820034980774 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 29.071372032165527 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 30.07408571243286 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 31.07673215866089 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 32.07946467399597 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 33.082204818725586 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 34.084932804107666 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test0\nRequest Rate: 1\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test0&amp;#x27;, request_rate=1, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test0/sglang_10_1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    1         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  8.57      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.17      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          228.82    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         323.85    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          552.67    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3303.60   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3584.70   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          28.21     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        25.02     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           47.05     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          11.66     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.11     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           13.37     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           11.85     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.15     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            18.10     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 17.19122624397278 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3303.5961230983958\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 3584.6959894697648\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 25.015140534378588\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.151920458767563\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 323.84962956347863\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 8.565703792031854\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:15", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:15&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:10,  1.22s/it]\r 20%|\u2588\u2588        | 2/10 [00:01&amp;lt;00:04,  1.72it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:06,  1.01it/s]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.17it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.55it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:04,  1.18s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:06&amp;lt;00:02,  1.21it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.56it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.62it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.33it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test1/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test1/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test1\nRequest Rate: 2\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test1&amp;#x27;, request_rate=2, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test1/sglang_10_2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    2         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  7.50      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.33      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          261.47    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         370.06    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          631.53    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3504.93   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3907.71   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          21.70     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        19.65     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           30.36     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.67     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.69     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           14.46     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.60     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.25     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            22.92     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 14.430417776107788 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3504.930759803392\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 3907.714600500185\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 19.6470909868367\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.254978519398719\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 370.0623045446569\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 7.496035035001114\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:06,  1.33it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:07,  1.06s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.29it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:02,  1.70it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:04,  1.00s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.76it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.65it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.45it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test2/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test2/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test2\nRequest Rate: 4\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test2&amp;#x27;, request_rate=4, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test2/sglang_10_4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    4         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.90      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.45      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          284.07    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         402.05    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          686.12    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3569.08   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4022.12   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          24.95     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        23.46     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           43.40     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.15     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.29     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           14.50     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.82     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.24     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            22.72     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 13.890064716339111 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3569.0784968959633\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 4022.1193010220304\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 23.45834398875013\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.240968022728339\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 402.0500418594225\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 6.899638629984111\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  2.17it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.48it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:09,  1.29s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.21it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.53it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:03,  1.07it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.93it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.69it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.51it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test3/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test3/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test3\nRequest Rate: 8\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test3&amp;#x27;, request_rate=8, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test3/sglang_10_8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    8         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.60      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.51      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          296.77    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         420.02    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          716.79    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3602.88   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4081.01   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          19.87     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        18.48     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           26.83     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.50     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.32     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           15.61     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.96     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.29     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            22.67     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 13.386727094650269 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3602.8764332062565\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 4081.0141540132463\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 18.483121501049027\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.2934249995742\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 420.02138744303926\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 6.604425591009203\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.18it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.98it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:08,  1.27s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.13it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.49it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.14it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.13it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.03it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.65it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.55it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test4/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test4/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test4\nRequest Rate: 16\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test4&amp;#x27;, request_rate=16, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test4/sglang_10_16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    16        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.44      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.55      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          304.56    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         431.04    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          735.60    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3612.49   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4107.42   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          20.37     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        19.27     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           26.65     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.71     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.38     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           16.69     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           13.00     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.29     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            16.72     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 13.228264570236206 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3612.4930440972093\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 4107.417264021933\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 19.2657109873835\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.291857991134748\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 431.044054611711\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 6.435537087963894\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  4.38it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:01,  4.47it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:08,  1.26s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.11it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.49it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.17it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.18it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.08it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.66it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.58it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test5/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test5/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-36/sglang_benchmark_test5\nRequest Rate: 32\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test5&amp;#x27;, request_rate=32, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-36/sglang_benchmark_test5/sglang_10_32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    32        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.34      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.58      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          308.97    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         437.29    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          746.26    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3601.99   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4100.72   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          20.82     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        19.02     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           26.28     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.58     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.36     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           16.49     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.96     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.27     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            16.79     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 13.092944383621216 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3601.992142800009\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 4100.719060486881\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 19.015414494788274\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.267861999338493\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 437.28681248306515\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 6.343662604980636\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}]}, "renderCollapsed": ["passed"], "initialSort": "result", "title": "shortfin_index.html"}' id="data-container"></div>
<script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
</footer>
</html>