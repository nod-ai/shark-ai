<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<title id="head-title">index.html</title>
<style type="text/css">body {
  font-family: Helvetica, Arial, sans-serif;
  font-size: 12px;
  /* do not increase min-width as some may use split screens */
  min-width: 800px;
  color: #999;
}

h1 {
  font-size: 24px;
  color: black;
}

h2 {
  font-size: 16px;
  color: black;
}

p {
  color: black;
}

a {
  color: #999;
}

table {
  border-collapse: collapse;
}

/******************************
 * SUMMARY INFORMATION
 ******************************/
#environment td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  vertical-align: top;
}
#environment tr:nth-child(odd) {
  background-color: #f6f6f6;
}
#environment ul {
  margin: 0;
  padding: 0 20px;
}

/******************************
 * TEST RESULT COLORS
 ******************************/
span.passed,
.passed .col-result {
  color: green;
}

span.skipped,
span.xfailed,
span.rerun,
.skipped .col-result,
.xfailed .col-result,
.rerun .col-result {
  color: orange;
}

span.error,
span.failed,
span.xpassed,
.error .col-result,
.failed .col-result,
.xpassed .col-result {
  color: red;
}

.col-links__extra {
  margin-right: 3px;
}

/******************************
 * RESULTS TABLE
 *
 * 1. Table Layout
 * 2. Extra
 * 3. Sorting items
 *
 ******************************/
/*------------------
 * 1. Table Layout
 *------------------*/
#results-table {
  border: 1px solid #e6e6e6;
  color: #999;
  font-size: 12px;
  width: 100%;
}
#results-table th,
#results-table td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  text-align: left;
}
#results-table th {
  font-weight: bold;
}

/*------------------
 * 2. Extra
 *------------------*/
.logwrapper {
  max-height: 230px;
  overflow-y: scroll;
  background-color: #e6e6e6;
}
.logwrapper.expanded {
  max-height: none;
}
.logwrapper.expanded .logexpander:after {
  content: "collapse [-]";
}
.logwrapper .logexpander {
  z-index: 1;
  position: sticky;
  top: 10px;
  width: max-content;
  border: 1px solid;
  border-radius: 3px;
  padding: 5px 7px;
  margin: 10px 0 10px calc(100% - 80px);
  cursor: pointer;
  background-color: #e6e6e6;
}
.logwrapper .logexpander:after {
  content: "expand [+]";
}
.logwrapper .logexpander:hover {
  color: #000;
  border-color: #000;
}
.logwrapper .log {
  min-height: 40px;
  position: relative;
  top: -50px;
  height: calc(100% + 50px);
  border: 1px solid #e6e6e6;
  color: black;
  display: block;
  font-family: "Courier New", Courier, monospace;
  padding: 5px;
  padding-right: 80px;
  white-space: pre-wrap;
}

div.media {
  border: 1px solid #e6e6e6;
  float: right;
  height: 240px;
  margin: 0 5px;
  overflow: hidden;
  width: 320px;
}

.media-container {
  display: grid;
  grid-template-columns: 25px auto 25px;
  align-items: center;
  flex: 1 1;
  overflow: hidden;
  height: 200px;
}

.media-container--fullscreen {
  grid-template-columns: 0px auto 0px;
}

.media-container__nav--right,
.media-container__nav--left {
  text-align: center;
  cursor: pointer;
}

.media-container__viewport {
  cursor: pointer;
  text-align: center;
  height: inherit;
}
.media-container__viewport img,
.media-container__viewport video {
  object-fit: cover;
  width: 100%;
  max-height: 100%;
}

.media__name,
.media__counter {
  display: flex;
  flex-direction: row;
  justify-content: space-around;
  flex: 0 0 25px;
  align-items: center;
}

.collapsible td:not(.col-links) {
  cursor: pointer;
}
.collapsible td:not(.col-links):hover::after {
  color: #bbb;
  font-style: italic;
  cursor: pointer;
}

.col-result {
  width: 130px;
}
.col-result:hover::after {
  content: " (hide details)";
}

.col-result.collapsed:hover::after {
  content: " (show details)";
}

#environment-header h2:hover::after {
  content: " (hide details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

#environment-header.collapsed h2:hover::after {
  content: " (show details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

/*------------------
 * 3. Sorting items
 *------------------*/
.sortable {
  cursor: pointer;
}
.sortable.desc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: -12.5px;
  border: 10px solid #4caf50;
  border-bottom: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}
.sortable.asc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: 12.5px;
  border: 10px solid #4caf50;
  border-top: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}

.hidden, .summary__reload__button.hidden {
  display: none;
}

.summary__data {
  flex: 0 0 550px;
}
.summary__reload {
  flex: 1 1;
  display: flex;
  justify-content: center;
}
.summary__reload__button {
  flex: 0 0 300px;
  display: flex;
  color: white;
  font-weight: bold;
  background-color: #4caf50;
  text-align: center;
  justify-content: center;
  align-items: center;
  border-radius: 3px;
  cursor: pointer;
}
.summary__reload__button:hover {
  background-color: #46a049;
}
.summary__spacer {
  flex: 0 0 550px;
}

.controls {
  display: flex;
  justify-content: space-between;
}

.filters,
.collapse {
  display: flex;
  align-items: center;
}
.filters button,
.collapse button {
  color: #999;
  border: none;
  background: none;
  cursor: pointer;
  text-decoration: underline;
}
.filters button:hover,
.collapse button:hover {
  color: #ccc;
}

.filter__label {
  margin-right: 10px;
}

      </style>
</head>
<body>
<h1 id="title">index.html</h1>
<p>Report generated on 15-Feb-2025 at 00:49:00 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
<div id="environment-header">
<h2>Environment</h2>
</div>
<table id="environment"></table>
<!-- TEMPLATES -->
<template id="template_environment_row">
<tr>
<td></td>
<td></td>
</tr>
</template>
<template id="template_results-table__body--empty">
<tbody class="results-table-row">
<tr id="not-found-message">
<td colspan="4">No results found. Check the filters.
</td></tr>
</tbody></template>
<template id="template_results-table__tbody">
<tbody class="results-table-row">
<tr class="collapsible">
</tr>
<tr class="extras-row">
<td class="extra" colspan="4">
<div class="extraHTML"></div>
<div class="media">
<div class="media-container">
<div class="media-container__nav--left">&lt;</div>
<div class="media-container__viewport">
<img src=""/>
<video controls="">
<source src="" type="video/mp4"/>
</video>
</div>
<div class="media-container__nav--right">&gt;</div>
</div>
<div class="media__name"></div>
<div class="media__counter"></div>
</div>
<div class="logwrapper">
<div class="logexpander"></div>
<div class="log"></div>
</div>
</td>
</tr>
</tbody>
</template>
<!-- END TEMPLATES -->
<div class="summary">
<div class="summary__data">
<h2>Summary</h2>
<div class="additional-summary prefix">
</div>
<p class="run-count">18 tests ran in 2533 seconds</p>
<p class="filter">(Un)check the boxes to filter the results.</p>
<div class="summary__reload">
<div class="summary__reload__button hidden" onclick="location.reload()">
<div>There are still tests running. <br/>Reload this page to get the latest results!</div>
</div>
</div>
<div class="summary__spacer"></div>
<div class="controls">
<div class="filters">
<input checked="true" class="filter" data-test-result="failed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="failed">0 Failed,</span>
<input checked="true" class="filter" data-test-result="passed" name="filter_checkbox" type="checkbox"/>
<span class="passed">18 Passed,</span>
<input checked="true" class="filter" data-test-result="skipped" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="skipped">0 Skipped,</span>
<input checked="true" class="filter" data-test-result="xfailed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xfailed">0 Expected failures,</span>
<input checked="true" class="filter" data-test-result="xpassed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xpassed">0 Unexpected passes,</span>
<input checked="true" class="filter" data-test-result="error" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="error">0 Errors,</span>
<input checked="true" class="filter" data-test-result="rerun" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="rerun">0 Reruns</span>
</div>
<div class="collapse">
<button id="show_all_details">Show all details</button> / <button id="hide_all_details">Hide all details</button>
</div>
</div>
</div>
<div class="additional-summary summary">
</div>
<div class="additional-summary postfix">
</div>
</div>
<table id="results-table">
<thead id="results-table-head">
<tr>
<th class="sortable" data-column-type="result">Result</th>
<th class="sortable" data-column-type="testId">Test</th>
<th class="sortable" data-column-type="duration">Duration</th>
<th>Links</th>
</tr>
</thead>
</table>
</body>
<footer>
<div data-jsonblob='{"environment": {"Python": "3.11.11", "Platform": "Linux-5.15.0-70-generic-x86_64-with-glibc2.35", "Packages": {"pytest": "8.0.0", "pluggy": "1.5.0"}, "Plugins": {"html": "4.1.1", "xdist": "3.5.0", "timeout": "2.3.1", "anyio": "4.8.0", "metadata": "3.1.1"}, "CI": "true"}, "tests": {"reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]", "duration": "00:11:41", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:11:41&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stdout setup -----------------------------\nExporting prefill_bs1\nExporting decode_bs1\nExporting prefill_bs4\nExporting decode_bs4\nGENERATED!\nExporting\nSaving to &amp;#x27;/tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir&amp;#x27;\n\n---------------------------- Captured stderr setup -----------------------------\n/home/runner/_work/shark-ai/shark-ai/sharktank/sharktank/types/gguf_interop/base.py:100: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n  data_tensor = torch.as_tensor(data.reshape(logical_shape))\n[2025-02-15 00:18:55] Started server process [4253]\n[2025-02-15 00:18:55] Waiting for application startup.\n[2025-02-15 00:18:58] Application startup complete.\n[2025-02-15 00:18:58] Uvicorn running on http://0.0.0.0:60907 (Press CTRL+C to quit)\n[2025-02-15 00:18:59] ::1:47320 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\n\n------------------------------ Captured log setup ------------------------------\nINFO     integration_tests.llm.model_management:model_management.py:134 Downloading model SanctumAI/Meta-Llama-3.1-8B-Instruct-GGUF from HuggingFace\nINFO     integration_tests.llm.model_management:model_management.py:201 Downloading tokenizer NousResearch/Meta-Llama-3.1-8B using transformers\nINFO     integration_tests.llm.model_management:model_management.py:217 Exporting model with following settings:\n  MLIR Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\n  Config Path: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json\n  Batch Sizes: 1,4\nINFO     integration_tests.llm.model_management:model_management.py:238 Model successfully exported to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\nINFO     integration_tests.llm.model_management:model_management.py:244 Compiling model to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\nINFO     integration_tests.llm.model_management:model_management.py:255 Model successfully compiled to /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\n\n----------------------------- Captured stderr call -----------------------------\n\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 0.00/642M [00:00&amp;lt;?, ?B/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   2%|\u258f         | 15.8M/642M [00:00&amp;lt;00:03, 166MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   5%|\u258c         | 33.3M/642M [00:00&amp;lt;00:03, 176MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   8%|\u258a         | 51.1M/642M [00:00&amp;lt;00:03, 181MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  11%|\u2588         | 68.8M/642M [00:00&amp;lt;00:03, 183MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  13%|\u2588\u258e        | 86.6M/642M [00:00&amp;lt;00:03, 184MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|\u2588\u258b        | 104M/642M [00:00&amp;lt;00:03, 185MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u2589        | 122M/642M [00:00&amp;lt;00:02, 185MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  22%|\u2588\u2588\u258f       | 140M/642M [00:00&amp;lt;00:02, 185MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  25%|\u2588\u2588\u258d       | 158M/642M [00:00&amp;lt;00:02, 186MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  27%|\u2588\u2588\u258b       | 175M/642M [00:01&amp;lt;00:02, 186MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2588       | 193M/642M [00:01&amp;lt;00:02, 185MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 211M/642M [00:01&amp;lt;00:02, 186MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  36%|\u2588\u2588\u2588\u258c      | 228M/642M [00:01&amp;lt;00:02, 186MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  38%|\u2588\u2588\u2588\u258a      | 246M/642M [00:01&amp;lt;00:02, 186MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588      | 264M/642M [00:01&amp;lt;00:02, 186MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258d     | 282M/642M [00:01&amp;lt;00:03, 120MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 299M/642M [00:01&amp;lt;00:02, 134MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  49%|\u2588\u2588\u2588\u2588\u2589     | 317M/642M [00:01&amp;lt;00:02, 146MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 334M/642M [00:02&amp;lt;00:02, 155MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 352M/642M [00:02&amp;lt;00:01, 163MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 369M/642M [00:02&amp;lt;00:01, 169MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 386M/642M [00:02&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 404M/642M [00:02&amp;lt;00:01, 176MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 422M/642M [00:02&amp;lt;00:01, 178MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 439M/642M [00:02&amp;lt;00:01, 181MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 457M/642M [00:02&amp;lt;00:01, 182MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 475M/642M [00:02&amp;lt;00:00, 183MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 492M/642M [00:02&amp;lt;00:00, 183MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 510M/642M [00:03&amp;lt;00:00, 184MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 528M/642M [00:03&amp;lt;00:00, 184MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 545M/642M [00:03&amp;lt;00:00, 185MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 563M/642M [00:03&amp;lt;00:00, 185MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 581M/642M [00:03&amp;lt;00:00, 185MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 599M/642M [00:03&amp;lt;00:00, 185MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 616M/642M [00:03&amp;lt;00:00, 186MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 634M/642M [00:03&amp;lt;00:00, 186MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642M/642M [00:03&amp;lt;00:00, 176MB/s]\n[2025-02-15 00:19:06] ::1:47336 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:19:23] ::1:55296 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:19:23] ::1:55304 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:19:25] ::1:55312 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:19:25] ::1:55326 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:19:25] ::1:55336 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:19:25] ::1:55340 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:19:25] ::1:55356 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:19:26] ::1:55368 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:19:26] ::1:55374 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:19:27] ::1:55384 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:33,  3.68s/it]\r 20%|\u2588\u2588        | 2/10 [00:06&amp;lt;00:23,  2.98s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:25&amp;lt;01:12, 10.38s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:29&amp;lt;00:48,  8.05s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:31&amp;lt;00:29,  5.86s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:34&amp;lt;00:18,  4.62s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:13,  4.49s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:58&amp;lt;00:19,  9.52s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:18&amp;lt;00:12, 12.65s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:39&amp;lt;00:00, 33.90s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:39&amp;lt;00:00, 15.95s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-1...\n::group::Benchmark run on llama31_8b_none-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:60907\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 184.0885715484619 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 159.54925415099933\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 237\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.06267657002354822\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 12.28460772461545\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 17.386480524532274\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 44482.07996600031\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 30862.85329800012\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 42831.11844810462\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 149808.3928404703\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 207.42230149990064\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 151.37681849955698\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 115.63912808893652\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 363.89949895061363\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 148.81437121935886\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 130.37609567006115\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 62.10693593784204\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 291.0353185422197\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 159.60229287527125\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 115.90288600018539\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 136.88698508802923\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 501.7836615595661\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.78798419978209\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]", "duration": "00:02:16", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:16&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:22:06] ::1:56198 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:22:23] ::1:46106 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:22:23] ::1:46110 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:22:24] ::1:46120 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:22:24] ::1:46130 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:22:24] ::1:46142 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:22:24] ::1:46152 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:22:24] ::1:46154 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:22:24] ::1:46164 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:22:24] ::1:46166 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:22:25] ::1:46174 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:24,  2.69s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:19,  2.41s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:27&amp;lt;01:21, 11.70s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:31&amp;lt;00:52,  8.73s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:34&amp;lt;00:32,  6.47s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:35&amp;lt;00:18,  4.74s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:41&amp;lt;00:14,  4.98s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:10&amp;lt;00:25, 12.58s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:15&amp;lt;00:10, 10.21s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:55&amp;lt;00:00, 19.57s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:55&amp;lt;00:00, 11.56s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-2...\n::group::Benchmark run on llama31_8b_none-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:60907\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 135.80019545555115 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 115.55782011699921\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 223\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.08653676566307038\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 16.961206069961793\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 24.005298794935722\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 42856.45267690015\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 34089.337383501515\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 32441.701684499778\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 110739.88585526019\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 291.3618113998382\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 300.3341384992382\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 129.80806229501974\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 466.9519215197033\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 147.01132387931057\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 139.3709766854831\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 41.31808073131581\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 215.5127500595632\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 153.4292422483781\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 120.87283250093606\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 122.25404795235671\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 502.7067793208334\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.7086588024513736\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]", "duration": "00:02:41", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:41&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:24:25] ::1:56220 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:24:42] ::1:43798 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:24:42] ::1:43810 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:24:42] ::1:43822 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:24:42] ::1:43828 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:24:42] ::1:43830 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:24:42] ::1:43838 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:24:42] ::1:43846 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:24:42] ::1:43860 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:24:43] ::1:43876 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:24:43] ::1:43888 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:19,  2.18s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.03s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:25&amp;lt;01:15, 10.75s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:28&amp;lt;00:47,  7.90s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:31&amp;lt;00:29,  5.90s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:32&amp;lt;00:17,  4.45s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:36&amp;lt;00:13,  4.38s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:04&amp;lt;00:23, 11.84s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:38&amp;lt;00:18, 18.78s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:17&amp;lt;00:00, 25.03s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:17&amp;lt;00:00, 13.79s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-4...\n::group::Benchmark run on llama31_8b_none-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:60907\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 161.39450907707214 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 137.8585284790006\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 242\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.0725381310124985\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 14.217473678449705\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 20.122077542867082\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 45750.262950900105\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 31597.57520150015\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 40475.86316940732\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 133730.60886325975\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 385.7828205998885\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 416.01400499894226\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 143.1231933209361\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 520.3172742998504\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 148.29213024581978\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 121.40284805508347\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 54.52524766772024\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 257.5812569400417\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 163.49591808687865\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 114.63889600054245\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 137.47354511568494\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 502.59640984946594\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.3186385677886476\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]", "duration": "00:02:26", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:26&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:27:04] ::1:49344 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:27:21] ::1:40466 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:27:21] ::1:40468 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:27:21] ::1:40480 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:27:21] ::1:40490 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:27:21] ::1:40498 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:27:21] ::1:40510 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:27:21] ::1:40520 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:27:21] ::1:40528 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:27:21] ::1:40530 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:27:21] ::1:40542 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:24,  2.70s/it]\r 20%|\u2588\u2588        | 2/10 [00:03&amp;lt;00:13,  1.73s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:26&amp;lt;01:18, 11.20s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:31&amp;lt;00:52,  8.75s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:32&amp;lt;00:30,  6.04s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:34&amp;lt;00:18,  4.66s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:13,  4.49s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:55&amp;lt;00:16,  8.46s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:12&amp;lt;00:10, 10.98s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:04&amp;lt;00:00, 23.81s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:04&amp;lt;00:00, 12.46s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-8...\n::group::Benchmark run on llama31_8b_none-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:60907\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 145.68590903282166 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 124.57493421100116\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 225\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.08027297034780938\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 15.733502188170638\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 22.26772197448232\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 41886.97208079975\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 33237.91061299926\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 33780.00926173965\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 119535.118373739\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 652.0506928000032\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 743.7545845004934\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 228.70118839302384\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 902.2051362499587\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 144.15969580502826\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 127.47738334816216\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 44.35152254297144\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 232.0813179304302\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 148.6291154394364\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 114.70025449943932\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 121.54463476103095\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 502.231253980317\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.3623916678015577\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]", "duration": "00:03:18", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:03:18&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:29:35] ::1:56642 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:29:52] ::1:48064 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:29:52] ::1:48068 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:29:52] ::1:48074 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:29:52] ::1:48088 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:29:52] ::1:48102 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:29:52] ::1:48110 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:29:52] ::1:48114 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:29:52] ::1:48116 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:29:52] ::1:48120 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:29:52] ::1:48134 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:18,  2.02s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:19,  2.44s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:25&amp;lt;01:16, 10.90s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:28&amp;lt;00:46,  7.67s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:30&amp;lt;00:27,  5.59s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:36&amp;lt;00:23,  5.92s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:37&amp;lt;00:12,  4.23s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:45&amp;lt;00:10,  5.38s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:37&amp;lt;00:19, 19.82s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:51&amp;lt;00:00, 36.73s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:51&amp;lt;00:00, 17.17s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-16...\n::group::Benchmark run on llama31_8b_none-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:60907\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 198.37672400474548 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 171.66368987900023\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 231\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.05825343732881807\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 11.417673716448341\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 16.15950351501413\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 47876.05308359998\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 33541.713318500115\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 48054.972706478446\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 164789.82823177986\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 683.8247497997145\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 743.9088865003214\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 225.80168234208884\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 853.2778371788663\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 153.74621835532136\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 134.14294626075954\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 68.9582116932784\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 315.9016448615345\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 170.10931844123957\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 120.69637049989979\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 140.8409526232707\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 502.4629995106261\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.7889446578566583\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]", "duration": "00:02:49", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:49&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:32:48] ::1:56272 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:33:05] ::1:50752 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:33:05] ::1:50760 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:33:05] ::1:50776 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:33:05] ::1:50784 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:33:05] ::1:50796 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:33:05] ::1:50800 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:33:05] ::1:50812 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:33:05] ::1:50822 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:33:05] ::1:50824 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:33:05] ::1:50834 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:20,  2.32s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:17,  2.24s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:24&amp;lt;01:11, 10.28s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:27&amp;lt;00:44,  7.39s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:29&amp;lt;00:27,  5.41s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:36&amp;lt;00:24,  6.00s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:37&amp;lt;00:13,  4.46s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:56&amp;lt;00:17,  8.91s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:03&amp;lt;00:08,  8.32s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:28&amp;lt;00:00, 32.00s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:28&amp;lt;00:00, 14.81s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-32...\n::group::Benchmark run on llama31_8b_none-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:60907\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 169.3461925983429 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 148.14916147000076\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 234\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.06749953830838884\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 13.229909508444212\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 18.724371926747065\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 42841.33752790003\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 32738.983309500327\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 39519.60860676266\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 140417.2256426694\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 688.712113800284\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 755.2601220004362\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 223.79081051757245\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 845.0221353598863\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 144.23503816066582\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 121.68310281257001\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 52.3658378098546\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 270.5918631379472\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 151.94270716149978\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 110.51887499979784\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 126.57071790178787\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 502.4186823901072\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.8917705036471046\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]", "duration": "00:02:49", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:49&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stderr setup -----------------------------\n[2025-02-15 00:35:33] Shutting down\n[2025-02-15 00:35:33] Waiting for application shutdown.\n[2025-02-15 00:35:33] Application shutdown complete.\n[2025-02-15 00:35:33] Finished server process [4253]\n[2025-02-15 00:35:35] Started server process [6028]\n[2025-02-15 00:35:35] Waiting for application startup.\n[2025-02-15 00:35:39] Application startup complete.\n[2025-02-15 00:35:39] Uvicorn running on http://0.0.0.0:35673 (Press CTRL+C to quit)\n[2025-02-15 00:35:39] ::1:45660 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\n\n----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:35:48] ::1:53166 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:36:05] ::1:51004 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:36:06] ::1:51020 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:36:07] ::1:45196 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:36:07] ::1:45204 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:36:07] ::1:45214 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:36:07] ::1:45216 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:36:08] ::1:45224 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:36:08] ::1:45230 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:36:08] ::1:45242 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:36:09] ::1:45256 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:32,  3.64s/it]\r 20%|\u2588\u2588        | 2/10 [00:06&amp;lt;00:24,  3.12s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:26&amp;lt;01:17, 11.07s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:29&amp;lt;00:45,  7.57s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:34&amp;lt;00:34,  6.95s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:37&amp;lt;00:21,  5.34s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:40&amp;lt;00:14,  4.82s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:43&amp;lt;00:08,  4.08s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:04&amp;lt;00:09,  9.37s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:17&amp;lt;00:00, 28.90s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:17&amp;lt;00:00, 13.71s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-1...\n::group::Benchmark run on llama31_8b_trie-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:35673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 162.95108723640442 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 137.064942590001\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 228\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.07295811613851365\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 14.299790763148676\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 20.23858141682369\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 40369.10094539999\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 34487.41635599981\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 35546.78871936661\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 128202.17242883936\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 195.084928599681\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 155.0364400000035\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 112.68272980282488\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 366.27706378007133\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 139.17324007906163\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 124.13941069163512\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 46.512543779785666\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 248.38463917734703\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 144.79746526027293\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 115.13508000007278\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 115.12316595822362\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 501.83526417071334\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.9452535551818735\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]", "duration": "00:02:35", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:35&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:38:32] ::1:34466 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:38:49] ::1:52082 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:38:50] ::1:52098 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:38:50] ::1:52104 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:38:50] ::1:52118 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:38:50] ::1:52132 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:38:50] ::1:52142 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:38:50] ::1:52158 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:38:51] ::1:52160 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:38:51] ::1:52170 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:38:51] ::1:52184 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:27,  3.08s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:17,  2.24s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:21&amp;lt;01:02,  8.99s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:29&amp;lt;00:51,  8.62s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:30&amp;lt;00:27,  5.58s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:31&amp;lt;00:16,  4.22s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:41&amp;lt;00:17,  5.93s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:07&amp;lt;00:25, 12.57s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:21&amp;lt;00:13, 13.03s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:08&amp;lt;00:00, 23.38s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:08&amp;lt;00:00, 12.84s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-2...\n::group::Benchmark run on llama31_8b_trie-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:35673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 155.39886021614075 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 128.40557909199924\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 233\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.07787823606040717\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 15.264134267839804\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 21.60342268315695\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 42985.66607059984\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 30203.414108001198\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 36421.39068973863\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 122986.56215257905\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 318.98330050007644\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 319.1439909996916\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 145.74442528856923\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 527.1058580001591\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 147.40438106610276\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 134.48506121866487\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 49.29861955999385\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 236.64087449185163\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 153.78431346791643\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 112.30410650023259\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 128.6917426974905\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 502.9304858300747\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.347647849460009\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]", "duration": "00:01:27", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:01:27&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:41:08] ::1:45746 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:41:25] ::1:55648 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:41:25] ::1:55658 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:41:25] ::1:55660 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:41:25] ::1:55672 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:41:25] ::1:55688 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:41:26] ::1:55692 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:41:26] ::1:55700 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:41:26] ::1:55702 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:41:26] ::1:55718 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:41:26] ::1:55734 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:24,  2.68s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:18,  2.35s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:30&amp;lt;01:32, 13.18s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:31&amp;lt;00:49,  8.27s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:34&amp;lt;00:31,  6.37s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:38&amp;lt;00:21,  5.37s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:42&amp;lt;00:15,  5.22s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:56&amp;lt;00:15,  7.79s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:57&amp;lt;00:05,  5.67s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:59&amp;lt;00:00,  4.67s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:59&amp;lt;00:00,  5.97s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-4...\n::group::Benchmark run on llama31_8b_trie-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:35673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 86.9577329158783 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 59.69273263499963\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 208\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.1675245806076015\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 32.83481779908989\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 46.471318660548654\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 35359.19395000019\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 36028.824532000726\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 18918.914582573136\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 58794.208378499934\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 392.90434710019326\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 414.96976899998117\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 159.29589085476633\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 606.633343518497\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 140.1538743139597\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 147.13680308010407\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 34.47211882906532\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 196.44407611375087\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 126.0406706121123\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 113.53851750027388\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 79.62924995677542\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 501.2652398391947\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.923534137096622\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]", "duration": "00:01:24", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:01:24&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:42:34] ::1:51172 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:42:51] ::1:48274 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:42:51] ::1:48290 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:42:51] ::1:48302 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:42:51] ::1:48310 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:42:51] ::1:48318 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:42:51] ::1:48330 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:42:51] ::1:48342 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:42:51] ::1:48356 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:42:51] ::1:48366 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:42:51] ::1:48372 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:19,  2.17s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.02s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:31&amp;lt;01:34, 13.44s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:34&amp;lt;00:57,  9.57s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:36&amp;lt;00:33,  6.73s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:38&amp;lt;00:20,  5.01s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:44&amp;lt;00:16,  5.54s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:52&amp;lt;00:04,  4.78s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:57&amp;lt;00:00,  4.70s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:57&amp;lt;00:00,  5.71s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-8...\n::group::Benchmark run on llama31_8b_trie-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:35673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 83.4301917552948 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 57.13825815400014\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 238\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.17501408553701106\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 34.30276076525416\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 48.548907327966866\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 34335.96848590005\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 37244.65567400057\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 17455.562412596457\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 56442.8232092191\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 657.3255432002043\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 723.474081999484\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 228.66794298696718\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 905.5270362499142\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 126.6078984986304\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 127.62648348417322\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 21.83206287431069\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 154.97154931563088\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 121.39013594808799\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 114.11647100067057\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 63.3830819373468\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 500.39889701944054\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 6.009278125587427\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]", "duration": "00:03:18", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:03:18&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:43:52] ::1:44148 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:44:09] ::1:40482 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:44:09] ::1:40486 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:44:09] ::1:40498 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:44:09] ::1:40514 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:44:09] ::1:40522 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:44:09] ::1:40528 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:44:09] ::1:40540 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:44:09] ::1:40544 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:44:10] ::1:40550 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:44:10] ::1:40556 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:18,  2.03s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.01s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:17&amp;lt;00:50,  7.15s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:23&amp;lt;00:40,  6.78s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:36&amp;lt;00:45,  9.08s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:37&amp;lt;00:14,  4.71s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:32&amp;lt;00:36, 18.07s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:48&amp;lt;00:17, 17.50s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:57&amp;lt;00:00, 31.91s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:57&amp;lt;00:00, 17.71s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-16...\n::group::Benchmark run on llama31_8b_trie-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:35673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 198.21518754959106 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 177.0750413120004\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 231\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.05647323262446868\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 11.068753594395861\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 15.665674730027613\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 53454.967342799864\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 36494.60004699904\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 52847.33069777298\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 170747.3118929113\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 683.4294747000968\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 754.3039234997195\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 224.21470878397443\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 839.3745926011616\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 162.98438525316965\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 132.15963256184847\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 82.6007784210619\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 327.74706270049904\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 190.2014231982701\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 125.41484649955237\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 155.49294127446902\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 503.4565733097406\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.018774805683313\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]", "duration": "00:01:53", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:01:53&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-15 00:47:21] ::1:44004 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-15 00:47:37] ::1:56652 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:47:37] ::1:56662 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:47:37] ::1:56670 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:47:37] ::1:56678 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:47:37] ::1:56682 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:47:37] ::1:56698 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:47:37] ::1:56712 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:47:37] ::1:56722 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:47:38] ::1:56730 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-15 00:47:38] ::1:56738 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:20,  2.26s/it]\r 20%|\u2588\u2588        | 2/10 [00:03&amp;lt;00:14,  1.82s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:20&amp;lt;00:59,  8.54s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:33&amp;lt;01:02, 10.43s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:36&amp;lt;00:38,  7.74s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:44&amp;lt;00:17,  5.69s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:46&amp;lt;00:09,  4.91s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:55&amp;lt;00:05,  5.90s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:21&amp;lt;00:00, 11.72s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:21&amp;lt;00:00,  8.18s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-32...\n::group::Benchmark run on llama31_8b_trie-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:35673\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 112.75467491149902 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 81.79575103000025\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 242\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.12225573913163652\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 23.96212486980076\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 33.91374203511597\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 36051.43883870005\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 36606.840701500005\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 22520.98722345956\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 79292.65182959099\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 695.691706999969\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 757.2476159994039\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 224.9472939131831\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 836.0748865498317\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 132.02502141360065\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 130.20448064315994\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 33.66700727234006\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 191.67092609933542\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 127.43876718060534\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 109.8559270003534\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 84.39117047003299\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 501.33828109994283\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 4.407495301984262\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n--------------------------- Captured stderr teardown ---------------------------\n[2025-02-15 00:48:59] Shutting down\n[2025-02-15 00:49:00] Waiting for application shutdown.\n[2025-02-15 00:49:00] Application shutdown complete.\n[2025-02-15 00:49:00] Finished server process [6028]\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]", "duration": "00:02:35", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:35&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 0.00/642M [00:00&amp;lt;?, ?B/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   1%|          | 7.94M/642M [00:00&amp;lt;00:07, 83.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   3%|\u258e         | 16.9M/642M [00:00&amp;lt;00:07, 89.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   4%|\u258d         | 25.4M/642M [00:00&amp;lt;00:07, 88.5MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   5%|\u258c         | 34.5M/642M [00:00&amp;lt;00:06, 91.3MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   7%|\u258b         | 43.7M/642M [00:00&amp;lt;00:06, 93.0MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   8%|\u258a         | 52.8M/642M [00:00&amp;lt;00:06, 93.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  10%|\u2589         | 61.9M/642M [00:00&amp;lt;00:06, 94.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  11%|\u2588         | 71.0M/642M [00:00&amp;lt;00:06, 94.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  13%|\u2588\u258e        | 80.2M/642M [00:00&amp;lt;00:06, 95.3MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  14%|\u2588\u258d        | 89.3M/642M [00:01&amp;lt;00:06, 94.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  15%|\u2588\u258c        | 98.4M/642M [00:01&amp;lt;00:06, 94.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 107M/642M [00:01&amp;lt;00:05, 93.8MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  18%|\u2588\u258a        | 116M/642M [00:01&amp;lt;00:05, 92.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  20%|\u2588\u2589        | 125M/642M [00:01&amp;lt;00:05, 90.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|\u2588\u2588        | 134M/642M [00:01&amp;lt;00:05, 89.8MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  22%|\u2588\u2588\u258f       | 142M/642M [00:01&amp;lt;00:05, 89.2MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  24%|\u2588\u2588\u258e       | 151M/642M [00:01&amp;lt;00:05, 88.6MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  25%|\u2588\u2588\u258d       | 159M/642M [00:01&amp;lt;00:05, 87.4MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  26%|\u2588\u2588\u258c       | 168M/642M [00:01&amp;lt;00:05, 88.1MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  28%|\u2588\u2588\u258a       | 177M/642M [00:02&amp;lt;00:05, 89.7MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  29%|\u2588\u2588\u2589       | 186M/642M [00:02&amp;lt;00:05, 91.3MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588       | 198M/642M [00:02&amp;lt;00:04, 101MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 214M/642M [00:02&amp;lt;00:03, 122MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  36%|\u2588\u2588\u2588\u258c      | 231M/642M [00:02&amp;lt;00:03, 137MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  39%|\u2588\u2588\u2588\u258a      | 247M/642M [00:02&amp;lt;00:02, 148MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  41%|\u2588\u2588\u2588\u2588      | 264M/642M [00:02&amp;lt;00:02, 155MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258e     | 280M/642M [00:02&amp;lt;00:02, 160MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  46%|\u2588\u2588\u2588\u2588\u258c     | 297M/642M [00:02&amp;lt;00:02, 165MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  49%|\u2588\u2588\u2588\u2588\u2589     | 313M/642M [00:02&amp;lt;00:02, 167MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 330M/642M [00:03&amp;lt;00:01, 170MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 347M/642M [00:03&amp;lt;00:01, 172MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 364M/642M [00:03&amp;lt;00:01, 174MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 381M/642M [00:03&amp;lt;00:01, 174MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 397M/642M [00:03&amp;lt;00:01, 174MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 414M/642M [00:03&amp;lt;00:01, 176MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 431M/642M [00:03&amp;lt;00:01, 177MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 448M/642M [00:03&amp;lt;00:01, 177MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 465M/642M [00:03&amp;lt;00:01, 177MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 482M/642M [00:03&amp;lt;00:00, 177MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 499M/642M [00:04&amp;lt;00:00, 176MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 516M/642M [00:04&amp;lt;00:00, 176MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 533M/642M [00:04&amp;lt;00:00, 177MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 550M/642M [00:04&amp;lt;00:00, 177MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 567M/642M [00:04&amp;lt;00:00, 176MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 584M/642M [00:04&amp;lt;00:00, 176MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 601M/642M [00:04&amp;lt;00:00, 177MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 618M/642M [00:04&amp;lt;00:00, 177MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 635M/642M [00:04&amp;lt;00:00, 178MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642M/642M [00:04&amp;lt;00:00, 138MB/s]\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:20,  2.30s/it]\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:08,  1.10s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:07,  1.06s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:04&amp;lt;00:05,  1.15it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:04&amp;lt;00:03,  1.51it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:04,  1.07s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:06&amp;lt;00:02,  1.24it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:01,  1.59it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:08&amp;lt;00:00,  1.24it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.24it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 0 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 1.003028392791748 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 2.0074875354766846 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 3.0110623836517334 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 4.014294385910034 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 5.017989158630371 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 6.022217035293579 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 7.025548696517944 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 8.029192686080933 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 9.033401727676392 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 10.037199974060059 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 11.044214248657227 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 12.048823356628418 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 13.050659656524658 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 14.055599927902222 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 15.059957027435303 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 16.064696311950684 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 17.06821298599243 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 18.073587656021118 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 19.077013731002808 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 20.08002996444702 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 21.08207392692566 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 22.085561752319336 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 23.088984727859497 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 24.09173893928528 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 25.09509587287903 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 26.097543954849243 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 27.10167908668518 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 28.105867624282837 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 29.111544847488403 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 30.115102767944336 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 31.118773460388184 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 32.12398862838745 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 33.1290283203125 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 34.1338107585907 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 35.13950729370117 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 36.14440059661865 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 37.14924693107605 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 38.15408134460449 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 39.159218072891235 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 40.16173195838928 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 41.16467785835266 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 42.167208433151245 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 43.16954016685486 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 44.17357516288757 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 45.17633533477783 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 46.178685665130615 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 47.18320846557617 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 48.185790061950684 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 49.188247203826904 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 50.19290256500244 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 51.19619011878967 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 52.20018982887268 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 53.20505881309509 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 54.20745849609375 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 55.211272954940796 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 56.21539831161499 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 57.22067427635193 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 58.22481679916382 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 59.229411602020264 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 60.233593225479126 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 61.23825716972351 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 62.24072265625 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 63.24365234375 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 64.24650454521179 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 65.24966025352478 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 66.25271463394165 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 67.25640511512756 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 68.26050734519958 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 69.2641818523407 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 70.26860308647156 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 71.27251124382019 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 72.27698945999146 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 73.28130054473877 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 74.28482675552368 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 75.28995561599731 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 76.29564213752747 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 77.29901671409607 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 78.3028359413147 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 79.30554389953613 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 80.30809569358826 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 81.31102705001831 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 82.31496715545654 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 83.31870317459106 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 84.32206320762634 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 85.32545185089111 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 86.3289544582367 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 87.33324551582336 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 88.33796858787537 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 89.3421790599823 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 90.34622645378113 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 91.34998822212219 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 92.35404372215271 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 93.35724353790283 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 94.36117053031921 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 95.36404991149902 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 96.36819219589233 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 97.3721489906311 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 98.37543296813965 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 99.37878894805908 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 100.3822112083435 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 101.38604402542114 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 102.38957834243774 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 103.39302706718445 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 104.39700627326965 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 105.40080523490906 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 106.40416264533997 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 107.40765047073364 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 108.41084313392639 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 109.41360783576965 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 110.41759300231934 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 111.42153191566467 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 112.42497253417969 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 113.42884612083435 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 114.43295073509216 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 115.43700170516968 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 116.44009804725647 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 117.44443035125732 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 118.44806051254272 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 119.45184755325317 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 120.45589208602905 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 121.4590814113617 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 122.46243238449097 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 123.46726083755493 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 124.47051763534546 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 125.47400522232056 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 126.47791576385498 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 127.48199677467346 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 128.48653531074524 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 129.4912872314453 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 130.4950487613678 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 131.49918222427368 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 132.50382256507874 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 133.5080225467682 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0\nRequest Rate: 1\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-67afdefa-71607e2d603f4f4e1130cf09;c15e6572-7c08-4b9d-ad1c-58588b96c402)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0&amp;#x27;, request_rate=1, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test0/sglang_10_1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Downloading from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json to /tmp/ShareGPT_V3_unfiltered_cleaned_split.json\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    1         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  8.06      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.24      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          243.17    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         344.16    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          587.32    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             4.02      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3242.99   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3686.40   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          39.73     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        37.60     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           64.50     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.22     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        11.22     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           16.32     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           11.59     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         10.97     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            28.57     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 19.02130436897278 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.060285043000476\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.2406509132929444\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 243.16757900541708\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 344.15656334746274\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3242.989169199973\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3686.40395949933\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1647.4916500735235\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 5605.229744469179\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 39.72501029984414\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 37.60268449968862\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 14.41808564055985\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 64.5010321499467\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 12.216180754755465\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 11.216692196335366\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.9030616736199961\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 16.315184990194265\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 11.589162350216379\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 10.965115499857347\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.447447986024907\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 28.57373289114546\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 4.023417474567073\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:11,  1.27s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:05,  1.21it/s]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.38it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:02,  1.76it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:03,  1.04it/s]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:05&amp;lt;00:02,  1.44it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.73it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.79it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.49it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1\nRequest Rate: 2\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-67afdf0d-77fa3e923f592ead22de2750;f7dfb25e-e1fd-402f-9db6-f6e96e283fd4)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1&amp;#x27;, request_rate=2, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test1/sglang_10_2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    2         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.70      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.49      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          292.36    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         413.77    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          706.13    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             4.52      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3032.95   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3360.22   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          30.60     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        23.61     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           58.92     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          11.29     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        11.01     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           13.11     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           10.86     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         10.82     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            25.54     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.228536128997803 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.704170228998919\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.4916089028803228\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 292.3553449645433\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 413.77230965900156\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3032.952009300243\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3360.220702999868\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1639.827710157826\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 5431.183198359686\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 30.601223799931176\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 23.613279499841155\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 12.954124742471633\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 58.92187325929626\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 11.290125732951209\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 11.013726403929736\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.8688506920229275\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 13.110148779223163\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 10.862274454775063\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 10.818480000125419\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.9246861796549934\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 25.544469161049683\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 4.523978219081006\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:13", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:13&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:06,  1.31it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:03,  2.61it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:07,  1.11s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.33it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:02,  1.86it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.10it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.97it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.80it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.60it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2\nRequest Rate: 4\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-67afdf19-5717da5d5cd291967538740a;8cc9f828-0001-4ed9-ba4d-c6e0586937a6)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2&amp;#x27;, request_rate=4, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test2/sglang_10_4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    4         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.26      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.60      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          313.04    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         443.04    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          756.08    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.12      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3202.75   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3592.61   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          35.96     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        31.91     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           75.70     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.14     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        11.66     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           14.82     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           11.46     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         11.28     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            27.72     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.98928952217102 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.261222289000216\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.597132243263126\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 313.0379196795727\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 443.04448428119116\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3202.753421300258\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3592.608604000816\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1695.6414822744346\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 5580.780992870151\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 35.95599470027082\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 31.909958001051564\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 18.567999257126587\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 75.70178492072955\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 12.139787884553913\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 11.664139014944078\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.3118258081467373\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 14.816280360179462\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 11.457235486975774\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 11.276311000983696\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.966137131309253\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 27.717044120090574\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.115220756379934\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  2.05it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.40it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:07,  1.12s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:02,  1.70it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.31it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:04&amp;lt;00:00,  2.18it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:05&amp;lt;00:00,  1.86it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:05&amp;lt;00:00,  1.73it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3\nRequest Rate: 8\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-67afdf26-4fa43f4a1b51c9885c4f74c4;56dde08e-96ce-4556-b882-9ffc192f6632)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3&amp;#x27;, request_rate=8, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test3/sglang_10_8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    8         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  5.78      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.73      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          338.97    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         479.75    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          818.73    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.36      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3098.53   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3478.88   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          32.50     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        25.64     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           54.35     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.19     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        11.38     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           17.84     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           11.09     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         10.82     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            24.31     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.628235340118408 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 5.78214679300072\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.7294614540234408\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 338.97444498859437\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 479.7526073461024\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3098.5312587001317\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3478.880487499737\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1620.0488419615583\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 5395.623855670748\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 32.50126099992485\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 25.636426999881223\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 12.336382790764855\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 54.34693297018384\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 12.193203722810992\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 11.381566447910073\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.26507662373961\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.842868700085912\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 11.09266378292368\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 10.823037499903876\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.314539876523106\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 24.30979143915465\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.358790376008612\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.02it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:01,  4.12it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:07,  1.07s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:02&amp;lt;00:04,  1.36it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:02,  1.79it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:02,  1.40it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.56it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:04&amp;lt;00:00,  2.35it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:05&amp;lt;00:00,  1.82it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:05&amp;lt;00:00,  1.79it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4\nRequest Rate: 16\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-67afdf32-771a795802bd254b7a891a17;94cb6a7a-8637-4997-b95f-569a77fde099)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4&amp;#x27;, request_rate=16, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test4/sglang_10_16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    16        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  5.58      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.79      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          351.22    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         497.08    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          848.29    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.48      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3055.55   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3428.72   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          36.05     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        33.75     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           50.55     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          11.95     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        11.16     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           17.40     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           10.92     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         10.77     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            13.33     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.832075834274292 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 5.5806063520012685\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.7919199759384368\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 351.21631528393357\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 497.0786013253223\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3055.5455262001487\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3428.722394000033\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1607.5957456846631\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 5344.040972219082\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 36.05137640024623\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 33.74757400069939\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 10.344513969183648\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 50.547110590814555\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 11.947672939381139\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 11.163470055192313\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.146127363586526\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.395214071980355\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 10.924300822358884\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 10.766902000796108\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.8720292874057893\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 13.329644180303147\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.475293065787368\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.84it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:01,  4.69it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:07,  1.05s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:02&amp;lt;00:04,  1.32it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:02,  1.76it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.18it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.19it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.13it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:05&amp;lt;00:00,  1.71it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:05&amp;lt;00:00,  1.69it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5\nRequest Rate: 32\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Fail to load tokenizer config with error=You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-67afdf3d-2442f4d8562ba8cf68e1d931;1d1ad48d-ba5c-4cfb-8631-d7d00867c084)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\nBecause when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5&amp;#x27;, request_rate=32, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-runner/pytest-0/sglang_benchmark_test5/sglang_10_32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    32        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  5.92      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.69      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          330.82    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         468.21    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          799.03    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.52      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3269.75   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3657.51   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          56.51     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        53.91     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           85.39     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          11.87     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        11.83     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           13.06     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           11.63     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         10.92     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            19.31     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.208302736282349 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 5.9246683189994656\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.6878581992398791\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 330.8202070510163\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 468.2118644691425\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3269.751856399671\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3657.513387999643\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1788.564713296496\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 5761.280050669884\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 56.50770169977477\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 53.91064200011897\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 21.594073715714778\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 85.38918589927562\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 11.870443794390397\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 11.827365231604645\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.6611937604253953\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 13.058488808253514\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 11.625260109262847\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 10.92196999979933\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 4.181780375504533\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 19.311461390443583\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.518877480304001\n\n"}]}, "renderCollapsed": ["passed"], "initialSort": "result", "title": "shortfin_index.html"}' id="data-container"></div>
<script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
</footer>
</html>