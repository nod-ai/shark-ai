<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<title id="head-title">index.html</title>
<style type="text/css">body {
  font-family: Helvetica, Arial, sans-serif;
  font-size: 12px;
  /* do not increase min-width as some may use split screens */
  min-width: 800px;
  color: #999;
}

h1 {
  font-size: 24px;
  color: black;
}

h2 {
  font-size: 16px;
  color: black;
}

p {
  color: black;
}

a {
  color: #999;
}

table {
  border-collapse: collapse;
}

/******************************
 * SUMMARY INFORMATION
 ******************************/
#environment td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  vertical-align: top;
}
#environment tr:nth-child(odd) {
  background-color: #f6f6f6;
}
#environment ul {
  margin: 0;
  padding: 0 20px;
}

/******************************
 * TEST RESULT COLORS
 ******************************/
span.passed,
.passed .col-result {
  color: green;
}

span.skipped,
span.xfailed,
span.rerun,
.skipped .col-result,
.xfailed .col-result,
.rerun .col-result {
  color: orange;
}

span.error,
span.failed,
span.xpassed,
.error .col-result,
.failed .col-result,
.xpassed .col-result {
  color: red;
}

.col-links__extra {
  margin-right: 3px;
}

/******************************
 * RESULTS TABLE
 *
 * 1. Table Layout
 * 2. Extra
 * 3. Sorting items
 *
 ******************************/
/*------------------
 * 1. Table Layout
 *------------------*/
#results-table {
  border: 1px solid #e6e6e6;
  color: #999;
  font-size: 12px;
  width: 100%;
}
#results-table th,
#results-table td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  text-align: left;
}
#results-table th {
  font-weight: bold;
}

/*------------------
 * 2. Extra
 *------------------*/
.logwrapper {
  max-height: 230px;
  overflow-y: scroll;
  background-color: #e6e6e6;
}
.logwrapper.expanded {
  max-height: none;
}
.logwrapper.expanded .logexpander:after {
  content: "collapse [-]";
}
.logwrapper .logexpander {
  z-index: 1;
  position: sticky;
  top: 10px;
  width: max-content;
  border: 1px solid;
  border-radius: 3px;
  padding: 5px 7px;
  margin: 10px 0 10px calc(100% - 80px);
  cursor: pointer;
  background-color: #e6e6e6;
}
.logwrapper .logexpander:after {
  content: "expand [+]";
}
.logwrapper .logexpander:hover {
  color: #000;
  border-color: #000;
}
.logwrapper .log {
  min-height: 40px;
  position: relative;
  top: -50px;
  height: calc(100% + 50px);
  border: 1px solid #e6e6e6;
  color: black;
  display: block;
  font-family: "Courier New", Courier, monospace;
  padding: 5px;
  padding-right: 80px;
  white-space: pre-wrap;
}

div.media {
  border: 1px solid #e6e6e6;
  float: right;
  height: 240px;
  margin: 0 5px;
  overflow: hidden;
  width: 320px;
}

.media-container {
  display: grid;
  grid-template-columns: 25px auto 25px;
  align-items: center;
  flex: 1 1;
  overflow: hidden;
  height: 200px;
}

.media-container--fullscreen {
  grid-template-columns: 0px auto 0px;
}

.media-container__nav--right,
.media-container__nav--left {
  text-align: center;
  cursor: pointer;
}

.media-container__viewport {
  cursor: pointer;
  text-align: center;
  height: inherit;
}
.media-container__viewport img,
.media-container__viewport video {
  object-fit: cover;
  width: 100%;
  max-height: 100%;
}

.media__name,
.media__counter {
  display: flex;
  flex-direction: row;
  justify-content: space-around;
  flex: 0 0 25px;
  align-items: center;
}

.collapsible td:not(.col-links) {
  cursor: pointer;
}
.collapsible td:not(.col-links):hover::after {
  color: #bbb;
  font-style: italic;
  cursor: pointer;
}

.col-result {
  width: 130px;
}
.col-result:hover::after {
  content: " (hide details)";
}

.col-result.collapsed:hover::after {
  content: " (show details)";
}

#environment-header h2:hover::after {
  content: " (hide details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

#environment-header.collapsed h2:hover::after {
  content: " (show details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

/*------------------
 * 3. Sorting items
 *------------------*/
.sortable {
  cursor: pointer;
}
.sortable.desc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: -12.5px;
  border: 10px solid #4caf50;
  border-bottom: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}
.sortable.asc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: 12.5px;
  border: 10px solid #4caf50;
  border-top: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}

.hidden, .summary__reload__button.hidden {
  display: none;
}

.summary__data {
  flex: 0 0 550px;
}
.summary__reload {
  flex: 1 1;
  display: flex;
  justify-content: center;
}
.summary__reload__button {
  flex: 0 0 300px;
  display: flex;
  color: white;
  font-weight: bold;
  background-color: #4caf50;
  text-align: center;
  justify-content: center;
  align-items: center;
  border-radius: 3px;
  cursor: pointer;
}
.summary__reload__button:hover {
  background-color: #46a049;
}
.summary__spacer {
  flex: 0 0 550px;
}

.controls {
  display: flex;
  justify-content: space-between;
}

.filters,
.collapse {
  display: flex;
  align-items: center;
}
.filters button,
.collapse button {
  color: #999;
  border: none;
  background: none;
  cursor: pointer;
  text-decoration: underline;
}
.filters button:hover,
.collapse button:hover {
  color: #ccc;
}

.filter__label {
  margin-right: 10px;
}

      </style>
</head>
<body>
<h1 id="title">index.html</h1>
<p>Report generated on 14-Jan-2025 at 12:23:58 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
<div id="environment-header">
<h2>Environment</h2>
</div>
<table id="environment"></table>
<!-- TEMPLATES -->
<template id="template_environment_row">
<tr>
<td></td>
<td></td>
</tr>
</template>
<template id="template_results-table__body--empty">
<tbody class="results-table-row">
<tr id="not-found-message">
<td colspan="4">No results found. Check the filters.
</td></tr>
</tbody></template>
<template id="template_results-table__tbody">
<tbody class="results-table-row">
<tr class="collapsible">
</tr>
<tr class="extras-row">
<td class="extra" colspan="4">
<div class="extraHTML"></div>
<div class="media">
<div class="media-container">
<div class="media-container__nav--left">&lt;</div>
<div class="media-container__viewport">
<img src=""/>
<video controls="">
<source src="" type="video/mp4"/>
</video>
</div>
<div class="media-container__nav--right">&gt;</div>
</div>
<div class="media__name"></div>
<div class="media__counter"></div>
</div>
<div class="logwrapper">
<div class="logexpander"></div>
<div class="log"></div>
</div>
</td>
</tr>
</tbody>
</template>
<!-- END TEMPLATES -->
<div class="summary">
<div class="summary__data">
<h2>Summary</h2>
<div class="additional-summary prefix">
</div>
<p class="run-count">18 tests ran in 4071 seconds</p>
<p class="filter">(Un)check the boxes to filter the results.</p>
<div class="summary__reload">
<div class="summary__reload__button hidden" onclick="location.reload()">
<div>There are still tests running. <br/>Reload this page to get the latest results!</div>
</div>
</div>
<div class="summary__spacer"></div>
<div class="controls">
<div class="filters">
<input checked="true" class="filter" data-test-result="failed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="failed">0 Failed,</span>
<input checked="true" class="filter" data-test-result="passed" name="filter_checkbox" type="checkbox"/>
<span class="passed">18 Passed,</span>
<input checked="true" class="filter" data-test-result="skipped" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="skipped">0 Skipped,</span>
<input checked="true" class="filter" data-test-result="xfailed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xfailed">0 Expected failures,</span>
<input checked="true" class="filter" data-test-result="xpassed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xpassed">0 Unexpected passes,</span>
<input checked="true" class="filter" data-test-result="error" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="error">0 Errors,</span>
<input checked="true" class="filter" data-test-result="rerun" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="rerun">0 Reruns</span>
</div>
<div class="collapse">
<button id="show_all_details">Show all details</button> / <button id="hide_all_details">Hide all details</button>
</div>
</div>
</div>
<div class="additional-summary summary">
</div>
<div class="additional-summary postfix">
</div>
</div>
<table id="results-table">
<thead id="results-table-head">
<tr>
<th class="sortable" data-column-type="result">Result</th>
<th class="sortable" data-column-type="testId">Test</th>
<th class="sortable" data-column-type="duration">Duration</th>
<th>Links</th>
</tr>
</thead>
</table>
</body>
<footer>
<div data-jsonblob='{"environment": {"Python": "3.11.10", "Platform": "Linux-5.15.0-130-generic-x86_64-with-glibc2.35", "Packages": {"pytest": "8.0.0", "pluggy": "1.5.0"}, "Plugins": {"timeout": "2.3.1", "anyio": "4.8.0", "html": "4.1.1", "xdist": "3.5.0", "metadata": "3.1.1"}, "CI": "true"}, "tests": {"reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-1-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-1-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:13:54", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-1-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:13:54&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stdout setup -----------------------------\nDownloading dataset llama3_8B_fp16\n  gguf: [PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/meta-llama-3.1-8b-instruct.f16.gguf&amp;#x27;)]\n  tokenizer_config.json: [PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/tokenizer_config.json&amp;#x27;), PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/tokenizer.json&amp;#x27;)]\nExporting prefill_bs1\nExporting decode_bs1\nExporting prefill_bs4\nExporting decode_bs4\nGENERATED!\nExporting\nSaving to &amp;#x27;/tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/model.mlir&amp;#x27;\n\n---------------------------- Captured stderr setup -----------------------------\n/home/nod/actions-runner-shark-ai/_work/shark-ai/shark-ai/sharktank/sharktank/types/gguf_interop/base.py:100: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n  data_tensor = torch.as_tensor(data.reshape(logical_shape))\n/home/nod/actions-runner-shark-ai/_work/shark-ai/shark-ai/.venv/lib/python3.11/site-packages/iree/turbine/runtime/op_reg/base.py:407: UserWarning: ComplexHalf support is experimental and many operators don&amp;#x27;t support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:41.)\n  return self.return_tensor(torch.empty(size, dtype=dtype, device=&amp;quot;meta&amp;quot;))\n\n------------------------------ Captured log setup ------------------------------\nINFO     sglang_benchmarks.conftest:conftest.py:34 Preparing model artifacts...\n::group::Preparing model artifacts\nINFO     integration_tests.llm.utils:utils.py:50 Download model llama3_8B_fp16 with `hf_datasets` to /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0...\nINFO     integration_tests.llm.utils:utils.py:62 Model llama3_8B_fp16 successfully downloaded.\nINFO     integration_tests.llm.utils:utils.py:82 Exporting model with following settings:\n  MLIR Path: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/model.mlir\n  Config Path: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/config.json\n  Batch Sizes: 1,4\nINFO     integration_tests.llm.utils:utils.py:101 Model successfully exported to /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/model.mlir\nINFO     integration_tests.llm.utils:utils.py:105 Compiling model to /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/model.vmfb\nINFO     integration_tests.llm.utils:utils.py:116 Model successfully compiled to /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/model.vmfb\nINFO     sglang_benchmarks.conftest:conftest.py:61 Model artifacts setup successfully\n::endgroup::\nINFO     sglang_benchmarks.conftest:conftest.py:96 Saving edited config to: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/1_4_none.json\n\nINFO     sglang_benchmarks.conftest:conftest.py:97 Config: {\n  &amp;quot;module_name&amp;quot;: &amp;quot;module&amp;quot;,\n  &amp;quot;module_abi_version&amp;quot;: 1,\n  &amp;quot;max_seq_len&amp;quot;: 131072,\n  &amp;quot;attn_head_dim&amp;quot;: 128,\n  &amp;quot;prefill_batch_sizes&amp;quot;: [\n    1,\n    4\n  ],\n  &amp;quot;decode_batch_sizes&amp;quot;: [\n    1,\n    4\n  ],\n  &amp;quot;transformer_block_count&amp;quot;: 32,\n  &amp;quot;paged_kv_cache&amp;quot;: {\n    &amp;quot;attention_head_count_kv&amp;quot;: 8,\n    &amp;quot;block_seq_stride&amp;quot;: 16,\n    &amp;quot;device_block_count&amp;quot;: 256,\n    &amp;quot;prefix_sharing_algorithm&amp;quot;: &amp;quot;none&amp;quot;\n  }\n}\n\n----------------------------- Captured stderr call -----------------------------\n[2025-01-14 11:26:57] Started server process [1999960]\n[2025-01-14 11:26:57] Waiting for application startup.\n[2025-01-14 11:27:01] Application startup complete.\n[2025-01-14 11:27:01] Uvicorn running on http://0.0.0.0:49395 (Press CTRL+C to quit)\n[2025-01-14 11:27:01] 127.0.0.1:33714 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 11:27:04] 127.0.0.1:33728 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 11:29:08] 127.0.0.1:38340 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:29:09] 127.0.0.1:38352 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:29:10] 127.0.0.1:38362 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:29:10] 127.0.0.1:38374 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:29:11] 127.0.0.1:38386 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:29:11] 127.0.0.1:38400 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:29:11] 127.0.0.1:38412 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:29:11] 127.0.0.1:38420 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:29:11] 127.0.0.1:38424 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:29:12] 127.0.0.1:38430 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:35,  3.97s/it]\r 20%|\u2588\u2588        | 2/10 [00:06&amp;lt;00:22,  2.84s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:18&amp;lt;00:49,  7.07s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:28&amp;lt;00:50,  8.48s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:40&amp;lt;00:47,  9.52s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:40&amp;lt;00:25,  6.40s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:41&amp;lt;00:13,  4.65s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:10&amp;lt;00:24, 12.41s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:31&amp;lt;00:14, 14.94s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:55&amp;lt;00:00, 36.28s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:55&amp;lt;00:00, 17.51s/it]\n[2025-01-14 11:32:04] Shutting down\n[2025-01-14 11:32:04] Waiting for application shutdown.\n[2025-01-14 11:32:04] Application shutdown complete.\n[2025-01-14 11:32:04] Finished server process [1999960]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 49395\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 49395...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:49395...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0027801990509033 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0042316913604736 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0066323280334473 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.009409189224243 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.011713266372681 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.013967037200928 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49395\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 302.25414061546326 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 898\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 49528.403610904934\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 36990.484665497206\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 154.70419498160481\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.0026240013539791107\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 15.841723984633921\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 175.1072044110042\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-2-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-2-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:45", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-2-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:45&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-14 11:32:07] Started server process [2000591]\n[2025-01-14 11:32:07] Waiting for application startup.\n[2025-01-14 11:32:11] Application startup complete.\n[2025-01-14 11:32:11] Uvicorn running on http://0.0.0.0:43659 (Press CTRL+C to quit)\n[2025-01-14 11:32:12] 127.0.0.1:46194 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 11:32:15] 127.0.0.1:46196 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 11:34:19] 127.0.0.1:47370 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:34:19] 127.0.0.1:47376 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:34:20] 127.0.0.1:47380 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:34:20] 127.0.0.1:47392 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:34:20] 127.0.0.1:47402 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:34:20] 127.0.0.1:47410 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:34:20] 127.0.0.1:47414 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:34:21] 127.0.0.1:47420 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:34:21] 127.0.0.1:47424 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:34:21] 127.0.0.1:47426 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:29,  3.30s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:18,  2.33s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:18&amp;lt;00:53,  7.62s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:23&amp;lt;00:38,  6.36s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:27&amp;lt;00:27,  5.46s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:37&amp;lt;00:28,  7.04s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:39&amp;lt;00:16,  5.34s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:35&amp;lt;00:43, 21.60s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [02:08&amp;lt;00:25, 25.03s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:29&amp;lt;00:00, 23.79s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:29&amp;lt;00:00, 14.91s/it]\n[2025-01-14 11:36:48] Shutting down\n[2025-01-14 11:36:49] Waiting for application shutdown.\n[2025-01-14 11:36:49] Application shutdown complete.\n[2025-01-14 11:36:49] Finished server process [2000591]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 43659\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 43659...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:43659...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0036351680755615 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.005404472351074 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.007537841796875 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.009671211242676 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.0118324756622314 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.013981103897095 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:43659\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 276.76099848747253 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 419\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 51643.78458021674\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 31654.167835018598\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 359.28291850723326\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.004797009751200676\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 18.604603359616863\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 149.10288310801843\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-4-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-4-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:05:17", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-4-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:05:17&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-14 11:36:52] Started server process [2001220]\n[2025-01-14 11:36:52] Waiting for application startup.\n[2025-01-14 11:36:57] Application startup complete.\n[2025-01-14 11:36:57] Uvicorn running on http://0.0.0.0:40853 (Press CTRL+C to quit)\n[2025-01-14 11:36:57] 127.0.0.1:52094 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 11:37:01] 127.0.0.1:47380 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 11:39:05] 127.0.0.1:40952 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:39:05] 127.0.0.1:40964 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:39:05] 127.0.0.1:40980 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:39:05] 127.0.0.1:40982 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:39:06] 127.0.0.1:40984 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:39:06] 127.0.0.1:40990 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:39:06] 127.0.0.1:41000 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:39:06] 127.0.0.1:41004 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:39:06] 127.0.0.1:41012 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:39:06] 127.0.0.1:41018 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:27,  3.10s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:17,  2.25s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:25&amp;lt;01:13, 10.57s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:28&amp;lt;00:45,  7.63s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:29&amp;lt;00:27,  5.40s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:36&amp;lt;00:23,  5.75s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:39&amp;lt;00:15,  5.07s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:41&amp;lt;00:07,  3.88s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [02:07&amp;lt;00:29, 29.73s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [03:00&amp;lt;00:00, 36.92s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [03:00&amp;lt;00:00, 18.08s/it]\n[2025-01-14 11:42:06] Shutting down\n[2025-01-14 11:42:06] Waiting for application shutdown.\n[2025-01-14 11:42:06] Application shutdown complete.\n[2025-01-14 11:42:06] Finished server process [2001220]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 40853\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 40853...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:40853...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.002936840057373 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0048844814300537 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.007107734680176 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.009434223175049 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.012281656265259 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.014438629150391 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:40853\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 308.5637950897217 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 817\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 51200.514365005074\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 32945.91426249826\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 659.2098690161947\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.004526984412223101\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 15.342965422242411\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 180.79946892004227\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-8-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-8-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:16", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-8-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:16&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-14 11:42:09] Started server process [2001838]\n[2025-01-14 11:42:09] Waiting for application startup.\n[2025-01-14 11:42:13] Application startup complete.\n[2025-01-14 11:42:13] Uvicorn running on http://0.0.0.0:52119 (Press CTRL+C to quit)\n[2025-01-14 11:42:14] 127.0.0.1:45648 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 11:42:17] 127.0.0.1:45658 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 11:44:21] 127.0.0.1:51746 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:44:21] 127.0.0.1:51748 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:44:21] 127.0.0.1:51760 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:44:21] 127.0.0.1:51766 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:44:22] 127.0.0.1:51778 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:44:22] 127.0.0.1:51792 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:44:22] 127.0.0.1:51802 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:44:22] 127.0.0.1:51808 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:44:22] 127.0.0.1:51818 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:44:22] 127.0.0.1:51826 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:24,  2.72s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:19,  2.38s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:16&amp;lt;00:45,  6.54s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:33&amp;lt;01:03, 10.54s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:34&amp;lt;00:35,  7.18s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:34&amp;lt;00:19,  4.84s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:40&amp;lt;00:15,  5.24s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:02&amp;lt;00:21, 10.55s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:34&amp;lt;00:17, 17.10s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:00&amp;lt;00:00, 20.00s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:00&amp;lt;00:00, 12.05s/it]\n[2025-01-14 11:46:22] Shutting down\n[2025-01-14 11:46:22] Waiting for application shutdown.\n[2025-01-14 11:46:22] Application shutdown complete.\n[2025-01-14 11:46:22] Finished server process [2001838]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 52119\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 52119...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:52119...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0029735565185547 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0058176517486572 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0079379081726074 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.010044813156128 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.012242555618286 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.014425754547119 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:52119\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 248.28247594833374 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 165\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 44085.50601369352\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 34256.17275948753\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 810.8561680128332\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.005006964784115553\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 23.01597396955802\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 120.52498858701438\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-16-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-16-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:05:21", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-16-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:05:21&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-14 11:46:25] Started server process [2002470]\n[2025-01-14 11:46:25] Waiting for application startup.\n[2025-01-14 11:46:29] Application startup complete.\n[2025-01-14 11:46:29] Uvicorn running on http://0.0.0.0:41581 (Press CTRL+C to quit)\n[2025-01-14 11:46:30] 127.0.0.1:39822 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 11:46:33] 127.0.0.1:39834 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 11:48:38] 127.0.0.1:41338 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:48:38] 127.0.0.1:41340 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:48:38] 127.0.0.1:41354 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:48:38] 127.0.0.1:41362 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:48:38] 127.0.0.1:41366 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:48:38] 127.0.0.1:41370 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:48:38] 127.0.0.1:41378 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:48:38] 127.0.0.1:41386 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:48:38] 127.0.0.1:41394 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:48:38] 127.0.0.1:41398 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:24,  2.77s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.08s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:24&amp;lt;01:12, 10.38s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:26&amp;lt;00:41,  6.85s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:36&amp;lt;00:40,  8.16s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:38&amp;lt;00:23,  5.99s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:12,  4.23s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:47&amp;lt;00:11,  5.75s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [02:17&amp;lt;00:31, 31.99s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [03:05&amp;lt;00:00, 36.80s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [03:05&amp;lt;00:00, 18.52s/it]\n[2025-01-14 11:51:43] Shutting down\n[2025-01-14 11:51:43] Waiting for application shutdown.\n[2025-01-14 11:51:43] Application shutdown complete.\n[2025-01-14 11:51:43] Finished server process [2002470]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 41581\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 41581...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:41581...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0014674663543701 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0035903453826904 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0057973861694336 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.007983207702637 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.010677814483643 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.012831211090088 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41581\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 312.93984055519104 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 1066\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 54101.30658819107\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 37310.80913299229\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 885.8351054950617\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.006399000994861126\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 14.97985628679264\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 185.18201689596754\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-32-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-32-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:01", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model0-write_config0-32-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:01&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-14 11:51:46] Started server process [2003089]\n[2025-01-14 11:51:46] Waiting for application startup.\n[2025-01-14 11:51:50] Application startup complete.\n[2025-01-14 11:51:50] Uvicorn running on http://0.0.0.0:43513 (Press CTRL+C to quit)\n[2025-01-14 11:51:51] 127.0.0.1:43854 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 11:51:54] 127.0.0.1:43858 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 11:53:58] 127.0.0.1:48072 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:53:58] 127.0.0.1:48078 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:53:59] 127.0.0.1:48092 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:53:59] 127.0.0.1:48094 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:53:59] 127.0.0.1:48104 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:53:59] 127.0.0.1:48114 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:53:59] 127.0.0.1:48120 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:53:59] 127.0.0.1:48124 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:53:59] 127.0.0.1:48132 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:53:59] 127.0.0.1:48144 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:22,  2.50s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:15,  1.94s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:15&amp;lt;00:43,  6.25s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:25&amp;lt;00:45,  7.66s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:29&amp;lt;00:31,  6.38s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:33&amp;lt;00:22,  5.53s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:15,  5.29s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:06&amp;lt;00:25, 12.83s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:41&amp;lt;00:19, 19.46s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:45&amp;lt;00:00, 14.84s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:45&amp;lt;00:00, 10.55s/it]\n[2025-01-14 11:55:44] Shutting down\n[2025-01-14 11:55:44] Waiting for application shutdown.\n[2025-01-14 11:55:44] Application shutdown complete.\n[2025-01-14 11:55:44] Finished server process [2003089]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 43513\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 43513...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:43513...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0030081272125244 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.004300117492676 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.00705623626709 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.009219646453857 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.0112833976745605 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.013434171676636 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith none...\n::group::Benchmark run with none algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: none\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:43513\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 233.28657269477844 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 152\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 42069.88810939365\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 31243.640942993807\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 893.802026490448\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.004606961738318205\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 26.28820623188277\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 105.52260491001653\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-1-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-1-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:59", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-1-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:59&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "------------------------------ Captured log setup ------------------------------\nINFO     sglang_benchmarks.conftest:conftest.py:34 Preparing model artifacts...\n::group::Preparing model artifacts\nINFO     sglang_benchmarks.conftest:conftest.py:40 Reusing existing model artifacts directory: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\n::endgroup::\nINFO     sglang_benchmarks.conftest:conftest.py:96 Saving edited config to: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0/1_4_trie.json\n\nINFO     sglang_benchmarks.conftest:conftest.py:97 Config: {\n  &amp;quot;module_name&amp;quot;: &amp;quot;module&amp;quot;,\n  &amp;quot;module_abi_version&amp;quot;: 1,\n  &amp;quot;max_seq_len&amp;quot;: 131072,\n  &amp;quot;attn_head_dim&amp;quot;: 128,\n  &amp;quot;prefill_batch_sizes&amp;quot;: [\n    1,\n    4\n  ],\n  &amp;quot;decode_batch_sizes&amp;quot;: [\n    1,\n    4\n  ],\n  &amp;quot;transformer_block_count&amp;quot;: 32,\n  &amp;quot;paged_kv_cache&amp;quot;: {\n    &amp;quot;attention_head_count_kv&amp;quot;: 8,\n    &amp;quot;block_seq_stride&amp;quot;: 16,\n    &amp;quot;device_block_count&amp;quot;: 256,\n    &amp;quot;prefix_sharing_algorithm&amp;quot;: &amp;quot;trie&amp;quot;\n  }\n}\n\n----------------------------- Captured stderr call -----------------------------\n[2025-01-14 11:55:47] Started server process [2003716]\n[2025-01-14 11:55:47] Waiting for application startup.\n[2025-01-14 11:55:52] Application startup complete.\n[2025-01-14 11:55:52] Uvicorn running on http://0.0.0.0:57199 (Press CTRL+C to quit)\n[2025-01-14 11:55:52] 127.0.0.1:35154 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 11:55:56] 127.0.0.1:35164 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 11:58:00] 127.0.0.1:59596 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:58:00] 127.0.0.1:59604 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:58:02] 127.0.0.1:59612 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:58:02] 127.0.0.1:59620 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:58:02] 127.0.0.1:59636 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:58:02] 127.0.0.1:59644 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:58:02] 127.0.0.1:59654 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:58:02] 127.0.0.1:59664 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:58:03] 127.0.0.1:59678 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 11:58:03] 127.0.0.1:59690 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:34,  3.88s/it]\r 20%|\u2588\u2588        | 2/10 [00:05&amp;lt;00:19,  2.45s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:18&amp;lt;00:51,  7.35s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:32&amp;lt;01:00, 10.16s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:33&amp;lt;00:33,  6.72s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:37&amp;lt;00:23,  5.79s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:41&amp;lt;00:15,  5.24s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:48&amp;lt;00:11,  5.63s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:20&amp;lt;00:13, 13.90s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:42&amp;lt;00:00, 35.12s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:42&amp;lt;00:00, 16.29s/it]\n[2025-01-14 12:00:43] Shutting down\n[2025-01-14 12:00:43] Waiting for application shutdown.\n[2025-01-14 12:00:43] Application shutdown complete.\n[2025-01-14 12:00:43] Finished server process [2003716]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 57199\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 57199...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:57199...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.00264310836792 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0045230388641357 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0064480304718018 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.008543491363525 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.010665416717529 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.013292551040649 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:57199\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 290.64684772491455 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 1980\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 44445.384234999074\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 34691.322066035355\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 169.01017195777968\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.005307956598699093\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 17.03005455312456\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 162.88849758799188\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-2-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-2-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:56", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-2-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:56&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-14 12:00:46] Started server process [2004337]\n[2025-01-14 12:00:46] Waiting for application startup.\n[2025-01-14 12:00:50] Application startup complete.\n[2025-01-14 12:00:50] Uvicorn running on http://0.0.0.0:47971 (Press CTRL+C to quit)\n[2025-01-14 12:00:51] 127.0.0.1:43700 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 12:00:54] 127.0.0.1:43704 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 12:02:59] 127.0.0.1:43350 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:02:59] 127.0.0.1:43358 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:02:59] 127.0.0.1:43360 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:02:59] 127.0.0.1:43370 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:03:00] 127.0.0.1:43386 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:03:00] 127.0.0.1:43388 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:03:00] 127.0.0.1:43398 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:03:00] 127.0.0.1:43400 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:03:00] 127.0.0.1:43412 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:03:00] 127.0.0.1:43418 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:32,  3.62s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:18,  2.28s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:17&amp;lt;00:49,  7.03s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:28&amp;lt;00:51,  8.54s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:34&amp;lt;00:37,  7.51s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:39&amp;lt;00:15,  5.05s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:18&amp;lt;00:28, 14.06s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:48&amp;lt;00:18, 18.47s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:40&amp;lt;00:00, 27.82s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:40&amp;lt;00:00, 16.01s/it]\n[2025-01-14 12:05:39] Shutting down\n[2025-01-14 12:05:39] Waiting for application shutdown.\n[2025-01-14 12:05:39] Application shutdown complete.\n[2025-01-14 12:05:39] Finished server process [2004337]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 47971\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 47971...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:47971...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0029175281524658 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0046677589416504 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0075552463531494 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.0096964836120605 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.011786222457886 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.0139172077178955 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:47971\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 287.7762758731842 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 1579\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 49941.274386801524\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 33439.44459196064\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 409.5392974850256\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.005969021003693342\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 17.32997173777711\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 160.06950513098855\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-4-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-4-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:42", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-4-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:42&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-14 12:05:42] Started server process [2004951]\n[2025-01-14 12:05:42] Waiting for application startup.\n[2025-01-14 12:05:46] Application startup complete.\n[2025-01-14 12:05:46] Uvicorn running on http://0.0.0.0:56451 (Press CTRL+C to quit)\n[2025-01-14 12:05:47] 127.0.0.1:56894 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 12:05:50] 127.0.0.1:39444 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 12:07:54] 127.0.0.1:55244 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:07:55] 127.0.0.1:55260 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:07:55] 127.0.0.1:55266 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:07:55] 127.0.0.1:55278 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:07:55] 127.0.0.1:55290 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:07:55] 127.0.0.1:55298 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:07:55] 127.0.0.1:55306 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:07:55] 127.0.0.1:55312 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:07:55] 127.0.0.1:55320 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:07:56] 127.0.0.1:55334 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:23,  2.60s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:19,  2.38s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:24&amp;lt;01:12, 10.41s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:29&amp;lt;00:48,  8.05s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:37&amp;lt;00:39,  7.97s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:39&amp;lt;00:24,  6.02s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:40&amp;lt;00:13,  4.36s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:40&amp;lt;00:06,  3.11s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [02:10&amp;lt;00:30, 30.16s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:26&amp;lt;00:00, 25.79s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:26&amp;lt;00:00, 14.63s/it]\n[2025-01-14 12:10:21] Shutting down\n[2025-01-14 12:10:21] Waiting for application shutdown.\n[2025-01-14 12:10:21] Application shutdown complete.\n[2025-01-14 12:10:21] Finished server process [2004951]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 56451\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 56451...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:56451...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.004835605621338 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.006913423538208 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.00905179977417 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.011133432388306 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.013251543045044 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.015332937240601 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:56451\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 274.06781125068665 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 1922\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 49013.01963039441\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 37556.54921848327\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 656.0166845156346\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.011036987416446209\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 18.960954403476713\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 146.30065243400168\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-8-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-8-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:05:01", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-8-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:05:01&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-14 12:10:24] Started server process [2005564]\n[2025-01-14 12:10:24] Waiting for application startup.\n[2025-01-14 12:10:28] Application startup complete.\n[2025-01-14 12:10:28] Uvicorn running on http://0.0.0.0:41239 (Press CTRL+C to quit)\n[2025-01-14 12:10:29] 127.0.0.1:58578 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 12:10:32] 127.0.0.1:58580 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 12:12:37] 127.0.0.1:33582 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:12:37] 127.0.0.1:33586 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:12:37] 127.0.0.1:33596 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:12:37] 127.0.0.1:33606 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:12:37] 127.0.0.1:33620 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:12:37] 127.0.0.1:33636 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:12:37] 127.0.0.1:33640 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:12:37] 127.0.0.1:33656 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:12:37] 127.0.0.1:33672 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:12:37] 127.0.0.1:33688 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:24,  2.75s/it]\r 20%|\u2588\u2588        | 2/10 [00:05&amp;lt;00:19,  2.46s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:23&amp;lt;01:09,  9.86s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:26&amp;lt;00:43,  7.27s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:30&amp;lt;00:30,  6.08s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:36&amp;lt;00:23,  5.89s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:13,  4.65s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:46&amp;lt;00:11,  5.70s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [02:05&amp;lt;00:28, 28.67s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:45&amp;lt;00:00, 32.03s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:45&amp;lt;00:00, 16.52s/it]\n[2025-01-14 12:15:22] Shutting down\n[2025-01-14 12:15:22] Waiting for application shutdown.\n[2025-01-14 12:15:22] Application shutdown complete.\n[2025-01-14 12:15:22] Finished server process [2005564]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 41239\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 41239...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:41239...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0014288425445557 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0026819705963135 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0039451122283936 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.0055835247039795 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.006832122802734 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.008074045181274 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:41239\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 293.14893674850464 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 388\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 49918.61493398901\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 33513.49279898568\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 811.351972486591\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.010315969120711088\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 16.78890160066371\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 165.22820050898008\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-16-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-16-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:03:40", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-16-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:03:40&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-14 12:15:25] Started server process [2006177]\n[2025-01-14 12:15:25] Waiting for application startup.\n[2025-01-14 12:15:29] Application startup complete.\n[2025-01-14 12:15:29] Uvicorn running on http://0.0.0.0:49891 (Press CTRL+C to quit)\n[2025-01-14 12:15:30] 127.0.0.1:44384 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 12:15:33] 127.0.0.1:44394 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 12:17:38] 127.0.0.1:34870 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:17:38] 127.0.0.1:34886 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:17:38] 127.0.0.1:34888 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:17:38] 127.0.0.1:34896 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:17:38] 127.0.0.1:34906 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:17:38] 127.0.0.1:34922 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:17:38] 127.0.0.1:34928 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:17:38] 127.0.0.1:34944 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:17:38] 127.0.0.1:34952 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:17:38] 127.0.0.1:34960 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:22,  2.47s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:15,  1.96s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:24&amp;lt;01:13, 10.44s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:31&amp;lt;00:54,  9.09s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:33&amp;lt;00:32,  6.42s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:38&amp;lt;00:24,  6.02s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:39&amp;lt;00:13,  4.50s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:02&amp;lt;00:20, 10.19s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:10&amp;lt;00:09,  9.51s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:24&amp;lt;00:00, 10.89s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:24&amp;lt;00:00,  8.43s/it]\n[2025-01-14 12:19:02] Shutting down\n[2025-01-14 12:19:02] Waiting for application shutdown.\n[2025-01-14 12:19:02] Application shutdown complete.\n[2025-01-14 12:19:02] Finished server process [2006177]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 49891\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 49891...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:49891...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0022039413452148 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.004465341567993 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0061192512512207 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.007766962051392 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.009393692016602 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.011070251464844 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:49891\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 212.0653998851776 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 1648\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 39013.12080679927\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 35842.07304901793\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 878.6902150022797\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.004411500412970781\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 32.90693635544277\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 84.29833668004721\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-32-meta-llama-3.1-8b-instruct.f16.gguf]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-32-meta-llama-3.1-8b-instruct.f16.gguf]", "duration": "00:04:55", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[pre_process_model1-write_config1-32-meta-llama-3.1-8b-instruct.f16.gguf]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:04:55&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-01-14 12:19:05] Started server process [2006791]\n[2025-01-14 12:19:05] Waiting for application startup.\n[2025-01-14 12:19:09] Application startup complete.\n[2025-01-14 12:19:09] Uvicorn running on http://0.0.0.0:35265 (Press CTRL+C to quit)\n[2025-01-14 12:19:10] 127.0.0.1:56966 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[2025-01-14 12:19:13] 127.0.0.1:56968 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-01-14 12:21:18] 127.0.0.1:42978 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:21:18] 127.0.0.1:42982 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:21:18] 127.0.0.1:42998 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:21:18] 127.0.0.1:43012 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:21:18] 127.0.0.1:43024 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:21:18] 127.0.0.1:43030 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:21:18] 127.0.0.1:43044 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:21:18] 127.0.0.1:43046 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:21:18] 127.0.0.1:43052 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-01-14 12:21:18] 127.0.0.1:43056 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:23,  2.59s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.08s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:24&amp;lt;01:12, 10.42s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:26&amp;lt;00:41,  6.88s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:30&amp;lt;00:29,  5.84s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:38&amp;lt;00:26,  6.56s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:40&amp;lt;00:15,  5.05s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:40&amp;lt;00:07,  3.72s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [02:09&amp;lt;00:30, 30.29s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:39&amp;lt;00:00, 30.05s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:39&amp;lt;00:00, 15.91s/it]\n[2025-01-14 12:23:57] Shutting down\n[2025-01-14 12:23:57] Waiting for application shutdown.\n[2025-01-14 12:23:57] Application shutdown complete.\n[2025-01-14 12:23:57] Finished server process [2006791]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:123 Finding available port...\nINFO     integration_tests.llm.utils:utils.py:128 Found available port: 35265\nINFO     integration_tests.llm.utils:utils.py:181 Starting LLM server on port 35265...\nINFO     integration_tests.llm.utils:utils.py:209 Process started... waiting for server\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:35265...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0029211044311523 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.0042378902435303 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0063869953155518 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.008572816848755 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.011253833770752 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.013369798660278 seconds; timeout: 30 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:109 Starting benchmark run with prefix sharing algorith trie...\n::group::Benchmark run with trie algorithm\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:113 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:114 Prefix sharing algorith: trie\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:115 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:35265\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-92/sglang_benchmark_test0\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:126 Benchmark run completed in 286.927880525589 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:127 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2014\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 49480.77397409943\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 34038.91755451332\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 892.9614115040749\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 0.00626896508038044\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 17.432749896417068\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 159.12578431301517\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:129 Benchmark run successful\n::endgroup::\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:51", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:51&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:18,  2.10s/it]\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:07,  1.02it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:04,  1.51it/s]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.43it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:04&amp;lt;00:04,  1.14it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:05,  1.41s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:07&amp;lt;00:03,  1.06s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:07&amp;lt;00:01,  1.27it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:08&amp;lt;00:00,  1.18it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.60it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.16it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test0/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test0/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 0 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 1.0024189949035645 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 2.004291534423828 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 3.0065791606903076 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 4.009193181991577 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 5.011677980422974 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 6.014360427856445 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 7.0162224769592285 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 8.018543481826782 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 9.021039724349976 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 10.023568630218506 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 11.025471210479736 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 12.028066873550415 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 13.030508279800415 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 14.033131122589111 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 15.035722732543945 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 16.0376079082489 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 17.04056143760681 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 18.0420982837677 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 19.044529914855957 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 20.04698610305786 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 21.049561738967896 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 22.05214834213257 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 23.055103540420532 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 24.057667016983032 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 25.060235738754272 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 26.06272530555725 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 27.065264463424683 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 28.067786931991577 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 29.070760011672974 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 30.073328495025635 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 31.07582116127014 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 32.078309535980225 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:142 Server has not started yet; waited 33.080894470214844 seconds; timeout: 600 seconds.\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test0\nRequest Rate: 1\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test0&amp;#x27;, request_rate=1, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test0/sglang_10_1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    1         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  8.62      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.16      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          227.27    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         321.66    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          548.93    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3334.26   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3620.02   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          31.60     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        27.46     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           51.97     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          11.76     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.20     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           13.63     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           11.95     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.20     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            24.11     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 16.62243151664734 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3334.264785418054\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 3620.0151719967835\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 27.457853488158435\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.201447494793683\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 321.6584978569655\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 8.624053206993267\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:15", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:15&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:10,  1.21s/it]\r 20%|\u2588\u2588        | 2/10 [00:01&amp;lt;00:04,  1.70it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:06,  1.01it/s]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.17it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.57it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:04,  1.17s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:06&amp;lt;00:02,  1.21it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.57it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.62it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.34it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test1/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test1/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test1\nRequest Rate: 2\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test1&amp;#x27;, request_rate=2, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test1/sglang_10_2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    2         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  7.48      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.34      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          261.88    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         370.63    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          632.51    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3493.30   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3894.45   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          21.81     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        22.88     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           26.66     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.64     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.64     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           14.54     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.56     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.23     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            22.99     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 14.425143241882324 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3493.29999529873\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 3894.4543679826893\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 22.88416647934355\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.233505509560928\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 370.6334523786899\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 7.484483610955067\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:06,  1.33it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:07,  1.07s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.29it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:02,  1.69it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:04,  1.00s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.77it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.64it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.44it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test2/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test2/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test2\nRequest Rate: 4\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test2&amp;#x27;, request_rate=4, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test2/sglang_10_4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    4         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.93      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.44      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          282.77    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         400.20    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          682.97    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3584.08   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4035.74   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          19.57     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        19.73     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           24.20     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.24     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.35     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           14.64     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.90     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.30     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            22.87     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 13.909915447235107 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3584.078363707522\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 4035.7357170141768\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 19.727508013602346\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.301140017574653\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 400.2010016022202\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 6.931516885000747\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  2.17it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.47it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:09,  1.29s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.21it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.53it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:03,  1.07it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.94it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.69it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.51it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test3/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test3/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test3\nRequest Rate: 8\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test3&amp;#x27;, request_rate=8, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test3/sglang_10_8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    8         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.60      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.51      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          296.79    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         420.05    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          716.85    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3605.56   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4088.39   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          21.01     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        20.33     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           31.51     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.53     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.32     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           15.74     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.97     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.26     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            22.85     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 13.383508205413818 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3605.5559302854817\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 4088.3860279864166\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 20.32909548142925\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.264391483971849\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 420.0545934679581\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 6.603903500013985\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.24it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:01,  4.01it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:08,  1.27s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.14it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.50it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.14it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.13it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.02it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.65it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.56it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test4/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test4/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test4\nRequest Rate: 16\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test4&amp;#x27;, request_rate=16, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test4/sglang_10_16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    16        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.43      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.56      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          304.91    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         431.54    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          736.45    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3606.80   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4099.99   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          19.40     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        19.15     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           24.94     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.67     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.35     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           16.71     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.98     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.27     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            16.16     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 13.31301999092102 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3606.8020261940546\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 4099.987190013053\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 19.14928198675625\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.272828509798273\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 431.54200414005453\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 6.428111222980078\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:14", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:14&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  4.41it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:01,  4.48it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:08,  1.27s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.10it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.48it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.17it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.17it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.07it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.65it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.57it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     integration_tests.llm.utils:utils.py:68 Preparing tokenizer_path: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test5/tokenizer.json...\nINFO     integration_tests.llm.utils:utils.py:70 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     integration_tests.llm.utils:utils.py:75 Tokenizer saved to /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test5/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Beginning SGLang benchmark test...\nINFO     integration_tests.llm.utils:utils.py:133 Waiting for server to start at http://localhost:30000...\nINFO     integration_tests.llm.utils:utils.py:139 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:55 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /tmp/pytest-of-nod/pytest-91/sglang_benchmark_test5\nRequest Rate: 32\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test5&amp;#x27;, request_rate=32, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/tmp/pytest-of-nod/pytest-91/sglang_benchmark_test5/sglang_10_32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    32        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.37      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.57      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          307.68    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         435.46    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          743.14    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3618.35   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4121.12   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          21.13     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        20.46     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           26.14     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.47     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.43     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           15.06     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           13.01     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.27     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            16.14     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:63 Benchmark run completed in 13.232262372970581 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:64 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:71 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:71 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:71 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:71 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:71 MEAN_E2E_LATENCY_MS: 3618.347769905813\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_E2E_LATENCY_MS: 4121.118729992304\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_TTFT_MS: 20.46272298321128\nINFO     sglang_benchmarks.utils:utils.py:71 MEDIAN_ITL_MS: 13.26768597937189\nINFO     sglang_benchmarks.utils:utils.py:71 OUTPUT_THROUGHPUT: 435.4603178899272\nINFO     sglang_benchmarks.utils:utils.py:71 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:71 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:71 DURATION: 6.370270461018663\nINFO     sglang_benchmarks.utils:utils.py:71 COMPLETED: 10\n\n"}]}, "renderCollapsed": ["passed"], "initialSort": "result", "title": "shortfin_index.html"}' id="data-container"></div>
<script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
</footer>
</html>