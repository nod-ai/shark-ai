<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<title id="head-title">index.html</title>
<style type="text/css">body {
  font-family: Helvetica, Arial, sans-serif;
  font-size: 12px;
  /* do not increase min-width as some may use split screens */
  min-width: 800px;
  color: #999;
}

h1 {
  font-size: 24px;
  color: black;
}

h2 {
  font-size: 16px;
  color: black;
}

p {
  color: black;
}

a {
  color: #999;
}

table {
  border-collapse: collapse;
}

/******************************
 * SUMMARY INFORMATION
 ******************************/
#environment td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  vertical-align: top;
}
#environment tr:nth-child(odd) {
  background-color: #f6f6f6;
}
#environment ul {
  margin: 0;
  padding: 0 20px;
}

/******************************
 * TEST RESULT COLORS
 ******************************/
span.passed,
.passed .col-result {
  color: green;
}

span.skipped,
span.xfailed,
span.rerun,
.skipped .col-result,
.xfailed .col-result,
.rerun .col-result {
  color: orange;
}

span.error,
span.failed,
span.xpassed,
.error .col-result,
.failed .col-result,
.xpassed .col-result {
  color: red;
}

.col-links__extra {
  margin-right: 3px;
}

/******************************
 * RESULTS TABLE
 *
 * 1. Table Layout
 * 2. Extra
 * 3. Sorting items
 *
 ******************************/
/*------------------
 * 1. Table Layout
 *------------------*/
#results-table {
  border: 1px solid #e6e6e6;
  color: #999;
  font-size: 12px;
  width: 100%;
}
#results-table th,
#results-table td {
  padding: 5px;
  border: 1px solid #e6e6e6;
  text-align: left;
}
#results-table th {
  font-weight: bold;
}

/*------------------
 * 2. Extra
 *------------------*/
.logwrapper {
  max-height: 230px;
  overflow-y: scroll;
  background-color: #e6e6e6;
}
.logwrapper.expanded {
  max-height: none;
}
.logwrapper.expanded .logexpander:after {
  content: "collapse [-]";
}
.logwrapper .logexpander {
  z-index: 1;
  position: sticky;
  top: 10px;
  width: max-content;
  border: 1px solid;
  border-radius: 3px;
  padding: 5px 7px;
  margin: 10px 0 10px calc(100% - 80px);
  cursor: pointer;
  background-color: #e6e6e6;
}
.logwrapper .logexpander:after {
  content: "expand [+]";
}
.logwrapper .logexpander:hover {
  color: #000;
  border-color: #000;
}
.logwrapper .log {
  min-height: 40px;
  position: relative;
  top: -50px;
  height: calc(100% + 50px);
  border: 1px solid #e6e6e6;
  color: black;
  display: block;
  font-family: "Courier New", Courier, monospace;
  padding: 5px;
  padding-right: 80px;
  white-space: pre-wrap;
}

div.media {
  border: 1px solid #e6e6e6;
  float: right;
  height: 240px;
  margin: 0 5px;
  overflow: hidden;
  width: 320px;
}

.media-container {
  display: grid;
  grid-template-columns: 25px auto 25px;
  align-items: center;
  flex: 1 1;
  overflow: hidden;
  height: 200px;
}

.media-container--fullscreen {
  grid-template-columns: 0px auto 0px;
}

.media-container__nav--right,
.media-container__nav--left {
  text-align: center;
  cursor: pointer;
}

.media-container__viewport {
  cursor: pointer;
  text-align: center;
  height: inherit;
}
.media-container__viewport img,
.media-container__viewport video {
  object-fit: cover;
  width: 100%;
  max-height: 100%;
}

.media__name,
.media__counter {
  display: flex;
  flex-direction: row;
  justify-content: space-around;
  flex: 0 0 25px;
  align-items: center;
}

.collapsible td:not(.col-links) {
  cursor: pointer;
}
.collapsible td:not(.col-links):hover::after {
  color: #bbb;
  font-style: italic;
  cursor: pointer;
}

.col-result {
  width: 130px;
}
.col-result:hover::after {
  content: " (hide details)";
}

.col-result.collapsed:hover::after {
  content: " (show details)";
}

#environment-header h2:hover::after {
  content: " (hide details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

#environment-header.collapsed h2:hover::after {
  content: " (show details)";
  color: #bbb;
  font-style: italic;
  cursor: pointer;
  font-size: 12px;
}

/*------------------
 * 3. Sorting items
 *------------------*/
.sortable {
  cursor: pointer;
}
.sortable.desc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: -12.5px;
  border: 10px solid #4caf50;
  border-bottom: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}
.sortable.asc:after {
  content: " ";
  position: relative;
  left: 5px;
  bottom: 12.5px;
  border: 10px solid #4caf50;
  border-top: 0;
  border-left-color: transparent;
  border-right-color: transparent;
}

.hidden, .summary__reload__button.hidden {
  display: none;
}

.summary__data {
  flex: 0 0 550px;
}
.summary__reload {
  flex: 1 1;
  display: flex;
  justify-content: center;
}
.summary__reload__button {
  flex: 0 0 300px;
  display: flex;
  color: white;
  font-weight: bold;
  background-color: #4caf50;
  text-align: center;
  justify-content: center;
  align-items: center;
  border-radius: 3px;
  cursor: pointer;
}
.summary__reload__button:hover {
  background-color: #46a049;
}
.summary__spacer {
  flex: 0 0 550px;
}

.controls {
  display: flex;
  justify-content: space-between;
}

.filters,
.collapse {
  display: flex;
  align-items: center;
}
.filters button,
.collapse button {
  color: #999;
  border: none;
  background: none;
  cursor: pointer;
  text-decoration: underline;
}
.filters button:hover,
.collapse button:hover {
  color: #ccc;
}

.filter__label {
  margin-right: 10px;
}

      </style>
</head>
<body>
<h1 id="title">index.html</h1>
<p>Report generated on 10-Feb-2025 at 11:42:01 by <a href="https://pypi.python.org/pypi/pytest-html">pytest-html</a>
        v4.1.1</p>
<div id="environment-header">
<h2>Environment</h2>
</div>
<table id="environment"></table>
<!-- TEMPLATES -->
<template id="template_environment_row">
<tr>
<td></td>
<td></td>
</tr>
</template>
<template id="template_results-table__body--empty">
<tbody class="results-table-row">
<tr id="not-found-message">
<td colspan="4">No results found. Check the filters.
</td></tr>
</tbody></template>
<template id="template_results-table__tbody">
<tbody class="results-table-row">
<tr class="collapsible">
</tr>
<tr class="extras-row">
<td class="extra" colspan="4">
<div class="extraHTML"></div>
<div class="media">
<div class="media-container">
<div class="media-container__nav--left">&lt;</div>
<div class="media-container__viewport">
<img src=""/>
<video controls="">
<source src="" type="video/mp4"/>
</video>
</div>
<div class="media-container__nav--right">&gt;</div>
</div>
<div class="media__name"></div>
<div class="media__counter"></div>
</div>
<div class="logwrapper">
<div class="logexpander"></div>
<div class="log"></div>
</div>
</td>
</tr>
</tbody>
</template>
<!-- END TEMPLATES -->
<div class="summary">
<div class="summary__data">
<h2>Summary</h2>
<div class="additional-summary prefix">
</div>
<p class="run-count">18 tests ran in 2419 seconds</p>
<p class="filter">(Un)check the boxes to filter the results.</p>
<div class="summary__reload">
<div class="summary__reload__button hidden" onclick="location.reload()">
<div>There are still tests running. <br/>Reload this page to get the latest results!</div>
</div>
</div>
<div class="summary__spacer"></div>
<div class="controls">
<div class="filters">
<input checked="true" class="filter" data-test-result="failed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="failed">0 Failed,</span>
<input checked="true" class="filter" data-test-result="passed" name="filter_checkbox" type="checkbox"/>
<span class="passed">18 Passed,</span>
<input checked="true" class="filter" data-test-result="skipped" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="skipped">0 Skipped,</span>
<input checked="true" class="filter" data-test-result="xfailed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xfailed">0 Expected failures,</span>
<input checked="true" class="filter" data-test-result="xpassed" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="xpassed">0 Unexpected passes,</span>
<input checked="true" class="filter" data-test-result="error" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="error">0 Errors,</span>
<input checked="true" class="filter" data-test-result="rerun" disabled="" name="filter_checkbox" type="checkbox"/>
<span class="rerun">0 Reruns</span>
</div>
<div class="collapse">
<button id="show_all_details">Show all details</button> / <button id="hide_all_details">Hide all details</button>
</div>
</div>
</div>
<div class="additional-summary summary">
</div>
<div class="additional-summary postfix">
</div>
</div>
<table id="results-table">
<thead id="results-table-head">
<tr>
<th class="sortable" data-column-type="result">Result</th>
<th class="sortable" data-column-type="testId">Test</th>
<th class="sortable" data-column-type="duration">Duration</th>
<th>Links</th>
</tr>
</thead>
</table>
</body>
<footer>
<div data-jsonblob='{"environment": {"Python": "3.11.11", "Platform": "Linux-5.15.0-131-generic-x86_64-with-glibc2.35", "Packages": {"pytest": "8.0.0", "pluggy": "1.5.0"}, "Plugins": {"metadata": "3.1.1", "timeout": "2.3.1", "html": "4.1.1", "anyio": "4.8.0", "xdist": "3.5.0"}, "CI": "true"}, "tests": {"reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]", "duration": "00:11:32", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-1]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:11:32&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stdout setup -----------------------------\nExporting prefill_bs1\nExporting decode_bs1\nExporting prefill_bs4\nExporting decode_bs4\nGENERATED!\nExporting\nSaving to &amp;#x27;/shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir&amp;#x27;\n\n---------------------------- Captured stderr setup -----------------------------\n/shark-dev/actions-runner-shark-ai-3/_work/shark-ai/shark-ai/sharktank/sharktank/types/gguf_interop/base.py:100: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n  data_tensor = torch.as_tensor(data.reshape(logical_shape))\n[2025-02-10 11:12:06] Started server process [407968]\n[2025-02-10 11:12:06] Waiting for application startup.\n[2025-02-10 11:12:07] Application startup complete.\n[2025-02-10 11:12:07] Uvicorn running on http://0.0.0.0:43499 (Press CTRL+C to quit)\n[2025-02-10 11:12:08] 127.0.0.1:52448 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\n\n------------------------------ Captured log setup ------------------------------\nINFO     integration_tests.llm.model_management:model_management.py:134 Downloading model SanctumAI/Meta-Llama-3.1-8B-Instruct-GGUF from HuggingFace\nINFO     integration_tests.llm.model_management:model_management.py:201 Downloading tokenizer NousResearch/Meta-Llama-3.1-8B using transformers\nINFO     integration_tests.llm.model_management:model_management.py:217 Exporting model with following settings:\n  MLIR Path: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\n  Config Path: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/config.json\n  Batch Sizes: 1,4\nINFO     integration_tests.llm.model_management:model_management.py:238 Model successfully exported to /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.mlir\nINFO     integration_tests.llm.model_management:model_management.py:244 Compiling model to /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\nINFO     integration_tests.llm.model_management:model_management.py:255 Model successfully compiled to /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF/model.vmfb\n\n----------------------------- Captured stderr call -----------------------------\n\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   0%|          | 0.00/642M [00:00&amp;lt;?, ?B/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   2%|\u258f         | 10.0M/642M [00:00&amp;lt;00:06, 105MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   3%|\u258e         | 21.4M/642M [00:00&amp;lt;00:05, 114MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   5%|\u258c         | 32.6M/642M [00:00&amp;lt;00:05, 115MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   7%|\u258b         | 43.8M/642M [00:00&amp;lt;00:05, 116MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:   9%|\u258a         | 55.0M/642M [00:00&amp;lt;00:05, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  10%|\u2588         | 66.2M/642M [00:00&amp;lt;00:05, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  12%|\u2588\u258f        | 77.4M/642M [00:00&amp;lt;00:05, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  14%|\u2588\u258d        | 88.6M/642M [00:00&amp;lt;00:04, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  16%|\u2588\u258c        | 99.8M/642M [00:00&amp;lt;00:04, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  17%|\u2588\u258b        | 111M/642M [00:01&amp;lt;00:04, 117MB/s] \r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  19%|\u2588\u2589        | 122M/642M [00:01&amp;lt;00:04, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  21%|\u2588\u2588        | 133M/642M [00:01&amp;lt;00:04, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  23%|\u2588\u2588\u258e       | 145M/642M [00:01&amp;lt;00:04, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  24%|\u2588\u2588\u258d       | 156M/642M [00:01&amp;lt;00:04, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  26%|\u2588\u2588\u258c       | 167M/642M [00:01&amp;lt;00:04, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  28%|\u2588\u2588\u258a       | 178M/642M [00:01&amp;lt;00:04, 116MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  30%|\u2588\u2588\u2589       | 190M/642M [00:01&amp;lt;00:04, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  31%|\u2588\u2588\u2588\u258f      | 201M/642M [00:01&amp;lt;00:03, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  33%|\u2588\u2588\u2588\u258e      | 212M/642M [00:01&amp;lt;00:03, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  35%|\u2588\u2588\u2588\u258d      | 224M/642M [00:02&amp;lt;00:03, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  37%|\u2588\u2588\u2588\u258b      | 235M/642M [00:02&amp;lt;00:03, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  38%|\u2588\u2588\u2588\u258a      | 246M/642M [00:02&amp;lt;00:03, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  40%|\u2588\u2588\u2588\u2588      | 257M/642M [00:02&amp;lt;00:03, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  42%|\u2588\u2588\u2588\u2588\u258f     | 268M/642M [00:02&amp;lt;00:03, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  44%|\u2588\u2588\u2588\u2588\u258e     | 280M/642M [00:02&amp;lt;00:03, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  45%|\u2588\u2588\u2588\u2588\u258c     | 291M/642M [00:02&amp;lt;00:03, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  47%|\u2588\u2588\u2588\u2588\u258b     | 302M/642M [00:02&amp;lt;00:03, 116MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  49%|\u2588\u2588\u2588\u2588\u2589     | 314M/642M [00:02&amp;lt;00:02, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  51%|\u2588\u2588\u2588\u2588\u2588     | 325M/642M [00:02&amp;lt;00:02, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 336M/642M [00:03&amp;lt;00:02, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 347M/642M [00:03&amp;lt;00:02, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 359M/642M [00:03&amp;lt;00:02, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 370M/642M [00:03&amp;lt;00:02, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 381M/642M [00:03&amp;lt;00:02, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 392M/642M [00:03&amp;lt;00:02, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 404M/642M [00:03&amp;lt;00:02, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 415M/642M [00:03&amp;lt;00:02, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 426M/642M [00:03&amp;lt;00:01, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 437M/642M [00:03&amp;lt;00:01, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 449M/642M [00:04&amp;lt;00:01, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 460M/642M [00:04&amp;lt;00:01, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 471M/642M [00:04&amp;lt;00:01, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 482M/642M [00:04&amp;lt;00:01, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 493M/642M [00:04&amp;lt;00:01, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 505M/642M [00:04&amp;lt;00:01, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 516M/642M [00:04&amp;lt;00:01, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 527M/642M [00:04&amp;lt;00:01, 116MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 538M/642M [00:04&amp;lt;00:00, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 550M/642M [00:04&amp;lt;00:00, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 561M/642M [00:05&amp;lt;00:00, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 572M/642M [00:05&amp;lt;00:00, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 584M/642M [00:05&amp;lt;00:00, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 595M/642M [00:05&amp;lt;00:00, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 606M/642M [00:05&amp;lt;00:00, 118MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 617M/642M [00:05&amp;lt;00:00, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 628M/642M [00:05&amp;lt;00:00, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 640M/642M [00:05&amp;lt;00:00, 117MB/s]\r/tmp/ShareGPT_V3_unfiltered_cleaned_split.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 642M/642M [00:05&amp;lt;00:00, 117MB/s]\n[2025-02-10 11:12:18] 127.0.0.1:59744 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:12:35] 127.0.0.1:39830 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:12:35] 127.0.0.1:39832 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:12:36] 127.0.0.1:39842 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:12:36] 127.0.0.1:39854 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:12:37] 127.0.0.1:39866 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:12:37] 127.0.0.1:39868 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:12:37] 127.0.0.1:39878 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:12:37] 127.0.0.1:39894 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:12:38] 127.0.0.1:39902 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:32,  3.59s/it][2025-02-10 11:12:38] 127.0.0.1:39904 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 20%|\u2588\u2588        | 2/10 [00:06&amp;lt;00:23,  2.96s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:26&amp;lt;01:16, 10.91s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:28&amp;lt;00:43,  7.30s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:31&amp;lt;00:28,  5.68s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:37&amp;lt;00:24,  6.02s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:39&amp;lt;00:13,  4.57s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:58&amp;lt;00:18,  9.13s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:06&amp;lt;00:08,  8.94s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:24&amp;lt;00:00, 30.30s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:24&amp;lt;00:00, 14.49s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-1...\n::group::Benchmark run on llama31_8b_none-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:43499\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 171.08251428604126 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 144.8628759730018\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 233\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.06903079849017844\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 13.530036504074976\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 19.1491435011755\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 42197.39914170059\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 32806.234404999486\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 38164.26964363577\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 135507.32667761153\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 200.01107870084525\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 134.94330800131138\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 116.25779787858431\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 363.2062171817597\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 141.06890807216826\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 121.83890254189345\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 49.07752794749109\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 262.441016283771\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 151.37954903028373\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 110.49565250141313\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 125.63609778814833\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 501.7865458007873\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.912920156960362\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]", "duration": "00:02:58", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-2]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:58&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:15:03] 127.0.0.1:33256 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:15:20] 127.0.0.1:57094 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:15:20] 127.0.0.1:57096 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:15:21] 127.0.0.1:57098 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:15:21] 127.0.0.1:57108 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:15:21] 127.0.0.1:57112 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:15:21] 127.0.0.1:57124 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:15:21] 127.0.0.1:57132 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:15:21] 127.0.0.1:57134 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:15:21] 127.0.0.1:57138 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:15:21] 127.0.0.1:57140 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:28,  3.19s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.07s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:22&amp;lt;01:04,  9.25s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:32&amp;lt;00:58,  9.81s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:33&amp;lt;00:31,  6.40s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:36&amp;lt;00:21,  5.38s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:12,  4.07s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:49&amp;lt;00:13,  6.56s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:24&amp;lt;00:15, 15.15s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:37&amp;lt;00:00, 33.20s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:37&amp;lt;00:00, 15.76s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-2...\n::group::Benchmark run on llama31_8b_none-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:43499\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 177.8481466770172 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 157.6051253250007\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 223\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.06344971319542304\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 12.436143786302917\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 17.600950440410355\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 45214.72489090011\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 34087.708540000676\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 42936.949408951936\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 149793.62730393908\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 296.16522239957703\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 308.39101600031427\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 125.38546987868409\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 474.09164148913993\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 153.52396122587538\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 132.11113979606705\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 64.45956261691435\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 290.78641194529894\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 161.90072994700523\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 119.81133799963573\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 133.86744359326104\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 502.1856260482309\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.8688613265375675\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]", "duration": "00:02:17", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-4]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:17&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:18:01] 127.0.0.1:43102 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:18:18] 127.0.0.1:43030 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:18:18] 127.0.0.1:43034 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:18:18] 127.0.0.1:43048 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:18:18] 127.0.0.1:43050 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:18:18] 127.0.0.1:43056 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:18:18] 127.0.0.1:43066 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:18:18] 127.0.0.1:43072 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:18:18] 127.0.0.1:43082 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:18:18] 127.0.0.1:43098 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:18:19] 127.0.0.1:43112 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:18,  2.03s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.12s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:19&amp;lt;00:57,  8.21s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:24&amp;lt;00:41,  6.88s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:30&amp;lt;00:33,  6.72s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:33&amp;lt;00:21,  5.37s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:39&amp;lt;00:16,  5.54s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:54&amp;lt;00:17,  8.68s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:55&amp;lt;00:06,  6.12s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:57&amp;lt;00:00, 23.24s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:57&amp;lt;00:00, 11.71s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-4...\n::group::Benchmark run on llama31_8b_none-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:43499\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 137.30664730072021 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 117.05253081400224\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 239\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.08543172822029889\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 16.74461873117858\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 23.69876140831091\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 37702.99740600021\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 31986.49225650115\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 31353.215841090212\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 110906.52113081884\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 386.74171959901287\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 386.5496489997895\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 156.50611814701503\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 565.0970869878802\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 127.50462672526544\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 127.65605844468095\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 39.20318104867158\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 214.55727067714608\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 134.49413265825584\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 108.42956250053248\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 111.77819538264579\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 501.35013972081657\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.2210322274800443\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]", "duration": "00:02:03", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-8]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:03&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:20:18] 127.0.0.1:46214 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:20:35] 127.0.0.1:50910 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:20:35] 127.0.0.1:50924 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:20:35] 127.0.0.1:50938 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:20:35] 127.0.0.1:50950 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:20:35] 127.0.0.1:50958 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:20:35] 127.0.0.1:50964 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:20:35] 127.0.0.1:50972 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:20:35] 127.0.0.1:50978 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:20:35] 127.0.0.1:50992 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:20:35] 127.0.0.1:50998 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:22,  2.48s/it]\r 20%|\u2588\u2588        | 2/10 [00:03&amp;lt;00:14,  1.87s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:25&amp;lt;01:14, 10.70s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:28&amp;lt;00:47,  7.89s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:31&amp;lt;00:30,  6.20s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:35&amp;lt;00:21,  5.27s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:40&amp;lt;00:15,  5.18s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:41&amp;lt;00:08,  4.01s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:03&amp;lt;00:09,  9.49s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:42&amp;lt;00:00, 18.61s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:42&amp;lt;00:00, 10.24s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-8...\n::group::Benchmark run on llama31_8b_none-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:43499\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 122.71128916740417 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 102.43887983099921\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 228\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.09761918537666284\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 19.133360333825916\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 27.07956202348627\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 37298.92542639973\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 33413.73711199958\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 27417.11293939165\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 98614.82097887143\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 653.1623367001885\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 718.8152284998068\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 230.31421183817045\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 907.1484597089147\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 129.79091850319108\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 117.45819576871789\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 28.4723604155699\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 190.56476277615508\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 132.08307260454345\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 107.95740299909085\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 100.77922784825574\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 501.2575822000872\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.641090715550038\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]", "duration": "00:02:05", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-16]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:05&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:22:21] 127.0.0.1:56174 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:22:38] 127.0.0.1:37904 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:22:38] 127.0.0.1:37914 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:22:38] 127.0.0.1:37918 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:22:38] 127.0.0.1:37924 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:22:38] 127.0.0.1:37934 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:22:38] 127.0.0.1:37950 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:22:38] 127.0.0.1:37966 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:22:38] 127.0.0.1:37970 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:22:38] 127.0.0.1:37980 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:22:38] 127.0.0.1:37984 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:24,  2.76s/it]\r 20%|\u2588\u2588        | 2/10 [00:03&amp;lt;00:14,  1.86s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:24&amp;lt;01:12, 10.29s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:27&amp;lt;00:45,  7.54s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:31&amp;lt;00:30,  6.20s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:38&amp;lt;00:25,  6.33s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:12,  4.31s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:58&amp;lt;00:18,  9.29s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:02&amp;lt;00:07,  7.78s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:45&amp;lt;00:00, 18.52s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:45&amp;lt;00:00, 10.52s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-16...\n::group::Benchmark run on llama31_8b_none-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:43499\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 125.4012598991394 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 105.1589111860012\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 230\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.09509417592116724\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 18.638458480548778\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 26.379124400531794\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 39091.0596624999\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 34637.16503700198\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 28700.637256022885\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 101181.17283358028\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 679.3756770999607\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 744.7835499988287\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 224.99670016159925\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 849.4751427618758\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 142.07126283971374\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 120.3990654017969\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 43.949461790209455\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 227.2229492958865\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 138.44548077613922\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 109.99688300034904\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 110.50637872247567\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 501.32946506910224\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.7173321044906102\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]", "duration": "00:02:31", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_none-32]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:31&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:24:26] 127.0.0.1:57296 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:24:43] 127.0.0.1:34186 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:24:43] 127.0.0.1:34194 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:24:43] 127.0.0.1:40214 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:24:43] 127.0.0.1:40224 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:24:43] 127.0.0.1:40232 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:24:43] 127.0.0.1:40248 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:24:43] 127.0.0.1:40262 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:24:43] 127.0.0.1:40264 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:24:43] 127.0.0.1:40270 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:24:43] 127.0.0.1:40280 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:22,  2.46s/it]\r 20%|\u2588\u2588        | 2/10 [00:03&amp;lt;00:14,  1.81s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:24&amp;lt;01:12, 10.32s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:28&amp;lt;00:46,  7.79s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:29&amp;lt;00:26,  5.39s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:34&amp;lt;00:21,  5.45s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:37&amp;lt;00:13,  4.39s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:03&amp;lt;00:23, 11.53s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:34&amp;lt;00:17, 17.48s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:10&amp;lt;00:00, 23.36s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:10&amp;lt;00:00, 13.10s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_none-32...\n::group::Benchmark run on llama31_8b_none-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_none-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:43499\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 151.24353289604187 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 130.976351917001\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 236\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.07634966048174059\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 14.964533454421154\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 21.179395817634838\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 44857.10398670053\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 32049.84508150119\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 38497.90488688732\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 127608.40792935011\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 693.554676300846\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 757.7590105011041\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 226.0354519488959\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 848.3483805618744\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 148.56893967003018\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 117.26957835364195\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 52.484537835622646\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 245.25180353424972\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 159.19176773071467\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 112.2205940009735\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 133.56308944463436\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 502.57925543046446\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.4248246595787175\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]", "duration": "00:03:00", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-1]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:03:00&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "---------------------------- Captured stderr setup -----------------------------\n[2025-02-10 11:26:54] Shutting down\n[2025-02-10 11:26:54] Waiting for application shutdown.\n[2025-02-10 11:26:54] Application shutdown complete.\n[2025-02-10 11:26:54] Finished server process [407968]\n[2025-02-10 11:26:56] Started server process [410025]\n[2025-02-10 11:26:56] Waiting for application startup.\n[2025-02-10 11:26:58] Application startup complete.\n[2025-02-10 11:26:58] Uvicorn running on http://0.0.0.0:55783 (Press CTRL+C to quit)\n[2025-02-10 11:26:59] 127.0.0.1:34724 - &amp;quot;GET /health HTTP/1.1&amp;quot; 200\n\n----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:27:03] 127.0.0.1:34736 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:27:20] 127.0.0.1:34306 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:27:20] 127.0.0.1:34310 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:27:21] 127.0.0.1:34316 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:27:21] 127.0.0.1:34330 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:27:22] 127.0.0.1:34336 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:27:22] 127.0.0.1:34338 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:27:22] 127.0.0.1:34354 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:27:22] 127.0.0.1:34368 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:27:23] 127.0.0.1:34372 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:27:23] 127.0.0.1:46678 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:03&amp;lt;00:33,  3.69s/it]\r 20%|\u2588\u2588        | 2/10 [00:05&amp;lt;00:22,  2.87s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:24&amp;lt;01:11, 10.21s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:27&amp;lt;00:43,  7.20s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:28&amp;lt;00:25,  5.14s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:37&amp;lt;00:25,  6.31s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:13,  4.39s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:06&amp;lt;00:24, 12.19s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:10&amp;lt;00:09,  9.47s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:34&amp;lt;00:00, 32.52s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:34&amp;lt;00:00, 15.45s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-1...\n::group::Benchmark run on llama31_8b_trie-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:55783\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 1\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 174.77021145820618 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 154.54042700400169\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 231\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.06470798737822213\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 12.682765526131538\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 17.94999569871882\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 43817.25025199994\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 31970.374476499273\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 41424.6354955997\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 144520.30721890987\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 215.552932999708\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 211.9864094984223\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 113.86749766901124\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 364.52805937853554\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 144.75939734142113\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 130.94980091917202\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 55.252828941669264\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 279.25412104914096\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 157.17594910923012\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 108.94464400007564\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 134.78717719709812\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 502.15247643853223\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.8353260762548125\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]", "duration": "00:02:47", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-2]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:47&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:29:57] 127.0.0.1:55314 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:30:14] 127.0.0.1:48334 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:30:15] 127.0.0.1:48350 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:30:15] 127.0.0.1:48354 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:30:15] 127.0.0.1:48368 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:30:15] 127.0.0.1:48378 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:30:15] 127.0.0.1:48394 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:30:16] 127.0.0.1:48400 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:30:16] 127.0.0.1:48414 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:30:16] 127.0.0.1:48428 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:30:16] 127.0.0.1:48442 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:23,  2.65s/it]\r 20%|\u2588\u2588        | 2/10 [00:05&amp;lt;00:21,  2.74s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:24&amp;lt;01:11, 10.27s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:26&amp;lt;00:41,  6.85s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:28&amp;lt;00:26,  5.24s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:36&amp;lt;00:24,  6.04s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:38&amp;lt;00:13,  4.66s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:39&amp;lt;00:07,  3.77s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:37&amp;lt;00:20, 20.44s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:26&amp;lt;00:00, 29.43s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:26&amp;lt;00:00, 14.66s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-2...\n::group::Benchmark run on llama31_8b_trie-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:55783\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 2\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 166.82963967323303 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 146.58622022500276\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 227\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.06821923632828845\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 13.370970320344536\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 18.924016157467218\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 43539.84470990035\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 31561.960586001078\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 41723.59249305127\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 140897.72606712944\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 296.01869589969283\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 291.9157024989545\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 135.03944072781593\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 500.3243793513684\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 143.27323590241696\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 109.02159176767643\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 57.86504235729903\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 271.26418351731894\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 155.87682102956015\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 110.13698100032343\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 131.61227690875177\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 502.096202200737\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 2.970254955961672\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]", "duration": "00:01:59", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-4]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:01:59&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:32:44] 127.0.0.1:50038 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:33:01] 127.0.0.1:52558 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:33:01] 127.0.0.1:52564 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:33:01] 127.0.0.1:52580 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:33:01] 127.0.0.1:52584 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:33:01] 127.0.0.1:52590 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:33:02] 127.0.0.1:52598 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:33:02] 127.0.0.1:52608 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:33:02] 127.0.0.1:52610 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:33:02] 127.0.0.1:52622 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:33:02] 127.0.0.1:52638 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:20,  2.25s/it]\r 20%|\u2588\u2588        | 2/10 [00:05&amp;lt;00:20,  2.55s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:23&amp;lt;01:10, 10.05s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:25&amp;lt;00:40,  6.79s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:28&amp;lt;00:26,  5.35s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:35&amp;lt;00:23,  6.00s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:41&amp;lt;00:17,  5.75s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:43&amp;lt;00:09,  4.85s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:45&amp;lt;00:03,  3.80s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:38&amp;lt;00:00, 19.01s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:38&amp;lt;00:00,  9.86s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-4...\n::group::Benchmark run on llama31_8b_trie-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:55783\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 4\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 118.58611011505127 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 98.56805301799977\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 240\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.10145274958585083\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 19.884738918826763\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 28.142992735115023\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 34536.39009340004\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 31740.229954499227\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 25413.115737891065\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 93174.45862114851\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 385.45133890038414\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 397.6874290001433\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 153.83808844546988\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 538.9987127515269\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 123.7993920525016\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 110.32744214791632\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 26.93752647343454\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 183.3370479692278\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 123.10162072819193\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 104.33125049894443\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 89.24388071206718\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 501.22727898007724\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.503811735744974\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]", "duration": "00:02:26", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-8]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:26&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:34:43] 127.0.0.1:48770 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:35:00] 127.0.0.1:53932 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:35:00] 127.0.0.1:53944 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:35:00] 127.0.0.1:53948 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:35:00] 127.0.0.1:53950 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:35:00] 127.0.0.1:53952 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:35:00] 127.0.0.1:53958 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:35:00] 127.0.0.1:53966 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:35:00] 127.0.0.1:53968 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:35:00] 127.0.0.1:53976 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:35:00] 127.0.0.1:53990 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:17,  1.98s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:16,  2.05s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:27&amp;lt;01:21, 11.62s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:33&amp;lt;00:58,  9.67s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:36&amp;lt;00:20,  5.20s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:39&amp;lt;00:14,  4.71s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:43&amp;lt;00:08,  4.50s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:18&amp;lt;00:13, 13.12s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:05&amp;lt;00:00, 23.14s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:05&amp;lt;00:00, 12.57s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-8...\n::group::Benchmark run on llama31_8b_trie-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:55783\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 8\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 145.9924921989441 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 125.71795888699853\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 226\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.07954313042091696\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 15.590453562499723\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 22.065264378762365\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 42146.169247800935\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 34771.37922100155\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 34268.11150784366\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 121142.4036295616\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 654.817181999897\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 719.6390424996935\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 230.7389530893105\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 926.6670409503786\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 139.30646581500116\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 123.18961856740539\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 45.38405603118154\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 234.98073891690743\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 149.55740854289778\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 119.10013899978367\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 117.20567193888456\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 501.5192781499355\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.3524382372198693\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]", "duration": "00:02:08", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-16]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:08&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:37:09] 127.0.0.1:56568 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:37:26] 127.0.0.1:37900 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:37:26] 127.0.0.1:37906 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:37:26] 127.0.0.1:37910 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:37:26] 127.0.0.1:37924 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:37:26] 127.0.0.1:37938 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:37:26] 127.0.0.1:37946 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:37:26] 127.0.0.1:37960 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:37:26] 127.0.0.1:37976 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:37:26] 127.0.0.1:37992 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:37:26] 127.0.0.1:38002 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:21,  2.40s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:19,  2.41s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:27&amp;lt;01:20, 11.48s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:29&amp;lt;00:47,  7.87s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:30&amp;lt;00:26,  5.38s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:35&amp;lt;00:20,  5.24s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:39&amp;lt;00:14,  4.98s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:51&amp;lt;00:13,  6.98s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:05&amp;lt;00:09,  9.33s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:47&amp;lt;00:00, 19.42s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:47&amp;lt;00:00, 10.76s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-16...\n::group::Benchmark run on llama31_8b_trie-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:55783\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 16\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 127.86351871490479 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 107.59346032599933\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 230\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.09294245179679901\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 18.216720552172607\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 25.782236128432046\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 39224.95389030046\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 32740.552859500895\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 28943.694572878067\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 103663.88972977002\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 679.7132604006038\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 746.6254150003806\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 224.11069643357035\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 855.1667459908276\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 141.40848314764648\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 129.3773094347924\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 36.1273287594456\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 201.48339323510336\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 138.94414603064257\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 110.77420000037819\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 109.31497380665947\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 500.98619782776956\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.6456633861809142\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n"}], "reports/shortfin_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]", "duration": "00:02:48", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/shortfin_benchmark_test.py::test_shortfin_benchmark[llama31_8b_trie-32]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:02:48&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\n[2025-02-10 11:39:17] 127.0.0.1:50484 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s][2025-02-10 11:39:34] 127.0.0.1:47688 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:39:34] 127.0.0.1:47698 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:39:34] 127.0.0.1:47700 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:39:34] 127.0.0.1:47716 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:39:34] 127.0.0.1:47722 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:39:34] 127.0.0.1:47736 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:39:34] 127.0.0.1:47742 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:39:34] 127.0.0.1:47748 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:39:34] 127.0.0.1:47756 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n[2025-02-10 11:39:34] 127.0.0.1:47770 - &amp;quot;POST /generate HTTP/1.1&amp;quot; 200\n\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:20,  2.30s/it]\r 20%|\u2588\u2588        | 2/10 [00:04&amp;lt;00:19,  2.44s/it]\r 30%|\u2588\u2588\u2588       | 3/10 [00:22&amp;lt;01:05,  9.36s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:24&amp;lt;00:39,  6.51s/it]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:28&amp;lt;00:28,  5.61s/it]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:31&amp;lt;00:18,  4.73s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:34&amp;lt;00:12,  4.23s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [01:13&amp;lt;00:30, 15.31s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [01:19&amp;lt;00:12, 12.40s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:27&amp;lt;00:00, 29.42s/it]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [02:27&amp;lt;00:00, 14.74s/it]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:87 Starting benchmark run on llama31_8b_trie-32...\n::group::Benchmark run on llama31_8b_trie-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:91 Running SGLang Benchmark with the following settings:\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:92 Test parameterization: llama31_8b_trie-32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:93 Benchmark Args: Backend: shortfin\nBase URL: http://localhost:55783\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-859/model_cache0/SanctumAI_Meta-Llama-3.1-8B-Instruct-GGUF\nRequest Rate: 32\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:104 Benchmark run completed in 167.63290810585022 seconds\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:105 \n\n======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: shortfin\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 147.38490149800054\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 225\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 0.06784955513326894\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 13.298512806120712\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 18.821466593968804\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 44952.91968439924\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 30057.76210050135\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 41752.13439646459\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 141224.2307079892\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 684.8906714996701\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 754.1995439987659\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 222.44074739669662\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 834.9845693076713\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 150.1285834817097\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 139.07839641841406\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 60.408175529008275\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 272.41245662194325\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 159.57079443691256\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 109.63986349997867\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 142.57462844731617\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 503.036958549128\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.0500356025280566\nINFO     sglang_benchmarks.shortfin_benchmark_test:shortfin_benchmark_test.py:107 Benchmark run successful\n::endgroup::\n\n--------------------------- Captured stderr teardown ---------------------------\n[2025-02-10 11:42:01] Shutting down\n[2025-02-10 11:42:01] Waiting for application shutdown.\n[2025-02-10 11:42:01] Application shutdown complete.\n[2025-02-10 11:42:01] Finished server process [410025]\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:47", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[1-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:47&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nCould not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:02&amp;lt;00:18,  2.09s/it]\r 20%|\u2588\u2588        | 2/10 [00:02&amp;lt;00:07,  1.03it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:04,  1.52it/s]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.46it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:04&amp;lt;00:04,  1.16it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:06&amp;lt;00:05,  1.38s/it]\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:07&amp;lt;00:03,  1.04s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:07&amp;lt;00:01,  1.28it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:08&amp;lt;00:00,  1.19it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.62it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&amp;lt;00:00,  1.17it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test0/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test0/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 0 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 1.0023248195648193 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 2.0043346881866455 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 3.0062642097473145 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 4.008528709411621 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 5.011754751205444 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 6.0136964321136475 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 7.016484260559082 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 8.01923418045044 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 9.02202296257019 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 10.024870157241821 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 11.028094291687012 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 12.030680418014526 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 13.03287935256958 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 14.035489559173584 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 15.03827953338623 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 16.040924787521362 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 17.04638433456421 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 18.04923725128174 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 19.051989316940308 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 20.054739952087402 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 21.057568788528442 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 22.060404539108276 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 23.063788414001465 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 24.066526889801025 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 25.069336891174316 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 26.072285890579224 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 27.074983596801758 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 28.07781958580017 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 29.08122754096985 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 30.083998918533325 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:54 Server has not started yet; waited 31.086756706237793 seconds; timeout: 600 seconds.\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test0\nRequest Rate: 1\nERROR    huggingface_hub.file_download:file_download.py:1387 Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test0&amp;#x27;, request_rate=1, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test0/sglang_10_1.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    1         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  8.51      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.17      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          230.28    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         325.92    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          556.20    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             3.84      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3272.06   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3551.90   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          30.12     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        24.05     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           51.83     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          11.54     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        11.99     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           13.20     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           11.73     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         12.95     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            16.81     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 14.185762643814087 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 1\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 8.511289746998955\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.1749100661889649\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 230.28237297303713\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 325.92005236081883\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3272.055218099922\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3551.895455000704\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1902.7761364067962\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6054.368890289916\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 30.121324100036873\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 24.05262899992522\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 11.850235877771908\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 51.83147910891421\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 11.543869924920472\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 11.988416607012207\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.422115734363287\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 13.198706744301766\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 11.729087055352615\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 12.951926999448915\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.8451073034096535\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 16.812995319342008\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 3.844370612871727\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:13", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[2-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:13&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nCould not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:01&amp;lt;00:11,  1.27s/it]\r 20%|\u2588\u2588        | 2/10 [00:01&amp;lt;00:04,  1.70it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:07,  1.02s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.16it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.56it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:04,  1.16s/it]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:06&amp;lt;00:01,  1.60it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.49it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.55it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07&amp;lt;00:00,  1.34it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test1/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test1/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test1\nRequest Rate: 2\nERROR    huggingface_hub.file_download:file_download.py:1387 Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test1&amp;#x27;, request_rate=2, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test1/sglang_10_2.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    2         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  7.48      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.34      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          262.21    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         371.10    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          633.31    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             4.70      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3510.79   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 3910.96   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          31.48     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        27.56     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           49.30     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          12.88     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        12.74     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           15.13     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.59     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.10     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            27.62     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 12.05805778503418 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 2\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 7.475019820998568\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.3377890948072597\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 262.2066625822229\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 371.10269489953384\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3510.7863251996605\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 3910.9576970004127\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1916.1812182031526\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6196.4477086891575\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 31.48115539952414\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 27.55947399782599\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 10.492401982732597\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 49.296166030289896\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 12.882914918597821\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 12.74451440041393\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.9734463492612953\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 15.132484956323921\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.587892639653486\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 13.097752998874057\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.8198818872491276\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 27.617546681067324\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 4.6966916600505595\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[4-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nCould not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:06,  1.31it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:03,  2.51it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:08,  1.25s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.19it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.63it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:05&amp;lt;00:04,  1.02s/it]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:06&amp;lt;00:00,  1.78it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.65it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.45it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test2/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test2/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test2\nRequest Rate: 4\nERROR    huggingface_hub.file_download:file_download.py:1387 Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test2&amp;#x27;, request_rate=4, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test2/sglang_10_4.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    4         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.91      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.45      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          283.72    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         401.56    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          685.28    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.21      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3599.27   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4051.03   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          34.04     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        29.68     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           54.26     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.48     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.25     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           15.76     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.90     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.16     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            28.18     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.57968521118164 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 4\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.908137598002213\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.4475681554015272\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 283.7233584586993\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 401.5554063083836\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3599.2708907000633\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4051.0271870007273\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1914.3289656313566\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6224.2816788198625\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 34.036354400450364\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 29.67732300021453\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 12.622618385232771\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 54.26084436079691\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.476294664408611\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 13.247257552837187\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.1174245477027214\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 15.76128320252774\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.898778387119682\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 13.15750100184232\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 2.7100898594186615\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 28.183617879076333\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.210189924041102\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[8-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nCould not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:04,  2.03it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.10it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:09,  1.29s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:04,  1.21it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.54it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.10it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  1.99it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.71it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.53it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test3/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test3/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test3\nRequest Rate: 8\nERROR    huggingface_hub.file_download:file_download.py:1387 Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test3&amp;#x27;, request_rate=8, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test3/sglang_10_8.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    8         \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.55      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.53      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          299.23    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         423.50    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          722.73    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.49      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3596.94   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4066.49   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          29.76     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        26.38     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           49.48     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          14.06     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.27     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           19.70     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.91     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.08     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            28.32     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.990949153900146 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 8\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.5501338819995\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.5266863517829945\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 299.23052494946694\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 423.5027939846027\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3596.943279700281\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4066.4869230022305\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1883.9509466930165\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6162.69654707874\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 29.7555615001329\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 26.380753499324783\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 11.110239119309034\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 49.48399296048592\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 14.059839937824595\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 13.265144257900419\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 2.3179059019133486\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 19.700114606451827\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.90584419934937\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 13.08371699997224\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.0883296638382993\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 28.317336698928557\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.491404213255981\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:12", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[16-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:12&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nCould not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:03,  2.90it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:02,  3.72it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:03&amp;lt;00:08,  1.27s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.13it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.51it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.16it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.16it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.07it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.66it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.56it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test4/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test4/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test4\nRequest Rate: 16\nERROR    huggingface_hub.file_download:file_download.py:1387 Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test4&amp;#x27;, request_rate=16, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test4/sglang_10_16.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    16        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.40      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.56      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          306.03    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         433.12    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          739.15    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.63      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3604.82   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4089.08   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          35.09     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        35.20     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           60.49     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.82     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.29     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           17.67     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.92     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.13     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            13.59     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 11.003920078277588 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 16\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.404678856000828\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.5613585356634323\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 306.02627299003274\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 433.1208577930361\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3604.8161160993914\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4089.078099998005\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1888.5476568294735\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6164.125307199793\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 35.0901490997785\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 35.19712199886271\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 12.542770673154713\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 60.48536434020207\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.818224567086235\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 13.290708742023915\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 1.7194765598237043\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 17.674939401968004\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.915032203689305\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 13.125583498549531\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.5150956015316788\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 13.587773089857365\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.628410412368887\n\n"}], "reports/sglang_index.html:app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]": [{"extras": [], "result": "Passed", "testId": "app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]", "duration": "00:00:11", "resultsTableRow": ["&lt;td class=\"col-result\"&gt;Passed&lt;/td&gt;", "&lt;td class=\"col-testId\"&gt;app_tests/benchmark_tests/llm/sglang_benchmarks/sglang_benchmark_test.py::test_sglang_benchmark[32-NousResearch/Meta-Llama-3-8B]&lt;/td&gt;", "&lt;td class=\"col-duration\"&gt;00:00:11&lt;/td&gt;", "&lt;td class=\"col-links\"&gt;&lt;/td&gt;"], "log": "----------------------------- Captured stderr call -----------------------------\nCould not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\n\r  0%|          | 0/10 [00:00&amp;lt;?, ?it/s]\r 10%|\u2588         | 1/10 [00:00&amp;lt;00:02,  3.70it/s]\r 20%|\u2588\u2588        | 2/10 [00:00&amp;lt;00:01,  4.20it/s]\r 30%|\u2588\u2588\u2588       | 3/10 [00:02&amp;lt;00:08,  1.24s/it]\r 40%|\u2588\u2588\u2588\u2588      | 4/10 [00:03&amp;lt;00:05,  1.12it/s]\r 50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:03&amp;lt;00:03,  1.50it/s]\r 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:04&amp;lt;00:03,  1.20it/s]\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:04&amp;lt;00:00,  2.22it/s]\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:05&amp;lt;00:00,  2.12it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.68it/s]\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:06&amp;lt;00:00,  1.59it/s]\n\n------------------------------ Captured log call -------------------------------\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:32 Preparing tokenizer_path: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test5/tokenizer.json...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:34 Downloading tokenizer NousResearch/Meta-Llama-3-8B from Hugging Face...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:39 Tokenizer saved to /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test5/tokenizer.json\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:71 Beginning SGLang benchmark test...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:45 Waiting for server to start at http://localhost:30000...\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:51 Server successfully started\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:93 Running SGLang Benchmark with the following args:\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:94 Backend: sglang\nBase URL: http://localhost:30000\nNum Prompt: 10\nTokenizer: /shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test5\nRequest Rate: 32\nERROR    huggingface_hub.file_download:file_download.py:1387 Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: &amp;#x27;/home/nod/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/.no_exist/0e9e39f249a16976918f6564b8830bc894c89659/chat_template.jinja&amp;#x27;\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Namespace(num_prompts=10, base_url=&amp;#x27;http://localhost:30000&amp;#x27;, tokenizer=&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test5&amp;#x27;, request_rate=32, backend=&amp;#x27;sglang&amp;#x27;, output_file=PosixPath(&amp;#x27;/shark-dev/pytest-of-nod/pytest-860/sglang_benchmark_test5/sglang_10_32.jsonl&amp;#x27;), seed=1, extra_request_body=None, port=8000, model=&amp;#x27;meta-llama/Llama-3.1-8B-Instruct&amp;#x27;, dataset_name=&amp;#x27;sharegpt&amp;#x27;, random_input_len=None, random_output_len=None, random_range_ratio=0.0, dataset_path=&amp;#x27;&amp;#x27;, sharegpt_output_len=None, multi=False, disable_tqdm=False, disable_stream=False, disable_ignore_eos=False, lora_name=None, profile=False, sharegpt_context_len=None, apply_chat_template=False, return_logprob=False, max_concurrency=None)\n\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Input tokens: 1960\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 #Output tokens: 2774\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Starting initial single prompt test run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Initial test run completed. Starting main benchmark run...\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 \n============ Serving Benchmark Result ============\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Backend:                                 sglang    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Traffic request rate:                    32        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Max reqeuest concurrency:                not set   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Successful requests:                     10        \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Benchmark duration (s):                  6.28      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total input tokens:                      1960      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens:                  2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total generated tokens (retokenized):    2774      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Request throughput (req/s):              1.59      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Input token throughput (tok/s):          312.01    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Output token throughput (tok/s):         441.59    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Total token throughput (tok/s):          753.60    \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Concurrency:                             5.70      \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ----------------End-to-End Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean E2E Latency (ms):                   3578.65   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median E2E Latency (ms):                 4066.95   \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Time to First Token----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TTFT (ms):                          54.27     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TTFT (ms):                        51.01     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TTFT (ms):                           81.46     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 -----Time per Output Token (excl. 1st token)------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean TPOT (ms):                          13.27     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median TPOT (ms):                        13.18     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 TPOT (ms):                           15.08     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ---------------Inter-token Latency----------------\nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Mean ITL (ms):                           12.75     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 Median ITL (ms):                         13.02     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 P99 ITL (ms):                            13.51     \nINFO     sglang_benchmarks.sglang_benchmark_test:mock.py:1189 ==================================================\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:102 Benchmark run completed in 10.9090735912323 seconds\nINFO     sglang_benchmarks.sglang_benchmark_test:sglang_benchmark_test.py:103 ======== RESULTS ========\nINFO     sglang_benchmarks.utils:utils.py:74 BACKEND: sglang\nINFO     sglang_benchmarks.utils:utils.py:74 DATASET_NAME: sharegpt\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_RATE: 32\nINFO     sglang_benchmarks.utils:utils.py:74 MAX_CONCURRENCY: None\nINFO     sglang_benchmarks.utils:utils.py:74 SHAREGPT_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_INPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_OUTPUT_LEN: None\nINFO     sglang_benchmarks.utils:utils.py:74 RANDOM_RANGE_RATIO: 0.0\nINFO     sglang_benchmarks.utils:utils.py:74 DURATION: 6.281819067997276\nINFO     sglang_benchmarks.utils:utils.py:74 COMPLETED: 10\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_INPUT_TOKENS: 1960\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 TOTAL_OUTPUT_TOKENS_RETOKENIZED: 2774\nINFO     sglang_benchmarks.utils:utils.py:74 REQUEST_THROUGHPUT: 1.5918955786143214\nINFO     sglang_benchmarks.utils:utils.py:74 INPUT_THROUGHPUT: 312.011533408407\nINFO     sglang_benchmarks.utils:utils.py:74 OUTPUT_THROUGHPUT: 441.5918335076127\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_E2E_LATENCY_MS: 3578.654624400224\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_E2E_LATENCY_MS: 4066.9491890002973\nINFO     sglang_benchmarks.utils:utils.py:74 STD_E2E_LATENCY_MS: 1878.8994988887366\nINFO     sglang_benchmarks.utils:utils.py:74 P99_E2E_LATENCY_MS: 6117.3870414994235\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TTFT_MS: 54.273052800272126\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TTFT_MS: 51.005240999074886\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TTFT_MS: 19.721222821556747\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TTFT_MS: 81.46235577150946\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_TPOT_MS: 13.273445509995652\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_TPOT_MS: 13.176527944204508\nINFO     sglang_benchmarks.utils:utils.py:74 STD_TPOT_MS: 0.9516783765666431\nINFO     sglang_benchmarks.utils:utils.py:74 P99_TPOT_MS: 15.078254423801264\nINFO     sglang_benchmarks.utils:utils.py:74 MEAN_ITL_MS: 12.750972921850769\nINFO     sglang_benchmarks.utils:utils.py:74 MEDIAN_ITL_MS: 13.015184000323643\nINFO     sglang_benchmarks.utils:utils.py:74 STD_ITL_MS: 3.4553384195989247\nINFO     sglang_benchmarks.utils:utils.py:74 P99_ITL_MS: 13.50836317953508\nINFO     sglang_benchmarks.utils:utils.py:74 CONCURRENCY: 5.696844473970411\n\n"}]}, "renderCollapsed": ["passed"], "initialSort": "result", "title": "shortfin_index.html"}' id="data-container"></div>
<script>
      (function(){function r(e,n,t){function o(i,f){if(!n[i]){if(!e[i]){var c="function"==typeof require&&require;if(!f&&c)return c(i,!0);if(u)return u(i,!0);var a=new Error("Cannot find module '"+i+"'");throw a.code="MODULE_NOT_FOUND",a}var p=n[i]={exports:{}};e[i][0].call(p.exports,function(r){var n=e[i][1][r];return o(n||r)},p,p.exports,r,e,n,t)}return n[i].exports}for(var u="function"==typeof require&&require,i=0;i<t.length;i++)o(t[i]);return o}return r})()({1:[function(require,module,exports){
const { getCollapsedCategory, setCollapsedIds } = require('./storage.js')

class DataManager {
    setManager(data) {
        const collapsedCategories = [...getCollapsedCategory(data.renderCollapsed)]
        const collapsedIds = []
        const tests = Object.values(data.tests).flat().map((test, index) => {
            const collapsed = collapsedCategories.includes(test.result.toLowerCase())
            const id = `test_${index}`
            if (collapsed) {
                collapsedIds.push(id)
            }
            return {
                ...test,
                id,
                collapsed,
            }
        })
        const dataBlob = { ...data, tests }
        this.data = { ...dataBlob }
        this.renderData = { ...dataBlob }
        setCollapsedIds(collapsedIds)
    }

    get allData() {
        return { ...this.data }
    }

    resetRender() {
        this.renderData = { ...this.data }
    }

    setRender(data) {
        this.renderData.tests = [...data]
    }

    toggleCollapsedItem(id) {
        this.renderData.tests = this.renderData.tests.map((test) =>
            test.id === id ? { ...test, collapsed: !test.collapsed } : test,
        )
    }

    set allCollapsed(collapsed) {
        this.renderData = { ...this.renderData, tests: [...this.renderData.tests.map((test) => (
            { ...test, collapsed }
        ))] }
    }

    get testSubset() {
        return [...this.renderData.tests]
    }

    get environment() {
        return this.renderData.environment
    }

    get initialSort() {
        return this.data.initialSort
    }
}

module.exports = {
    manager: new DataManager(),
}

},{"./storage.js":8}],2:[function(require,module,exports){
const mediaViewer = require('./mediaviewer.js')
const templateEnvRow = document.getElementById('template_environment_row')
const templateResult = document.getElementById('template_results-table__tbody')

function htmlToElements(html) {
    const temp = document.createElement('template')
    temp.innerHTML = html
    return temp.content.childNodes
}

const find = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return elem.querySelector(selector)
}

const findAll = (selector, elem) => {
    if (!elem) {
        elem = document
    }
    return [...elem.querySelectorAll(selector)]
}

const dom = {
    getStaticRow: (key, value) => {
        const envRow = templateEnvRow.content.cloneNode(true)
        const isObj = typeof value === 'object' && value !== null
        const values = isObj ? Object.keys(value).map((k) => `${k}: ${value[k]}`) : null

        const valuesElement = htmlToElements(
            values ? `<ul>${values.map((val) => `<li>${val}</li>`).join('')}<ul>` : `<div>${value}</div>`)[0]
        const td = findAll('td', envRow)
        td[0].textContent = key
        td[1].appendChild(valuesElement)

        return envRow
    },
    getResultTBody: ({ testId, id, log, extras, resultsTableRow, tableHtml, result, collapsed }) => {
        const resultBody = templateResult.content.cloneNode(true)
        resultBody.querySelector('tbody').classList.add(result.toLowerCase())
        resultBody.querySelector('tbody').id = testId
        resultBody.querySelector('.collapsible').dataset.id = id

        resultsTableRow.forEach((html) => {
            const t = document.createElement('template')
            t.innerHTML = html
            resultBody.querySelector('.collapsible').appendChild(t.content)
        })

        if (log) {
            // Wrap lines starting with "E" with span.error to color those lines red
            const wrappedLog = log.replace(/^E.*$/gm, (match) => `<span class="error">${match}</span>`)
            resultBody.querySelector('.log').innerHTML = wrappedLog
        } else {
            resultBody.querySelector('.log').remove()
        }

        if (collapsed) {
            resultBody.querySelector('.collapsible > td')?.classList.add('collapsed')
            resultBody.querySelector('.extras-row').classList.add('hidden')
        } else {
            resultBody.querySelector('.collapsible > td')?.classList.remove('collapsed')
        }

        const media = []
        extras?.forEach(({ name, format_type, content }) => {
            if (['image', 'video'].includes(format_type)) {
                media.push({ path: content, name, format_type })
            }

            if (format_type === 'html') {
                resultBody.querySelector('.extraHTML').insertAdjacentHTML('beforeend', `<div>${content}</div>`)
            }
        })
        mediaViewer.setup(resultBody, media)

        // Add custom html from the pytest_html_results_table_html hook
        tableHtml?.forEach((item) => {
            resultBody.querySelector('td[class="extra"]').insertAdjacentHTML('beforeend', item)
        })

        return resultBody
    },
}

module.exports = {
    dom,
    htmlToElements,
    find,
    findAll,
}

},{"./mediaviewer.js":6}],3:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const storageModule = require('./storage.js')

const getFilteredSubSet = (filter) =>
    manager.allData.tests.filter(({ result }) => filter.includes(result.toLowerCase()))

const doInitFilter = () => {
    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)
}

const doFilter = (type, show) => {
    if (show) {
        storageModule.showCategory(type)
    } else {
        storageModule.hideCategory(type)
    }

    const currentFilter = storageModule.getVisible()
    const filteredSubset = getFilteredSubSet(currentFilter)
    manager.setRender(filteredSubset)

    const sortColumn = storageModule.getSort()
    doSort(sortColumn, true)
}

module.exports = {
    doFilter,
    doInitFilter,
}

},{"./datamanager.js":1,"./sort.js":7,"./storage.js":8}],4:[function(require,module,exports){
const { redraw, bindEvents, renderStatic } = require('./main.js')
const { doInitFilter } = require('./filter.js')
const { doInitSort } = require('./sort.js')
const { manager } = require('./datamanager.js')
const data = JSON.parse(document.getElementById('data-container').dataset.jsonblob)

function init() {
    manager.setManager(data)
    doInitFilter()
    doInitSort()
    renderStatic()
    redraw()
    bindEvents()
}

init()

},{"./datamanager.js":1,"./filter.js":3,"./main.js":5,"./sort.js":7}],5:[function(require,module,exports){
const { dom, find, findAll } = require('./dom.js')
const { manager } = require('./datamanager.js')
const { doSort } = require('./sort.js')
const { doFilter } = require('./filter.js')
const {
    getVisible,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    getSortDirection,
    possibleFilters,
} = require('./storage.js')

const removeChildren = (node) => {
    while (node.firstChild) {
        node.removeChild(node.firstChild)
    }
}

const renderStatic = () => {
    const renderEnvironmentTable = () => {
        const environment = manager.environment
        const rows = Object.keys(environment).map((key) => dom.getStaticRow(key, environment[key]))
        const table = document.getElementById('environment')
        removeChildren(table)
        rows.forEach((row) => table.appendChild(row))
    }
    renderEnvironmentTable()
}

const addItemToggleListener = (elem) => {
    elem.addEventListener('click', ({ target }) => {
        const id = target.parentElement.dataset.id
        manager.toggleCollapsedItem(id)

        const collapsedIds = getCollapsedIds()
        if (collapsedIds.includes(id)) {
            const updated = collapsedIds.filter((item) => item !== id)
            setCollapsedIds(updated)
        } else {
            collapsedIds.push(id)
            setCollapsedIds(collapsedIds)
        }
        redraw()
    })
}

const renderContent = (tests) => {
    const sortAttr = getSort(manager.initialSort)
    const sortAsc = JSON.parse(getSortDirection())
    const rows = tests.map(dom.getResultTBody)
    const table = document.getElementById('results-table')
    const tableHeader = document.getElementById('results-table-head')

    const newTable = document.createElement('table')
    newTable.id = 'results-table'

    // remove all sorting classes and set the relevant
    findAll('.sortable', tableHeader).forEach((elem) => elem.classList.remove('asc', 'desc'))
    tableHeader.querySelector(`.sortable[data-column-type="${sortAttr}"]`)?.classList.add(sortAsc ? 'desc' : 'asc')
    newTable.appendChild(tableHeader)

    if (!rows.length) {
        const emptyTable = document.getElementById('template_results-table__body--empty').content.cloneNode(true)
        newTable.appendChild(emptyTable)
    } else {
        rows.forEach((row) => {
            if (!!row) {
                findAll('.collapsible td:not(.col-links', row).forEach(addItemToggleListener)
                find('.logexpander', row).addEventListener('click',
                    (evt) => evt.target.parentNode.classList.toggle('expanded'),
                )
                newTable.appendChild(row)
            }
        })
    }

    table.replaceWith(newTable)
}

const renderDerived = () => {
    const currentFilter = getVisible()
    possibleFilters.forEach((result) => {
        const input = document.querySelector(`input[data-test-result="${result}"]`)
        input.checked = currentFilter.includes(result)
    })
}

const bindEvents = () => {
    const filterColumn = (evt) => {
        const { target: element } = evt
        const { testResult } = element.dataset

        doFilter(testResult, element.checked)
        const collapsedIds = getCollapsedIds()
        const updated = manager.renderData.tests.map((test) => {
            return {
                ...test,
                collapsed: collapsedIds.includes(test.id),
            }
        })
        manager.setRender(updated)
        redraw()
    }

    const header = document.getElementById('environment-header')
    header.addEventListener('click', () => {
        const table = document.getElementById('environment')
        table.classList.toggle('hidden')
        header.classList.toggle('collapsed')
    })

    findAll('input[name="filter_checkbox"]').forEach((elem) => {
        elem.addEventListener('click', filterColumn)
    })

    findAll('.sortable').forEach((elem) => {
        elem.addEventListener('click', (evt) => {
            const { target: element } = evt
            const { columnType } = element.dataset
            doSort(columnType)
            redraw()
        })
    })

    document.getElementById('show_all_details').addEventListener('click', () => {
        manager.allCollapsed = false
        setCollapsedIds([])
        redraw()
    })
    document.getElementById('hide_all_details').addEventListener('click', () => {
        manager.allCollapsed = true
        const allIds = manager.renderData.tests.map((test) => test.id)
        setCollapsedIds(allIds)
        redraw()
    })
}

const redraw = () => {
    const { testSubset } = manager

    renderContent(testSubset)
    renderDerived()
}

module.exports = {
    redraw,
    bindEvents,
    renderStatic,
}

},{"./datamanager.js":1,"./dom.js":2,"./filter.js":3,"./sort.js":7,"./storage.js":8}],6:[function(require,module,exports){
class MediaViewer {
    constructor(assets) {
        this.assets = assets
        this.index = 0
    }

    nextActive() {
        this.index = this.index === this.assets.length - 1 ? 0 : this.index + 1
        return [this.activeFile, this.index]
    }

    prevActive() {
        this.index = this.index === 0 ? this.assets.length - 1 : this.index -1
        return [this.activeFile, this.index]
    }

    get currentIndex() {
        return this.index
    }

    get activeFile() {
        return this.assets[this.index]
    }
}


const setup = (resultBody, assets) => {
    if (!assets.length) {
        resultBody.querySelector('.media').classList.add('hidden')
        return
    }

    const mediaViewer = new MediaViewer(assets)
    const container = resultBody.querySelector('.media-container')
    const leftArrow = resultBody.querySelector('.media-container__nav--left')
    const rightArrow = resultBody.querySelector('.media-container__nav--right')
    const mediaName = resultBody.querySelector('.media__name')
    const counter = resultBody.querySelector('.media__counter')
    const imageEl = resultBody.querySelector('img')
    const sourceEl = resultBody.querySelector('source')
    const videoEl = resultBody.querySelector('video')

    const setImg = (media, index) => {
        if (media?.format_type === 'image') {
            imageEl.src = media.path

            imageEl.classList.remove('hidden')
            videoEl.classList.add('hidden')
        } else if (media?.format_type === 'video') {
            sourceEl.src = media.path

            videoEl.classList.remove('hidden')
            imageEl.classList.add('hidden')
        }

        mediaName.innerText = media?.name
        counter.innerText = `${index + 1} / ${assets.length}`
    }
    setImg(mediaViewer.activeFile, mediaViewer.currentIndex)

    const moveLeft = () => {
        const [media, index] = mediaViewer.prevActive()
        setImg(media, index)
    }
    const doRight = () => {
        const [media, index] = mediaViewer.nextActive()
        setImg(media, index)
    }
    const openImg = () => {
        window.open(mediaViewer.activeFile.path, '_blank')
    }
    if (assets.length === 1) {
        container.classList.add('media-container--fullscreen')
    } else {
        leftArrow.addEventListener('click', moveLeft)
        rightArrow.addEventListener('click', doRight)
    }
    imageEl.addEventListener('click', openImg)
}

module.exports = {
    setup,
}

},{}],7:[function(require,module,exports){
const { manager } = require('./datamanager.js')
const storageModule = require('./storage.js')

const genericSort = (list, key, ascending, customOrder) => {
    let sorted
    if (customOrder) {
        sorted = list.sort((a, b) => {
            const aValue = a.result.toLowerCase()
            const bValue = b.result.toLowerCase()

            const aIndex = customOrder.findIndex((item) => item.toLowerCase() === aValue)
            const bIndex = customOrder.findIndex((item) => item.toLowerCase() === bValue)

            // Compare the indices to determine the sort order
            return aIndex - bIndex
        })
    } else {
        sorted = list.sort((a, b) => a[key] === b[key] ? 0 : a[key] > b[key] ? 1 : -1)
    }

    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const durationSort = (list, ascending) => {
    const parseDuration = (duration) => {
        if (duration.includes(':')) {
            // If it's in the format "HH:mm:ss"
            const [hours, minutes, seconds] = duration.split(':').map(Number)
            return (hours * 3600 + minutes * 60 + seconds) * 1000
        } else {
            // If it's in the format "nnn ms"
            return parseInt(duration)
        }
    }
    const sorted = list.sort((a, b) => parseDuration(a['duration']) - parseDuration(b['duration']))
    if (ascending) {
        sorted.reverse()
    }
    return sorted
}

const doInitSort = () => {
    const type = storageModule.getSort(manager.initialSort)
    const ascending = storageModule.getSortDirection()
    const list = manager.testSubset
    const initialOrder = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed', 'Skipped', 'Passed']

    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    if (type?.toLowerCase() === 'original') {
        manager.setRender(list)
    } else {
        let sortedList
        switch (type) {
        case 'duration':
            sortedList = durationSort(list, ascending)
            break
        case 'result':
            sortedList = genericSort(list, type, ascending, initialOrder)
            break
        default:
            sortedList = genericSort(list, type, ascending)
            break
        }
        manager.setRender(sortedList)
    }
}

const doSort = (type, skipDirection) => {
    const newSortType = storageModule.getSort(manager.initialSort) !== type
    const currentAsc = storageModule.getSortDirection()
    let ascending
    if (skipDirection) {
        ascending = currentAsc
    } else {
        ascending = newSortType ? false : !currentAsc
    }
    storageModule.setSort(type)
    storageModule.setSortDirection(ascending)

    const list = manager.testSubset
    const sortedList = type === 'duration' ? durationSort(list, ascending) : genericSort(list, type, ascending)
    manager.setRender(sortedList)
}

module.exports = {
    doInitSort,
    doSort,
}

},{"./datamanager.js":1,"./storage.js":8}],8:[function(require,module,exports){
const possibleFilters = [
    'passed',
    'skipped',
    'failed',
    'error',
    'xfailed',
    'xpassed',
    'rerun',
]

const getVisible = () => {
    const url = new URL(window.location.href)
    const settings = new URLSearchParams(url.search).get('visible')
    const lower = (item) => {
        const lowerItem = item.toLowerCase()
        if (possibleFilters.includes(lowerItem)) {
            return lowerItem
        }
        return null
    }
    return settings === null ?
        possibleFilters :
        [...new Set(settings?.split(',').map(lower).filter((item) => item))]
}

const hideCategory = (categoryToHide) => {
    const url = new URL(window.location.href)
    const visibleParams = new URLSearchParams(url.search).get('visible')
    const currentVisible = visibleParams ? visibleParams.split(',') : [...possibleFilters]
    const settings = [...new Set(currentVisible)].filter((f) => f !== categoryToHide).join(',')

    url.searchParams.set('visible', settings)
    window.history.pushState({}, null, unescape(url.href))
}

const showCategory = (categoryToShow) => {
    if (typeof window === 'undefined') {
        return
    }
    const url = new URL(window.location.href)
    const currentVisible = new URLSearchParams(url.search).get('visible')?.split(',').filter(Boolean) ||
        [...possibleFilters]
    const settings = [...new Set([categoryToShow, ...currentVisible])]
    const noFilter = possibleFilters.length === settings.length || !settings.length

    noFilter ? url.searchParams.delete('visible') : url.searchParams.set('visible', settings.join(','))
    window.history.pushState({}, null, unescape(url.href))
}

const getSort = (initialSort) => {
    const url = new URL(window.location.href)
    let sort = new URLSearchParams(url.search).get('sort')
    if (!sort) {
        sort = initialSort || 'result'
    }
    return sort
}

const setSort = (type) => {
    const url = new URL(window.location.href)
    url.searchParams.set('sort', type)
    window.history.pushState({}, null, unescape(url.href))
}

const getCollapsedCategory = (renderCollapsed) => {
    let categories
    if (typeof window !== 'undefined') {
        const url = new URL(window.location.href)
        const collapsedItems = new URLSearchParams(url.search).get('collapsed')
        switch (true) {
        case !renderCollapsed && collapsedItems === null:
            categories = ['passed']
            break
        case collapsedItems?.length === 0 || /^["']{2}$/.test(collapsedItems):
            categories = []
            break
        case /^all$/.test(collapsedItems) || collapsedItems === null && /^all$/.test(renderCollapsed):
            categories = [...possibleFilters]
            break
        default:
            categories = collapsedItems?.split(',').map((item) => item.toLowerCase()) || renderCollapsed
            break
        }
    } else {
        categories = []
    }
    return categories
}

const getSortDirection = () => JSON.parse(sessionStorage.getItem('sortAsc')) || false
const setSortDirection = (ascending) => sessionStorage.setItem('sortAsc', ascending)

const getCollapsedIds = () => JSON.parse(sessionStorage.getItem('collapsedIds')) || []
const setCollapsedIds = (list) => sessionStorage.setItem('collapsedIds', JSON.stringify(list))

module.exports = {
    getVisible,
    hideCategory,
    showCategory,
    getCollapsedIds,
    setCollapsedIds,
    getSort,
    setSort,
    getSortDirection,
    setSortDirection,
    getCollapsedCategory,
    possibleFilters,
}

},{}]},{},[4]);
    </script>
</footer>
</html>